var tipuesearch = {"pages":[{"title":"Blog","text":"This blog is mainly needed for me to write down my notes. But some of it might even be useful to others. I also try to write some small tutorials for others to learn from. Feel free to contact me for any suggestions. Here are all tags of my blog. There is a category overview page for the blog. And here is the chronological order .","tags":"pages","url":"pages/blog.html","loc":"pages/blog.html"},{"title":"Books","text":"C and C++ Name Authors Publisher Effective Modern C++ Scott Meyers Addison-Wesley Effective C++ Scott Meyers Addison-Wesley Die C++ Programmiersprache Bjarne Stroustrup Addison-Wesley Modern C++ Design Andrei Alexandrescu Addison-Wesley Secure Coding in C and C++ Robert C. Seacord Addison-Wesley Exceptional C++ Herb Sutter Addison-Wesley Algorithms in C (Parts 1-5) Robert Sedgewick Addison-Wesley Boost C++ Application Development Cookbook Antony Polukhin Packt Publishing Architecture, Design Patterns and Refactoring Name Authors Publisher Design Patterns Erich Gamma et al. (Gang of Four) Addison-Wesley Head First Design Patterns Eric Freeman et al. O'Reilly Refactoring Martin Fowler Addison-Wesley Effektives Arbeiten mit Legacy Code Michael C. Feathers mitp Fundamentals of Software Architecture Neal Ford, Mark Richards O'Reilly Software Architecture Patterns Mark Richards O'Reilly Domain Driven Design Quickly Abel Avram, et al. InfoQ Testing and QA Name Authors Publisher Agile Testing: A Practical Guide for Testers and Agile Teams Lisa Crispin, Janet Gregory Addison-Wesley More Agile Testing: Learning Journeys for the Whole Team Lisa Crispin, Janet Gregory Addison-Wesley The Art of Unit Testing, 2nd Edition Roy Osherove Manning Publications Test Driven Development for Embedded C James W. Grenning The Pragmatic Bookshelf Using Node.js for UI Testing Pedro Teixeira Packt Publishing Hacking mit Python Justin Seitz dpunkt.verlag Aus dem Tagebuch eines Bughunters Tobias Klein dpunkt.verlag Cucumber Cookbook Shankar Garg Packt Publishing Testing Angular Applications Corinna Cohn, Craig Nishina, Jesse Palmer, Michael Giambalvo Manning Publications Skills & Tools Name Authors Publisher Mastering Regular Expressions Jeffrey E.F. Friedl O'Reilly Mastering Python Regular Expressions Félix López, Víctor Romero Packt Publishing Seven Concurrency Models […] Paul Butcher The Pragmatic Bookshelf gRPC: Up and Running Danesh Kuruppu, Kasun Indrasiri O'Reilly Mastering CMake Ken Martin, Bill Hofmann Kitware Language Implementation Patterns Terence Parr The Pragmatic Bookshelf Learning GNU Emacs Debra Cameron, et al. O'Reilly Programming Collective Intelligence Toby Segaran O'Reilly UML 2.0 Dan Pilone O'Reilly Understanding Computation Tom Stuart O'Reilly Structure and Interpretation of Computer Programs Hal Abelson, Jerry Sussman and Julie Sussman MIT Press tmux (Productive Mouse-Free Development) Brian P. Hogan The Pragmatic Bookshelf Einstieg in XML Helmut Vonhoegen Galileo Computing Mastering Spring 5.0 Ranga Karanam Packt Publishing Presentations Name Authors Publisher Presentation Patterns: Techniques for Crafting Better Presentations Neal Ford, Matthew McCullough, Nate Schutta Addison-Wesley Instant HTML5 Presentations How-to Davi Ferreira Packt Publishing Agile Software Development and Scrum Name Authors Publisher Scrum and XP from the Trenches (second edition) Henrik Kniberg InfoQ Co-Pilot Till Bay, Benno Baumgartner, Matthias Hüni, Eva Jutzeler, Michela Pedroni buch & netz Operating Systems and Networking Name Authors Publisher Moderne Betriebssysteme Andrew S. Tanenbaum Pearson Using the FreeRTOS Real Time Kernel Richard Barry Richard Barry FreeRTOS Reference Manual Real Time Engineers ltd. Real Time Engineers ltd. Linux- UNIX -Programmierung Jürgen Wolf Rheinwerk Shell-Programmierung Jürgen Wolf Rheinwerk What You Need to Know about Docker Scott Gallagher Packt Publishing TCP / IP für Dummies Candace Leiden, Marshall Wilensky Wiley- VCH Verlag GmbH Embedded Systems and Computer Architecture Name Authors Publisher Vom Gatter zu VHDL M. Künzli vdf Lehrbuch Technische Informatik I+ II T. Müller et al. vdf Lehrbuch x86-64 Assembly Language Programming with Ubuntu Ed Jorgensen Ed Jorgensen Mastering Embedded Linux Programming Chris Simmonds Packt Publishing Version Control Name Authors Publisher Version Control with Git Jon Loeliger O'Reilly Pro Git (Second Edition) Scott Chacon, Ben Straub Apress Version Control with Subversion C. Michael Pilato et al. O'Reilly Robotics Name Authors Publisher A Gentle Introduction to ROS Jason M. O'Kane University of South Carolina Modern Robotics Kevin M. Lynch, Frank C. Park Cambridge University Press Learning Robotics Using Python Lentin Joseph Packt Publishing Mechanical Engineering Name Authors Publisher Normen-Auszug 2006 Swissmem Swissmem 3D-Drucken für Einsteiger Heiner Stiller Franzis (onleihe)","tags":"pages","url":"pages/books.html","loc":"pages/books.html"},{"title":"Contact","text":"I'm alway interested in new challenges and open for new input. Feel free to contact me: The best way to contact me is on LinkedIn . You can check out some of my work on GitHub . I also like stack overflow for getting advice. Here is my XING account. You can also write me an Sorry, you need Javascript on to email me. If you are a recruiter (head hunter) then please read this page before contacting me.","tags":"pages","url":"pages/contact.html","loc":"pages/contact.html"},{"title":"Courses","text":"It's very important to stay up to date about technological progress and to improve own skills. One of the best way to keep the pace on this fast changig topcs is to attend to courses. Another one is to read books about the topics. Traditional Classes Here is a list of the most important courses I attended in the last few years. Topic Course Legic Identification ( RFID ) Legic Wetzikon SMD Soldering Meili Data C++ for Embedded Systems HSR Rapperswil Agile Software Development Golo Roden Online Courses I also like to attend online courses. It's a flexible way to learn new things. Topic Course Agile Testing Essentials informit Software Architecture Fundamentals O'Reilly The Zen of Software Architecture O'Reilly Domain-Driven Design Distilled O'Reilly Moving to Microservices: Using Domain-Driven Design […] O'Reilly Angular 8 - The Complete Guide O'Reilly Angular Crash Course O'Reilly Learning Qt 5 Packt Effective Modern C++ live! O'Reilly Getting Started with C++17 Programming Packt Designing RESTful APIs Udacity Building Cloud Services with the Java Spring Framework Coursera gRPC [Java] Master Class […] Packt Cucumber with Java Build Automation Framework […] Packt JavaScript Basics Udacity How to Use Git and GitHub Udacity Advanced Operating Systems Udacity Clojure Clojure for the Brave and True Programming Languages Udacity iOS Development iTunes U Scheme (Lisp) MIT OpenCourseWare UX Design for Mobile Developers Udacity Finance and capital markets Khan Academy","tags":"pages","url":"pages/courses.html","loc":"pages/courses.html"},{"title":"Welcome to my Site","text":"This page is about my profesional life as a developer. Im mainly a software/firmware developer. But I know also a lot about electronical and mechanical engineering. There is an overview about my resume and my main skills . There are also some notes about my knowledge and what I've learned so far. I also write casually some small blog articles. Feel free to contact me.","tags":"pages","url":".","loc":"."},{"title":"Projects","text":"Work Experience Profidata AG Currently I am working as a senior software developer at Profidata. My current task there is to ensure software quality of the highly scalable and parallelized server application Xentis. I implemented system tests to check our REST API 's as well as integration tests and unit tests. To test a big legacy application I found that characterization tests help to increase code quality and capture current behavior. With my experience in quality software design and testing techinques I support other developpers with testing and refactoring the huge legacy codebase. Previous tasks included the extension of the Xentis server application with REST services. This enables other systems to use the functionality of Xentis outside of the core application. I also worked on the implementation of different functional modules such as investment performance and price services. The development is done with modern and generic C++ on Linux and with Java. Part of the testing infrastructure is written in Python. I am also learning a lot about financial markets, investment and fintech. Another responsibility of mine is the maintenance of our build environment and continuous integration infrastructure (CMake, Jenkins pipeline, Gerrit), integration of static and dynamic analysis tools (Sanitizers, Clang Static Analyzer), as well as regression- and unit-tests. I also became an expert in development on Linux. www.profidatagroup.com Kaba: Portable Configuration and Programming Device ( PD1460 ) At first I was responsible for the development of the firmware for a programming device. The programmer consists of a lot of peripheral devices including a LCD display, a keypad, NFC , USB , RS -232 and SD card. It runs on an ARM controller. The firmware was developed in C. Electronic Locks Later I was involved in the embedded software development of electronic locks that were based on different micro controller architectures, such as ARM Cortex, ATmega and Coldfire. I extended and maintained the firmware that was written in C. Later it was decided to rewrite the embedded software using C++ for it. Beside my role as a programmer, I acted as the main software architect on this project, driving the development of a modular, object oriented design. It was designed for usability, testability, extensibility and reuse. To enforce quality and security, system- and unit-tests were implemented with a Test Driven Development approach. I also mentored and supported my colleagues on how to use C++ in an embedded, battery driven environment with with low resources. All these devices contained different peripherals, such as NFC readers, EEPROM , ZigBee, USB and motors, which were connected by common digital communication interfaces. My team also extended and maintained PC simulations for all devices (firmware and hardware) to simplify development, testing and bug fixing. Other Tasks Besides programming, I defined functional requirements, specifications and maintained documentation. Moreover, I was the project manager in charge of developing a customer specific device. www.dormakaba.com CodeCheck AG As a small project at CodeCheck I developed a complete mobile app for Symbian with Qt and C++. Learned a lot about development for mobile platforms and REST services. Improved my C++ and Qt skills. In the process of developing the app I also ported a barcode scanner from Objective-C to C++. Schiller AG After my studies as electronic engineer I worked as an intern at Schiller. There I created an application for the configuration and combination of filters for electrocardiograph devices ( ECG ). The software reads the configuration from a XML file and shows it graphically as connected boxes which represent the filters. The configuration can be stored again in a XML file. For creating and editing configurations the filters are placed on a workspace via drag and drop and they can be connected with mouse clicks. The application was developed in C++ with Qt, Boost and Xerces for Windows and Linux. Beside improving my C++ skills I also learned a lot of general programming tools and techniques as Design Patterns, OOA / OOD , UML , XML ( SAX / DOM ), Doxygen. I not only learned a lot of technical stuff but also about medical engineering. As side projects I set up automated integration testing for the GUI of an ECG application and evaluated installer tools for Windows and Linux. AIM Industrielle Montage AG After my apprenticeship I worked at AIM as mechanical engineer. There I assembled mechanical devices and fabricated small series and individual mechanical parts. I also made some improvements on existing parts. An other task was to manage the warehouse of mechanical parts and devices (logistics). Educational Projects Bachelor Thesis The task of my Bachelor thesis was to develop and build a quad copter in the shape of a film reel to entertain visitors at Disneyland. The project was a cooperation between the Autonomous Systems Lab at the ETH and the ZHAW , where I obtained my degree in Electrical Engineering. We were an interdisciplinary team of electrical and mechanical engineers. My task was to develop the firmware and create the controlling algorithm for the embedded hardware. The hardware, based on two micro controllers, had many different peripheral devices, such as gyroscopes, a compass, pressure-, ultrasonic- and acceleration-sensors, as well as the motors to drive the propellers. Different interfaces were used for communication with the peripheral devices, such as UART , SPI , I2C , ADC and ZigBee. The bachelor thesis was rewarded with a grade of 5.5 (of 6). More information on the project can be found on the homepage: www.reely.ethz.ch Private Projects PX4 Firmware for Drones From time to time I'm contributing to the PX4 project. It provides an open source auto pilot firmware for drones. The framework is used in several commercial and industrial applications. Cucumber Recently I started to contribute to the C++ implementation of Cucumber . My main motivation is to modernize it's codebase and to remove it's dependency on the Ruby implementation of Cucumber.","tags":"pages","url":"pages/projects.html","loc":"pages/projects.html"},{"title":"Note to Recruiters and Headhunters","text":"If you are a recruiter (headhunter) and are looking to hire me, please read this page before contacting me . I'm currently not looking for a new job. But I'm always open for new interesting challenges. There are some requirements that a new position has to meet to catch my attention: Musts: Embedded Software: I prefer Firmware development to programming of PC or server applications. C/C++: I like C but C++ is even better. There must be at least some C++ development involved. Working site in Zürich or near (max. 30 minutes by public transport): I just don't like to lose too much time commuting. Possibility to work part time (80%): This is an absolute must. I have a family that I need to take care of. Possibility to work from home occasionally (home office): Sometimes it's just convenient to work from home. Mustn't: no Windows centric development: It's just not as nice and powerful as UNIX /Linux and it lacks a lot of important tools. I'll never work for the weapons industry: I don't like wars! Nice to have Possibility to make sport (jogging in the neighbourhood, showers) Team leading position","tags":"pages","url":"pages/note_to_recruiters_and_headhunters.html","loc":"pages/note_to_recruiters_and_headhunters.html"},{"title":"Resume","text":"Personal Data Lukas Woodtli Zürich Nationality Swiss, Czech Actual Job Software Developer C++ on Linux at Profidata. Work Experience Time Occupation 2016 - present Software engineer C++ on Linux, Profidata AG 2010 - 2015 Senior firmware/software engineer, Kaba AG , Wetzikon 2010 (4 months) Software engineer, Codecheck Zürich 2010 (6 months) Internship as software engineer, Schiller AG Baar 2002 (6 months) Mechanic, AIM Horgen Education Years: 2015 - 2016 Certificate Program in Computer Science ( CAS - INFK ) 2004 - 2009 Bachelor of Electrical Engineering, University of Applied Science Zürich ( ZHAW ) 2003 - 2004 Teacher Training College, PH Zürich 1997 - 2001 Apprenticeship as Mechanic, MSW Winterthur Courses See my page about the courses I attended. Stays Abroad When Where 1984 Ten months in Ann Arbor, Michigan ( USA ) 2002-2003 Six months in New Zealand Language Skills German (Mother tongue): Very good writing and speaking knowledge Czech (Mother tongue): Very good speaking and basic writing knowledge English: Good writing and speaking knowledge French: Some basic knowledge Membership Association for Computing Machinery ( ACM ) Online Profiles and Social Networks Home page GitHub stack overflow LinkedIn XING Hobbies Sports: Mountain Climbing Running Sailing Kung Fu and Eskrima Cooking Reading books References Available on request Application Documents Documents Compact Application Documents (compact) Full Application Documents (full) Recruiters and Head Hunter If you are a recruiter (head hunter) then please read this page before contacting me.","tags":"pages","url":"pages/resume.html","loc":"pages/resume.html"},{"title":"Skills","text":"Please check also the page with my projects . Programming Languages Language Knowledge C++ very good C very good Python very good Java good Assembler (x86, ARM ) good Shell scripting (bash, dash, sh, zsh) good Matlab/Simulink basic Lisp (Scheme, Clojure) basic Scala basic Ruby basic Objective C basic Oberon basic JavaScript / Typescript basic VHDL basic Frameworks Framework Knowledge Qt very good STL very good Boost very good Spring basic JavaRx basic Cocoa Touch basic Angular basic Build Tools Tool Knowledge CMake very good Make good Gradle good Travis CI very good Jenkins (pipelines) very good Gerrit good GitLab/Github good Unit Tests and Quality Engineering Test Tool Knowledge Catch2 very good Google Test good Boost Test good Qt Test good Test Coverage (gcc, clang, Java) good boost::di (dependency injection) good Mocking Frameworks (Trompeloeil, Mockito) good ApprovalTests very good Serenity BDD good Cucumber good REST -assured good Static Analysis Analysis Tool Knowledge Clang static analyzer, clang tidy good CppCheck good Pylint good OCLint basic Dynamic Analysis Tool Knowledge Sanitizers (gcc, clang) very good Valgrind basic system tap basic Instruments (XCode) basic Perf basic Version Control VCS Knowledge Git very good SVN very good OS 's OS Knowledge OS X very good Linux ( RHEL , Fedora, Ubuntu) very good Windows good Skills Skill Knowledge OOP / OOD very good Design/Architecture Patterns very good Clean Code, SOLID , TDD , … very good Regexp very good UML very good Project Management / Scrum / XP very good Legacy Code (refactoring, testing, improving…) very good JSON very good Debugging (gdb, Visual Studio) very good XML good SQL basic UX / UI Design basic Documentation Tool Knowledge Doxygen very good Graphviz very good MediaWiki good Markdown very good LaTeX good IDE 's IDE Knowledge CLion, IntelliJ, PyCharm very good Qt Creator very good Visual Studio & Visual Studio Code good Eclipse good XCode good IAR Studio good Electronic and Control Skill Level Embedded Systems and Micro Controllers very good Control Systems Engeneering good Signals and Systems good RFID good Digital Signal Processing basic Hardware Development (Analog & Digital) basic Processor Architectures Intel/ AMD x86 & x64 ARM Cortex-M3 (Silicon Labs, STMicroelectronics) ARM Cortex-A7/A8 (Raspberry Pi 2, BeagleBone Black) Atmel Atmega Freescale ColdFire Mechanics Skill Knowledge Production Engineering (milling, turning, drilling, …) good Assembly good CNC Programming (G-Code) basic CAD and CAM basic PLC / SPS (Simatic) basic","tags":"pages","url":"pages/skills.html","loc":"pages/skills.html"},{"title":"Seven Concurrency Models in Seven Weeks","text":"This page collects notes taken from Seven Concurrency Models in Seven Weeks When Threads Unravel by Paul Butcher Some of my examples are here Chapter 1 Introduction Concurrent or Parallel? Related but Different \" A concurrent program has multiple logical threads of control. These threads may or may not run in parallel.\" \" A parallel program potentially runs more quickly than a sequential program by executing different parts of the computation simultaneously (in parallel). It may or may not have more than one logical thread of control.\" \" […] concurrency is an aspect of the problem domain — your program needs to handle multiple simultaneous (or near-simultaneous) events.\" \" Parallelism […] is an aspect of the solution domain — you want to make your program faster by processing different portions of the problem in parallel.\" \" Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once.\" Beyond Sequencial Programming \" […] traditional threads and locks don't provide any direct support for parallelism.\" \" […] concurrent programs are often nondetermistic - they will give different results depending on the precise timing of events.\" \" Parallelism […] doesn't necessarily imply nondetermism\" Data Parallelism \" Data-parallel (sometimes called SIMD […]) architectures are capable of performing the same operations on a large quantity of data in parallel.\" Chapter 2 Threads and Locks The Simplest Thing That Could Possibly Work \" [Threads] also provide almost no help to the poor programmer, making programs very difficult to get right in the first place and even more difficult to maintain.\" Day 1: Mutual Exclusion and Memory Models \" [There is something very] basic you need to worry about when dealing with shared memory - the Memory Model.\" Creating a Thread \" Threads communicate with each other via shared memory.\" Mysterious Memory \" The compiler is allowed to statically optimize your code by reordering things.\" \" The JVM is allowed to dynamically optimize your code by reordering things.\" \" The hardware you're running on is allowed to optimize performance by reordering things.\" \" Sometimes effects don't become visible to other threads at all.\" Memory Visibility \" The Java memory model defines when changes to memory made by one thread become visible to another thread.\"\" \" An important point that's easily missed is that both threads need to use synchronization. It's not enough for just the thread making changes to do so.\" Multiple Locks \" Deadlock is a danger whenever a thread tries to hold more than one lock. Happily, there is a simple rule that guarantees you will never deadlock - always acquire locks in a fixed, global order.\" The Perils of Alien Methods \" avoid calling alien methods while holding a lock\" Day 1 Wrap Up \" […] three primary perils of threads and locks - race conditions, deadlock and memory visibility […] rules that help us avoiding them:\" \" Synchronize all access to shared variables.\" Both the writing and the reading threads need to use synchronization.\" Acquire multiple locks in a fixed, global order.\" Don't call alien methods while holding a lock.\" Hold locks for the shortest possible amount of time.\" Day 2: Beyond Intrinsic Locks \" Intrinsic locks are convenient but limited.\" \" There is no way to interrupt a thread that's blocked as a result of trying to acquire an intrinsic lock.\" \" There is no way to time out while trying to acquire an intrinsic lock.\" \" There's exactly one way to acquire an intrinsic lock: a synchronized block.\" Synchronized block: synchronized ​ ( object ) { // use shared resources } \" This means that lock acquisition and release have to take place in the same method and have to be strictly nested.\" \" Note that declaring a method as synchronized is just syntactic sugar for surrounding the method's body with the following:\" ​ synchronized ​ ( this ) { ​ // method body } \" ReentrantLock allows us to transcend these restrictions by providing explicit lock and unlock methods instead of using synchronized .\" ​ Lock ​ lock = ​ new ReentrantLock ​ (); lock . lock (); try ​ { // use shared resources​ } finally { lock . unlock (); } Hand-over-Hand Locking \" Hand-over-hand locking is an alternative in which we lock only a small portion of the list, allowing other threads unfettered access as long as they're not looking at the particular nodes we've got locked.\" Condition Variables \" Concurrent programming often involves waiting for something to happen. […] This type of situation is what condition variables are designed to address.\" ​ ReentrantLock lock = new ReentrantLock ​ (); Condition condition = lock . newCondition (); lock . lock (); try { while ( ! condition is true ) condition . await (); // use shared resources } finally { lock . unlock (); } \" A condition variable is associated with a lock, and a thread must hold that lock before being able to wait on the condition. Once it holds the lock, it checks to see if the condition that it's interested in is already true. If it is, then it continues with whatever it wants to do and unlocks the lock. If, however, the condition is not true, it calls await , which atomically unlocks the lock and blocks on the condition variable.\" \" An operation is atomic if, from the point of view of another thread, it appears to be a single operation that has either happened or not - it never appears to be halfway through.\" \" When another thread calls signal or signalAll [on the condition variable] to indicate that the condition might now be true, await unblocks and automatically reacquires the lock. An important point is that when await returns, it only indicates that the condition might be true. This is why await is called within a loop - we need to go back, recheck whether the condition is true, and potentially block on await again if necessary.\" Atomic Variables \" Using an atomic variable instead of locks has a number of benefits. First, it's not possible to forget to acquire the lock when necessary.\" \" Second, because no locks are involved, it's impossible for an operation on an atomic variable to deadlock.\" \" atomic variables are the foundation of non-blocking, lock-free algorithms, which achieve synchronization without locks or blocking.\" If you think that programming with locks is tricky, then just wait until you try writing lock-free code. What About Volatile? \" Java allows us to mark a variable as volatile . Doing so guarantees that reads and writes to that variable will not be reordered.\" \" valid use cases for volatile variables are rare. If you find yourself considering volatile , you should probably use one of the java.util.concurrent.atomic classes instead.\" In C++ volatile has a different meaning. It indicates that the read of a variable is not allowed to be optimized out. This is important for reading memory mapped I/O or memory mapped registers and similar use cases. Day 2 Wrap-Up \" [With ReentrantLock and java.util.concurrent.atomic ] threads can do the following: \" Be interrupted while trying to acquire a lock\" \" Time out while acquiring a lock\" \" Acquire and release locks in any order [Danger! Dead locks!]\" \" Use condition variables to wait for arbitrary conditions to become true\" \" Avoid locks entirely by using atomic variables\" Day 3: On the Shoulders of Giants How Large Should My Thread Pool Be? \" The optimum number of threads will vary according to the hardware […], whether your threads are IO or CPU bound, what else the machine is doing at the same time, and a host of other factors. […] a good rule of thumb is that for computation-intensive tasks, [to have] approximately the same number of threads as available cores. Larger numbers are appropriate for IO -intensive tasks. Beyond this rule of thumb, your best bet is to create a realistic load test and break out the stopwatch.\" Why a Blocking Queue? \" As well as blocking queues, java.util.concurrent provides ConcurrentLinkedQueue , an unbounded, wait-free, and nonblocking queue. That sounds like a very desirable set of attributes [for the producer-consumer pattern]. The issue is that […] if the producer runs faster than the consumer, the queue will get larger and larger.\" \" The beauty of the producer-consumer pattern is that it allows us not only to produce and consume values in parallel, but also to have multiple producers and/or multiple consumers.\" \" Unfortunately, synchronized collections don't provide atomic read-modify-write methods, so this isn't going to help us. If we want to use a HashMap , we're going to have to synchronize access to it ourselves.\" \" Happily, we're not beaten yet. ConcurrentHashMap in java.util.concurrent looks like exactly what we need. Not only does it provide atomic read-modify-write methods, but it's been designed to support high levels of concurrent access (via a technique called lock striping).\" \" Instead of put, we're now using a combination of putIfAbsent and replace .\" \" Here's the documentation for putIfAbsent : If the specified key is not already associated with a value, associate it with the given value. This is equivalent to\" if ( ! map . containsKey ( key )) return map . put ( key , value ); else return ​ map . get ( key ); \" except that the action is performed atomically.\" \" And for replace : Replaces the entry for a key only if currently mapped to a given value. This is equivalent to\" if ( map . containsKey ( key ) && map . get ( key ). equals ( oldValue )) { map . put ( key , newValue ); return true ; } else return false ; \" except that the action is performed atomically.\" Day 3 Wrap-Up What We Learned in Day 3 \" the facilities provided by java.util.concurrent [make] programs safer and more efficient by doing the following:\" \" Using thread pools instead of creating threads directly\" \" Creating simpler and more efficient listener-management code with CopyOnWriteArrayList \" \" Allowing producers and consumers to communicate efficiently with ArrayBlockingQueue \" \" Supporting highly concurrent access to a map with ConcurrentHashMap \" Wrap-Up Weaknesses \" Threads and locks provide no direct support for parallelism […] they can be used to parallelize a sequential algorithm, but this has to be constructed out of concurrent primitives, which introduces the specter of nondeterminism.\" \" threads and locks support only shared-memory architectures. If you need to support distributed memory (and, by extension, either geographical distribution or resilience), you will need to look elsewhere.\" The Elephant in the Room \" […] what makes multithreaded programming difficult is not that writing it is hard, but that testing it is hard . It's not the pitfalls that you can fall into; it's the fact that you don't necessarily know whether you've fallen into one of them.\" Maintenance \" It's one thing to make sure that everything's synchronized correctly, locks are acquired in the right order, and no foreign functions are called with locks held. It's quite another to guarantee that it will remain that way after twelve months of maintenance by ten different programmers.\" \" if you can't reliably test for threading problems, you can't reliably refactor multithreaded code.\" \" think very carefully about our multithreaded code. And then when we've done that, think about it very carefully some more.\" Other Languages \" the general principles we covered in this chapter are broadly applicable. The rules about using synchronization to access shared variables; acquiring locks in a fixed, global order; and avoiding alien method calls while holding a lock are applicable to any language with threads and locks.\" \" a memory model was added to the C11 and C++ 11 standards.\" Additional Notes Race Condition A race condition is the behavior of a software system where the output is dependent on the sequence or timing of other uncontrollable events. It becomes a bug when events do not happen in the order the programmer intended. Memory Visibility The memory model defines when changes to memory made by one thread become visible to another thread. Deadlocks & Livelocks Deadlock is a danger whenever a thread tries to hold more than one lock. Happily, there is a simple rule that guarantees you will never deadlock - always acquire locks in a fixed, global order. Chapter 3 Functional Programming \" In contrast to an imperative program, which consists of a series of statements that change global state when executed, a functional program models computation as the evaluation of expressions. Those expressions are built from pure mathematical functions that are both first-class (can be manipulated like any other value) and side effect-free. It's particularly useful when dealing with concurrency because the lack of side effects makes reasoning about thread safety much easier.\" If It Hurts, Stop Doing It \" Functional programs have no mutable state, so they cannot suffer from any of the problems associated with shared mutable state.\" Day 1: Programming Without Mutable State It's Good to Be Lazy \" Sequences in Clojure are lazy - elements of a lazy sequence are generated only when they're needed.\" Realizing a (lazy) sequence means to fully evaluate it. \" One final aspect of lazy sequences is that not only do we not need to generate the elements at the end of a sequence until we need them (which might be never), but we can discard the elements at the front if we've finished with them (if we don't \"hold on to our head\").\" Day 2: Functional Parallelism Reducers \" A reducer is a recipe that describes how to reduce a collection. The normal version of map takes a function and a (possibly lazy) sequence and returns another (possibly lazy) sequence\" \" A reducible isn't a directly usable value - it's just something that can subsequently be passed to reduce [or into , which uses reduce internally]\" \" A reducer […], returns a recipe for creating a result - a recipe that isn't executed until it's passed to either reduce or fold . This has two primary benefits: It's more efficient than a chain of functions returning lazy sequences, because no intermediate sequences need to be created. It allows fold to parallelize the entire chain of operations on the underlying collection.\" Day 3: Functional Concurrency Same Structure, Different Evaluation Order \" Functional languages have a much more declarative feel [than OOP or procedual languages (imperative)].\" Futures \" A future takes a body of code and executes it in another thread. Its return value is a future object.\" \" We can retrieve the value of a future by dereferencing it with either deref or the shorthand @ \" \" Dereferencing a future will block until the value is available (or realized).\" Promises \" A promise is very similar to a future in that it's a value that's realized asynhronously and accessed with deref or @ , which will block until it's realized. The difference is that creating a promise does not cause any code to run - instead its value is set with deliver.\" A promse can be used to pass a result from one thread to another one. Wrap-Up Weaknesses \" Many people expect that functional code will be less efficient than its impertive equivalent. Although there are performance implications for some types of problem, the penalty is likely to be less than you fear. And any small performance hit is likely to be more than worth it for the payoff of increased robustness and scalability.\" Chapter 4 The Clojure Way - Separating Identity from State Day 1: Atoms and Persistent Data Structures Atoms \" An atom is an atomic variable […] (in fact, Clojure's atoms are built on top of java.util.concurrent.atomic ).\" Persistent Data Structures \" Persistence in this case doesn't have anything to do with persistence on disk or within a database. Instead it refers to a data structure that always preserves its previous version when it's modified, which allows code to have a consistent view of the data in the face of modifications.\" \" Persistent data structures behave as though a complete copy is made each time they're modified.\" \" The implementation is much more clever than that and makes use of structure sharing.\" Identity or State? \" Persistent data structures are invaluable for concurrent programming because once a thread has a reference to a data structure, it will see no changes made by any other thread. Persistent data structures separate identity from state.\" \" A variable in an imperative language complects (interweaves, interconnects) identity and state - a single identity can only ever have a single value, making it easy to lose sight of the fact that the state is really a sequence of values over time . Persistent data structures separate identity from state - if we retrieve the current state associated with an identity, that state is immutable and unchanging, no matter what happens to the identity from which we retrieved it in the future.\" Retries \" Atoms can be lockless - internally they make use of the compareAndSet method in java.util.concurrent.AtomicReference . That means that they're very fast and don't block. […] But it also means that swap! needs to handle the case where the value of the atom has been changed by another thread in between it calling the function to generate a new value and it trying to change that value.\" \" If that happens, swap! will retry. It will discard the value returned by the function and call it again with the atom's new value. […] This means that it's essential that the function passed to swap! has no side effects\" Day 1 Wrap-Up \" Because functional data structures are persistent, changes made by one thread will not affect a second thread that already has a reference to that data structure.\" \" This allows us to separate identity from state, recognizing the fact that the state associated with an identity is really a sequence of values over time.\" Day 2: Agents and Software Transactional Memory Agents \" An agent is similar to an atom in that it encapsulates a reference to a single value\" \" If multiple threads call send concurrently, execution of the functions passed to send is serialized: only one will execute at a time. This means that they will not be retried and can therefore contain side effects.\" Is an Agent an Actor? \" An agent has a value that can be retrieved directly with deref . An actor encapsulates state but provides no direct means to access it.\" \" An actor encapsulates behavior; an agent does not\" \" Actors provide sophisticated support for error detection and recovery.\" \" Actors can be remote [distributed].\" \" Composing actors can deadlock\" Waiting for Agent Actions to Complete \" Clojure provides the await function, which blocks until all actions dispatched from the current thread to the given agent(s) have completed.\" Error Handling \" Like atoms, agents also support both validators and watchers.\" \" Once an agent experiences an error, it enters a failed state by default, and attempts to dispatch new actions fail. We can find out if an agent is failed (and if it is, why) with agent-error , and we can restart it with restart-agent \" Software Transactional Memory \" Refs are more sophisticated than atoms and agents, providing software transactional memory ( STM )\" Transactions \" STM transactions are atomic, consistent, and isolated [but not durable]\" \" Everything within the body of dosync constitutes a single transaction.\" Multiple Refs \" If the STM runtime detects that concurrent transactions are trying to make conflicting changes, one or more of the transactions will be retried. This means that, as when modifying an atom, transactions should not have side effects .\" Safe Side Effects in Transactions \" Agents are transaction aware\" \" If you use send to modify an agent within a transaction, that send will take place only if the transaction succeeds. Therefore, if you want to achieve some side effect when a transaction succeeds, using send is an excellent way to do so.\" What's with the Exclamation Marks? \" Clojure uses an exclamation mark to indicate that functions […] are not transaction-safe .\" Shared Mutable State in Clojure \" An atom allows you to make synchronous changes to a single value - synchronous because when swap! returns, the update has taken place. Updates to one atom are not coordinated with other updates.\" \" An agent allows you to make asynchronous changes to a single value - asynchronous because the update takes place after send returns. Updates to one agent are not coordinated with other updates.\" \" Refs allow you to make synchronous, coordinated changes to multiple values.\" Day 2 Wrap-Up \" Atoms enable independent, synchronous changes to single values. Agents enable independent, asynchronous changes to single values. Refs enable coordinated, synchronous changes to multiple values.\" Day 3: In Depth Atoms or STM ? \" Atoms enable independent changes to single values, whereas refs enable coordinated changes to multiple values.\" \" whenever we need to coordinate modifications of multiple values we can either use multiple refs and coordinate access to them with transactions or collect those values together into a compound data structure stored in a single atom\" \" Experienced Clojure programmers tend to find that atoms suffice for most problems, as the language's functional nature leads to minimal use of mutable data\" What Is Loop/Recur? \" The loop macro defines a target that recur can jump to\" Wrap-Up Final Thoughts \" Clojure has found a good balance between functional programming and mutable state, allowing programmers with experience in imperative languages to get started more easily than they might in a pure functional language. And yet it does so while retaining most of functional programming's benefits, in particular its excellent support for concurrency.\" Additional Notes Atom Create: (atom <initial-val>) Read: (defef <agent>) (or @ ) Update (with function): (swap! <atom> <fn> <args>) Set value: (reset! <atom> <value> Agent Writes are serialized, no retries occur Create: (agent <initial-val>) Read: (defef <agent>) (or @ ) Update (with function): (send <agent> <update-fn> <args>) Wait for completion: (await <agent>) Refs Software Transactional Memory Modification only possible in transaction: (dosync <...>) Create: (ref <initial-val>) Read: (defef <ref>) (or @ ) Update (with function): (alter <ref> <update-fn> <args>) Set value: (ref-set <ref> <val>) Protect ref from modification by other transaction: (ensure <ref>)","tags":"Programming","url":"seven_concurrency_models_in_seven_weeks.html","loc":"seven_concurrency_models_in_seven_weeks.html"},{"title":"Domain Driven Design","text":"Introduction Domain Driven Design ( DDD ) is a set of methodologies, tools and patterns that help to develop complex software systems. The design techniques are divided into two major groups: strategical and tactical. Strategical These ideas include the cooperation of a lot of different people participating in a project. This are mainly the business analysts (domain experts) and the developers. Tactical The tactical ideas contain mostly design- and architecture-patterns. As well as common best practices regarding software engineering. Domain Model When starting to develop a complex software it helps to have a good model of the domain where the product will be used. The model is an abstraction of that domain. It is the internal representation of the domain for the software. The model can be described with various techniques including: diagrams, text documents, use cases and scenarios ( BDD ). Usually the model is not fully defined from the beginning. It is refined and improved while developing the software. Cooperation of Business Experts and Developers To define a useful and solid model it is necessary that the software engineers (developers, architects, testers) that build the software collaborate with the business experts (analysts, users…) and figure out the essential concepts and the functionality that the software will implement. This is an iterative process where the model is constantly refined, extended and cleaned up. Also the Ubiquitous Language is evolved and refined in this process. The communication is bidirectional and defining the model is a combined effort. The resulting model needs to be understood by all participants. If a business expert doesn't understand a part of the model then there is probably something wrong with it. The discussions are a time consuming process. But it's important to have a good model and a common language. Ubiquitous Language While defining the model, a common language between business experts and developers emerges. This language should be improved to be distinct and unambiguous. It is called Ubiquitous Language and has to be used throughout the project including: Source Code DB Schema Documentation The ubiquitous language helps to overcome the problem that software engineers and business experts use different communication styles and languages. It connects all parts of the design. To form the ubiquitous language it might be helpful to experiment with alternative expressions to refine and improve the current model. Then the code (and database) need to be refactored and renamed to conform the new model. The model and the ubiquitous language are strongly connected to each other. Usually the nouns of the ubiquitous language become names of classes. Behavior of these classes (the methods) are often named with verbs of the ubiquitous language. This is a simple naming scheme that should be only used as a rule of thumb. Some behaviour (verbs) needs interaction of multiple objects and classes. This is implemented in services. Model Driven Development The transformation of the model into code is an difficult process. The developers that write this code should know the model very well and are responsible for its integrity. A change in the code needs to be immediately reflected in the model. The classes and methods of the software are a direct mapping of the model and ubiquitous Language to code. If this mapping is made obvious it makes the code very readable. Anybody that works on the code needs to know how to express the model in code. It is necessary that all developers are involved into the discussions about the model with the domain experts. If the design of the system or of central components don't reflect the model, the correctness of the software is in danger. Building Blocks and Patterns The original patterns of DDD are based on object oriented programming. But they can also be applied (in modified form) to other languages. Architecture The classical DDD system uses a layered architecture. But other architecture styles may be applied. Important concepts of an architecture is to allow decoupling in a kind of packages and to forbid cyclic dependencies. Layers This is a list of traditional layers in an n-tier application: UI : Represent information to the user and allow interaction. Application Layer: Coordination of application logic. It contains no business logic. Domain Layer: Business logic and objects. Infrastructure Layer: Persistency, Networking, I/O Building Blocks Entities Business objects that have an identity. The identity is usually a field or a combination of fields which is unique. Two objects with the same identity are considered as the same object. Special care needs to be taken to guarantee that distinct objects don't get the same identity. Value Objects Value objects are domain objects that don't have an identity. Looking at a single object, it doesn't matter which object it is but what properties it has. Two objects with the same properties are considered to be the same value object. Value objects should be preferred over entities when designing the system. Value objects shall be immutable. If an value object needs to be updated a new object is created with the new properties. Immutability makes easy to share value objects. Value objects can contain other value objects and even constant references to entitles. Services Behavior that is important for a domain but doesn't belong to only one class should be put into a service. Services work on multiple objects, possibly from different classes. They are stateless and provide simply functionality (a concept) for the domain. They should not be used to replace functionality that belong to individual classes. Instead they act as a connection point for multiple objects. Business processes should be implemented as services. Services can not only be used in the domain layer but also in the application or infrastructure layer. Domain and application services usually act on domain entities and value objects. It needs to be reassured that the layers are kept isolated from each other. Modules Large and complex models are split into modules. Modules should have low coupling between each other and high cohesion inside them. Together with well defined interfaces this allows for good testing. Module names become part of the ubiquitous language. Aggregates Aggregates are used to define ownership and boundaries of domain objects. They help to simplify relationships between objects by restricting them to be only inside the aggregate. An aggregate is also the root regarding persistence. It has to handle transactions and contract (invariant) checking. Each aggregate has one root which is the entry point for objects outside of the aggregate and it is the base entity for persistence. Only the root should be obtainable through queries from the database. Rules for Modeling Aggregates Design small aggregates Protect business invariants inside of aggregates Reference other aggregates only by identity Make a dependecy graph of the aggregates Define acceptable time frame for updates to dependent aggregates (together with domain experts) immediate eventually ( n seconds) Put all components that need immediate update in the same aggregate The other aggregates will be updated eventually Domain Events These objects model the events that the users of the system (and the domain experts) are interested in. Usually they are implemented as value objects. Factories Construction of complex business objects (like aggregates and entities) should be done by factories (factory method, abstract factory, …). Usually it is not feasible to build a complete object tree in a constructor of a single class. The creation process should be atomic and raise an exception if the construction is not possible. During the construction phase all the invariants need to be enforced. Recreate persisted objects from a database needs special care. Corrupt data needs to be fixed somehow (see Repositories). Factories are pure domain objects. Repositories Repositories encapsulate the recreation objects (aggregates) from a persistent store. They can access the infrastructure layer. But this access has to be abstracted so that it can be exchanged or replaced. The repository itself is part of the domain model. Factories are used to create new object trees. Where repositories are needed to store and retrieve already created object trees to and from a persistent store. Internally they can use factories for creating an object tree and ensure that the invariants are met. Refactoring and Model Refinement The code as well as the domain model need constant refinement and refactoring. Working on the model and improving it is an continuous effort. Therefore it's important to keep all the parts of the system aligned and up to date with the model. The code needs to be flexible to allow to introduce new concepts. If the code is not flexible enough it needs to be refactored and improved. Rich and useful domain models are developed in an iterative manner by refactoring, improvement and refinement. Bounded Context A Bounded Context is a part of the domain. It is built with one model using its Ubiquitous Language. Each Bounded context has its own model. Ideally each Bounded Context is developed and maintained by one team. The boundaries of the Context (and its model) need to be explicit. The model has to be contained completely in the Bounded Context. It's also important to define whant isn't part of a bounded context. A Bounded Context contains usually multiple modules. It‘s important to keep the borders of the bonded contexts consistent. There needs to be a means of communication between them. All the Bounded Contexts need to be continuously be integrated in the system. It's also important to check the correctness of the system with automated tests. Bounded Contexts have a name that is part of the ubiquitous language. Context Map The contexts and their relationships should be documented with a context map. This is usually done with a diagram. Everybody working on the system needs to know and understand the context map. The Context Map can also show problems with badly defined Bounded Contexts. For example if two (or more) contexts share a part of the system. When getting started with DDD it's a good starting point to draw a context map of the system. Common Patterns Shared Kernel If there is a part of the system that is present in two (or more) contexts is it called a Shared Kernel. This is a sign of uncoordinated teams working on closely related parts of the system. It's also possible to actively to decide that two teams share a part of the system and maintain this part collectively. This reduces the effort of duplication and has the benefit of a partly common Ubiquitous Language. The scope of the Shared Kernel has to be clearly defined. And any changes to it needs coordination between the teams that share it. Customer-Supplier If one subsystem depends strongly on another one they are in a Customer-Supplier relationship. The teams developing these subsystems need to meet regularly. The „customer\" has to specify its requirements. The „supplier\" team has to implement them. There should be automated acceptance tests that guarantee the proper implementation. Anti-Corruption Layer When connecting with legacy systems or third party software it's important that the domain model doesn't get polluted by the other system. The anti-corruption layer sits between the domain model and the external model. It is a natural part of the domain model (with its ubiquitous language) but translates between the two systems. Usually it is implemented as a service. Internally it uses the Facade and the Adapter patterns. It makes sense to implement even multiple services in the anti-corruption layer. Core Domain and Subdomain As big enterprise applications have huge models it makes sense to split them in separate parts (bounded contexts). These parts have a focus on different aspects of the domain: Core Domain This contains the main functionality of the application. It is the unique selling point. The best developers should be assigned to work on that part of the domain. The core should stay as small as possible. It contains a lot of business logic. Subdomains The other parts of the model is implemented in subdomains. They could be developed by an other company or could even be an generic third party tool that is integrated into the system. Generic Subdomains They should be generic and do not contain any specialties. Ideally they could be reused for other applications. Supporting Subdomains They are part of the specific domain of the application. But they don't contain core functionality.","tags":"Programming","url":"domain_driven_design.html","loc":"domain_driven_design.html"},{"title":"Exponential Coordinates of Rotations (Rodrigues' formula)","text":"Rodrigues' formula Given a vector \\(\\hat{\\omega}\\theta \\in \\mathbb{R}&#94;3\\) such tha \\(\\theta\\) is a scalar and \\(\\hat{\\omega}\\) is a unit vector in \\(\\mathbb{R}&#94;3\\) , the matrix exponential of \\([\\hat{\\omega}]\\theta = [\\hat{\\omega}\\theta] \\in so(3)\\) is: $$Rot(\\hat{\\omega}, \\theta) = e&#94;{[\\hat{\\omega}]\\theta} \\exp(\\hat{\\omega}, \\theta)=I+\\sin(\\theta)[\\hat{\\omega}]+(1-\\cos(\\theta))[\\hat{\\omega}]&#94;{2}$$ With: \\(Rot(\\hat{\\omega}, \\theta) \\in SO(3)\\) \\(\\hat{\\omega}\\) rotation axis \\(\\theta\\) rotation angle Rotation of a Vector \\(e&#94;{[\\hat{\\omega}]\\theta}p\\) has the effect of rotating \\(p \\in \\mathbb{R}&#94;3\\) about the fixed frame axis \\(\\hat{\\omega}\\) by angle \\(\\theta\\) . Rotation of a Frame \\(e&#94;{[\\hat{\\omega}] \\theta} R = Rot(\\hat{\\omega}, \\theta)R\\) is the orientation achieved by rotating \\(R\\) by \\(\\theta\\) about the axis \\(\\hat{\\omega}\\) in the fixed frame . \\(Re&#94;{[\\hat{\\omega}] \\theta} = R Rot(\\hat{\\omega}, \\theta)\\) is the orientation achieved by rotating \\(R\\) by \\(\\theta\\) about the axis \\(\\hat{\\omega}\\) in the body frame . Exponentiation Exponentiation integrates the angular velocity \\(\\hat{\\omega}\\) for time \\(\\theta\\) seconds going from the identity matrix \\(I\\) to the final rotation matrix \\(R\\) . Exp: \\([\\hat{\\omega}]\\theta \\in so(3) \\rightarrow R \\in SO(3)\\) Matrix exponential is like integration Logarithm The inverse of th matrix exponential (matrix logarithm) takes a rotation matrix \\(R\\) and returns the skew-symmetric representation of the exponential coordinates that achieve it starting from the identity orientation \\(I\\) . Log: \\(R \\in SO(3) \\rightarrow [\\hat{\\omega}]\\theta \\in so(3)\\) Matrix log is like differentiation It returns the angular velocity and the integration time that achieves the rotation matrix \\(R\\) . The matrix logarithm is an algorithm that inverts Rodrigues' formula.","tags":"Mathematics","url":"exponential_coordinates_of_rotations_(rodrigues'_formula).html","loc":"exponential_coordinates_of_rotations_(rodrigues'_formula).html"},{"title":"Angular Velocity","text":"Rotation of a body in space can be represented with a unit rotation vector (axis) and an angle of rotation around this axis. The angular velocity \\(w\\) can be defined as: $$w = \\hat{w} \\dot{\\theta}$$ Where: \\(\\hat{w}\\) : Rotation axis (unit vector), coordinate frame free \\(\\dot{\\theta}\\) : Rate of rotation Matrix Representation in a Coordinate Frame To represent the angular velocity \\(w\\) in coordinates a reference frame needs to be chosen. For example the stationary frame \\(\\{s\\}\\) . Angular Velocity \\(\\omega_s \\in \\mathbb{R}&#94;3\\) is the angular velocity \\(w\\) expressed in fixed frame \\(\\{s\\}\\) coordinates. $$\\dot{r}_x = \\omega_s \\times r_{\\hat{x}}$$ $$\\dot{r}_y = \\omega_s \\times r_{\\hat{y}}$$ $$\\dot{r}_z = \\omega_s \\times r_{\\hat{z}}$$ Where: \\(r_i\\) : unit axes \\(\\hat{x}\\) , \\(\\hat{y}\\) and \\(\\hat{z}\\) in fixed frame coordinates (the columns of the rotation matrix representing the fixed frame) \\(\\dot{r_i}\\) : rate of change (angular velocity) of axis \\(i\\) around the rotation axis These equations can be combined: $$\\dot{R}_{sb} = \\begin{bmatrix} \\omega_s \\times r_1 & \\omega_s \\times r_2 & \\omega_s \\times r_3 \\end{bmatrix} = \\omega_s \\times R_{sb}$$ Where: \\(R_{sb}\\) : the rotation matrix that describes the orientation of frame \\(\\{b\\}\\) with respect to the fixed frame \\(\\{s\\}\\) \\(\\dot{R}_{sb}\\) : its rate of change This can be simplified to: $$\\dot{R}_{sb} = [\\omega_s]R_{sb}$$ where: \\([\\omega_s]\\) is a \\(3 \\times 3\\) skew-symmetric matrix representation of the angular velocity \\(\\omega_s \\in \\mathbb{R}&#94;3\\) represented in the coordinate frame \\(\\{s\\}\\) . General Relations $$[\\omega_s] = \\dot{R}_{sb}R_{sb}&#94;{-1} = \\dot{R}_{sb}R_{sb}&#94;T$$ and $$[\\omega_b] = R_{sb}&#94;{-1}\\dot{R}_{sb}= R_{sb}&#94;T\\dot{R}_{sb}$$ Where \\([\\omega_s] \\in so(3)\\) : fixed frame \\(\\{s\\}\\) representation of the angular velocity \\(w\\) in skew-symmetric matrix representation \\([\\omega_b] \\in so(3)\\) : body frame \\(\\{b\\}\\) representation of the angular velocity \\(w\\) in skew-symmetric matrix representation Conversion between Frames An angular velocity \\(\\omega\\) expressed in an arbitrary frame \\(\\{d\\}\\) can be represented in another frame \\(\\{c\\}\\) using the subscript cancellation rule: $$\\omega_c = R_{cd}\\omega_d$$ $$\\omega_d = R_{dc}\\omega_c = R&#94;{-1}_{cd}\\omega_c = R&#94;T_{cd}\\omega_c$$ Literature Notes taken from: Modern Robotics: Mechanics, Planning, and Control by Kevin M. Lynch and Frank C. Park, Cambridge University Press, 2017","tags":"Mathematics","url":"angular_velocity.html","loc":"angular_velocity.html"},{"title":"Skew-symmetric matrix","text":"A skew-symmetric matrix has the property: $$A&#94;{T}=-A$$ or expressed differently: $$a_{{ij}}=-a_{{ji}}\\qquad \\forall i,j\\in \\{1,\\ldots ,n\\}$$ It can be formed from a vector \\(v = \\left ( a_1 a_2 a_3 \\right )\\) as $${\\displaystyle [a]={\\begin{pmatrix}0&-a_{3}&a_{2}\\\\a_{3}&0&-a_{1}\\\\-a_{2}&a_{1}&0\\end{pmatrix}}}$$ For real valued skew-symmetric matrices \\(A\\in \\mathbb {R}&#94;{n\\times n}\\) the diagonal values are \\(0\\) and the eigenvalues are pure imaginary or \\(0\\) . \\(so(3)\\) \\(so(3)\\) is the set of all \\(3\\times 3\\) skew-symmetric matrices. Angular Velocity \\([\\omega]\\) is the matrix representation of an angular velocity \\(\\omega \\in {\\mathbb {R}}&#94;{3}\\) and is an element of \\(so(3)\\) . Cross product For the special case \\(n = 3\\) the skew-symmetric matrices can be used to express a vector cross product as a matrix multiplication. The cross product of two vectors \\(a\\in {\\mathbb {R}}&#94;{3}\\) and \\(b\\in {\\mathbb {R}}&#94;{3}\\) can be expressed as: $$a\\times b=[a] \\cdot b$$ This allows to differentiate formula with a cross product: $${\\frac {\\partial }{\\partial b}}(a\\times b)={\\frac {\\partial }{\\partial b}}([a] b)=[a]$$ Relation to Rotation Matrices $$[\\omega_s] = \\dot{R}_{sb}R&#94;{-1}_{sb}$$ $$[\\omega_b] = R&#94;{-1}_{sb}\\dot{R}_{sb}$$ Where \\([\\omega_c] \\in so(3)\\) and \\([\\omega_b] \\in so(3)\\) are the angular velocities represented in the the reference frame \\(\\{s\\}\\) and the body frame \\(\\{b\\}\\) , respectively, as skew-symmetric matrices. Note: \\(R\\) and \\(\\dot{R}\\) individually depend on both \\(\\{s\\}\\) and \\(\\{b\\}\\) . But: \\(\\dot{R}R&#94;{-1}\\) is independent of \\(\\{b\\}\\) \\(R&#94;{-1}\\dot{R}\\) is independent of \\(\\{s\\}\\)","tags":"Mathematics","url":"skew-symmetric_matrix.html","loc":"skew-symmetric_matrix.html"},{"title":"Timezones on Unix","text":"There is an extensive time zones database maintained by IANA : tz database Informations about time zones supported by the systems are found in /usr/share/zoneinfo/ Read the files with zdump Information about current (local) time zone: /etc/localtime (links to /usr/share/zoneinfo/... ) Print information from that file: zdump /etc/localtime For setting the local time zone the environment variable TZ can be used the value of TZ (if present) is appended to /usr/share/zoneinfo/ to point to the time zone file","tags":"Programming","url":"timezones_on_unix.html","loc":"timezones_on_unix.html"},{"title":"Notes on std::chrono","text":"There is a good documentaion at cppreference.com General Use duration::count() only for legacy code and debugging Use explicit casts ( duration_cast and timepoint_cast ) only when precission loss is required lossless conversions are implicit duration_cast truncates towards zero Clocks system_clock : wall Clock calendar steady_clock : stop watch for timing measurements high_resolution_clock : usually a type alias to one of the other two types Date Main type year/month/day : errors are either caught at compile time or can be detected at runtime with .ok() (needs to be done by user!)","tags":"Programming","url":"notes_on_std__chrono.html","loc":"notes_on_std__chrono.html"},{"title":"Robot Operating System ( ROS )","text":"Currently I'm using Melodic Morenia . rosbash There is tab -completion for almost everything Load ROS Environment For Bash: source /opt/ros/melodic/setup.bash or Zsh: source /opt/ros/melodic/setup.zsh Commands Command Function rosls <package_name> List package content roscd <package_name> Go to package directory rospack Command Function rospack list List installed packages rospack find <package-name> Search for a package Each package is defined by a package directory that contains a manifest file called package.xml . Package executables are not stored in the package directory. They are placed in a separate standardized directory hierarchy. The Master ( roscore ) roscore starts the master. There are no command line arguments. Nodes Start a node ( rosrun ) rosrun <package-name> <executable-name> package-name : the name of the node's package executable-name : the executable name in that package Handle multiple nodes ( roslaunch ) Command Function roslaunch <package-name> <launch-file-name> Launch nodes from launch file in package roslaunch <launch-file-path> Launch nodes from launch file given with full path Notes about how to writ launch files can be found at roslaunch/ XML and Roslaunch tips for large projects Flags: -v : Print verbose output --screen : Display output for all nodes in launch file Commands for handling Nodes Command Function rosnode list Print the running nodes rosnode info <node-name> Inspect a running node rosnode kill <node-name> Stop and remove a running node rosnode cleanup Remove killed nodes from the list Topics and Messages Command Function rostopic list Print all topics currently published or subscribed rostopic list -v Print also publishers and subscribers rostopic echo <topic> Print data published on a topic rostopic info <topic> Print message type, publishers and subscribers rostopic type <topic> Get the message type published on a topic rosmsg show <msg-type> Get the fields of a message type rostopic pub <topic> <msg-type> -- <args> Publish on a topic (use TAB completion to get a template message) Parameters ( rosparam ) Command Function rosparam list Print a list of all parameters rosparam get <parameter-name> Ask for the value of a parameter rosparam get <namespace> Ask for all values of all parameters in a namespace rosparam set <parameter-name> <value> Set the value of a parameter rosparam set <namespace> <values> Set the values of parameters in a namespace using YAML rosparam dump <filename> <namespace> Save all parameters from a namespace in a YAML file rosparam load <filename> <namespace> Load all parameters from a YAML file into a namespace Services A client node sends a request to a server node and waits for a response . Command Function rosservice list Get a list of currently active services rosservice node <service-name> Print the node that offers a given service rosservice info <service-name> Finding the data type of a service rossrv show <service-data-type-name> Show the data fields of the request and the response rosservice call <service-name> <request> Get a response from a service rosservice : interacting with services that are currently offered by a node rossrv : information about service data types Information about writing services and clients in C++ can be found here Writing a Simple Service and Client (C++) . Topics, Messages and Services Topics Services active entities rostopic rosservice data types rosmsg rossrv Bag files Command Function rosbag record -O <filename> <topic-names> Record all messages of the given topics (stop with Ctrl-C ) rosrun rosbag record -O <filename> <topic-names> Alternative to be used in launch files rosbag play <filename> Replay bag file rosrun rosbag play <filename> Alternative to be used in launch files rosbag info <filename> Inspect bag file Flags: -j : Compress bag files Workspaces and Packages All packages belonging to one project should be placed in one workspace. Creating a Workspaces mkdir -p <workspace-name>/src # don't forget the `src` subdirectory cd <workspace-name>/ catkin_make Creating a Package cd <workspace-name>/src catkin_create_pkg <package-name> [ list of dependencies ] Building a Workspace cd <workspace-name>/ catkin_make Running a Node from Workspace A master is required to run before running any nodes. cd <workspace-name>/ source devel/setup.zsh rosrun <package-name> <executable-name> URDF and Xacro To generate an URFD file from a xacro file: xacro input.xacro > output.urdf Checking URDF files: check_urdf file.urdf Create graph for URDF : urdf_to_graphiz file.urdf # note the missing 'v' in graphiz Checking for Problems roswtf : Perform sanity checks for the running ROS system References A Gentle Introduction to ROS ROS Wiki","tags":"Mechanics","url":"robot_operating_system_(ros).html","loc":"robot_operating_system_(ros).html"},{"title":"vim","text":"Modes Key Mode i Insert ESC Normal Navigation Key Movement h ← (left) j ↓ (down) k ↑ (up) l → (right) Hint: j looks like an arrow pointing down Editing Key Action x Delete (cut) char under cursor dd Delete (cut) current line p Paste Help :help [command] : Get help (without command: general help) See Also Learn Vim Progressively","tags":"Programming","url":"vim.html","loc":"vim.html"},{"title":"Eigenvectors and Eigenvalues","text":"An eigenvector of a linear transformation is a vector (non-zero) that, when the linear transformation is applied to it, changes by only a scalar factor. This scalar factor is called eigenvalue. $$A \\cdot \\overrightarrow{v} = \\lambda \\cdot \\overrightarrow{v}$$ Where: \\(A\\) : Transformation Matrix \\( \\overrightarrow{v}\\) : Eigenvector \\(\\lambda\\) : Eigenvalue To calculate the Eigenvectors we need to find the Eigenvalues first. Finding the Eigenvalues From $$A \\cdot \\overrightarrow{v} = \\lambda \\cdot \\overrightarrow{v} \\rightarrow A \\cdot \\overrightarrow{v} = \\lambda \\cdot I \\cdot \\overrightarrow{v}$$ follows $$(A - \\lambda I) \\cdot \\overrightarrow{v} = 0$$ which is only solvable if \\(det(A - \\lambda I) = 0\\) (characteristic equation). This means \\((A - \\lambda I)\\) not invertible. The solution of the characteristic equation ( \\(det(A - \\lambda I) = 0\\) ) are the eigenvalues. Eigenvectors There is one independent Eigenvector for each Eigenvalue. For each Eigenvalue solve $$(A - \\lambda I) \\cdot \\overrightarrow{v} = 0$$ to get the corresponding Eigenvector \\(\\overrightarrow{v}\\) for a given Eigenvalue \\(\\lambda\\) .","tags":"Mathematics","url":"eigenvectors_and_eigenvalues.html","loc":"eigenvectors_and_eigenvalues.html"},{"title":"Electrical Motors","text":"This page is work in progress AC Motors High power, single- or multi-phase, constant torque and speed, large loads DC Motors Normal DC motors almost linear characteristics: rotation speed determined by applied DC voltage output torque determined by current Speed: few rpm (revolutions per minute) to many thousands rpm Brushed Motor Cheap, small, easy to control Brushless Motors Use Hall effect switches for stator field rotation sequence, smaller, more efficient, better torque/speed characteristics, more expensive than brushed motors Servo Motors Brushed motor with positional feedback control, can be controlled by PWM . A DC ( RC ) Servo Motor consists of a DC motor, gearbox, position feedback device and error correction (closed loop control). Stepper Motor Accurate positioning, fast response to starting, stoping and reversing.","tags":"Mechanics","url":"electrical_motors.html","loc":"electrical_motors.html"},{"title":"Twist, Screw and Wrench","text":"Screw A rigid body can be moved from one position to any other by rotation around a line (vector) and translation along that line. This is called a screw motion. The concept of screw can be applied to motion (twist) or to forces and momentum (wrench). Twist The instantaneous spatial velocity of a rigid body in terms of its linear and angular components is called a twist. It can be represented as a point in \\(\\mathbb{R}&#94;6\\) , defined by 3 angular and 3 linear velocities. A twist can be seen as an infinitesimal screw motion. Any configuration of a rigid-body can be achieved by starting from a fixed point (reference frame) and integrating a constant twist for a specified time (Exponential Coordinates). Such a motion resembles the motion of a screw, rotating about and translating along the same fixed axis. Wrench A system of forces acting on a rigid body can be replaced by a single force along a line and a torque about that line. These forces are referred to as wrench. Many theorems that apply to twists can be extended to wrenches. Similar to angular and linear velocities are packed into a vector in \\(\\mathbb{R}&#94;6\\) (twist), moments (torques) and forces are packed together into a vector in \\(\\mathbb{R}&#94;6\\) called wrench ( spacial forces ). Literature Notes taken from: Modern Robotics: Mechanics, Planning, and Control by Kevin M. Lynch and Frank C. Park, Cambridge University Press, 2017 StackExchange: Physics","tags":"Mechanics","url":"twist,_screw_and_wrench.html","loc":"twist,_screw_and_wrench.html"},{"title":"Pose and Position","text":"This page is work in progress Position Position of point \\(B\\) relative to point \\(A\\) : $$\\mathbf{r}_{AB}$$ In 3D space positions are represented by vectors \\(\\mathbf{r} \\in \\mathbb{R}&#94;3\\) It is necessary to define a reference frame \\(A\\) and to express the vector in this frame: $$\\mathcal{A}&#94;\\mathbf{r}AB$$ With: \\(\\mathcal{A}\\) : Frame \\(\\mathcal{A}\\) is used to express position vector \\(\\mathbf{r}_{AB}\\) \\(\\mathbf{r}_{AB}\\) : Position vector of point \\(B\\) with with respect to the origin of frame \\(\\mathcal{A}\\) The unit vectors \\(\\left ( \\mathbf{e}&#94;\\mathcal{A}_x, \\mathbf{e}&#94;\\mathcal{A}_y, \\mathbf{e}&#94;\\mathcal{A}_z \\right )\\) of frame \\(\\mathcal{A}\\) form an ortho-normal basis of \\(\\mathbb{R}&#94;3\\) . An alternative notation: $${}&#94;A \\mathbf{p}_B$$ Meaning: Vector \\(\\mathbf{p}\\) to \\(B\\) with respect to coordinate frame \\({A}\\) . (The bound vector \\(\\mathbf{p}\\) pointing from the origin of \\({A}\\) to the point \\(B\\) , Where everything is in the coordnate frame \\({A}\\) ). Representation of Positions Cartesian Coordinates $$\\mathbf{\\chi}_{Pc} = \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}$$ Where \\(\\mathbf{\\chi}_P\\) are the stacked parameters of the position representation. The position vector is given by $$\\mathcal{A}&#94;\\mathbf{r} = x\\mathbf{e}&#94;\\mathcal{A}_x + y\\mathbf{e}&#94;\\mathcal{A}_y + z\\mathbf{e}&#94;\\mathcal{A}_z = \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}$$ Where: \\(\\mathbf{e}&#94;\\mathcal{A}_i\\) : Unit vector of dimension \\(i\\) in frame \\(\\mathcal{A}\\) Cylindrical coordinates $$\\mathbf{\\chi}_{Pz} = \\begin{pmatrix} \\rho \\\\ \\theta \\\\ z \\end{pmatrix}$$ A position vector is given by $$\\mathcal{A}&#94;\\mathbf{r} = \\begin{pmatrix} \\rho \\cos \\theta \\\\ \\rho \\sin \\theta \\\\ z \\end{pmatrix}$$ Spherical coordinates $$\\mathbf{\\chi}_{Pz} = \\begin{pmatrix} r \\\\ \\theta \\\\ \\phi \\end{pmatrix}$$ A position vector is given by $$\\mathcal{A}&#94;\\mathbf{r} = \\begin{pmatrix} r \\cos \\theta \\sin \\phi \\\\ r \\sin \\theta \\sin \\phi \\\\ r \\cos \\phi \\end{pmatrix}$$ Where: \\(\\theta\\) : azimuthal angle \\(\\phi\\) : polar angle (sometimes \\(\\varphi\\) ) Pose To fully describe the configuration of a rigid body (pose) a position and a rotation is needed. A pose \\(\\xi\\) in 2D has 3 parameters \\((x, y, \\theta)\\) . It can be considered as the motion (translation and rotation) of a coordinate frame. The pose of a coordinate frame can be described with respect to another coordinate frame: $${}&#94;A\\xi_B$$ Meaning: Pose \\(\\xi\\) of coordinate frame \\({B}\\) with respect to frame \\({A}\\) . Sources Notes taken from: Robot Academy","tags":"Mechanics","url":"pose_and_position.html","loc":"pose_and_position.html"},{"title":"Practical C++ Metaprogramming","text":"Notes taken from: Practical C++ Metaprogramming Edouard Alligand and Joel Falcou O'Reilly Media, Inc. My repository with examples on Github Chapter 1. Introduction The Early History of Metaprogramming \" LISP macros were able to be used to extend the languages from within.\" \" [In C/C++] An X-macro is, in fact, a header file containing a list of similar macro invocations—often called the components—which can be included multiple times. Each inclusion is prefixed by the redefinition of said macro to generate different code fragments for the same list of components.\" Enter C++ Templates \" It was quickly discovered that by supporting partial specialization, compile-time equivalents of recursion or conditional statements were feasible.\" \" We could turn templates into a very crude and syntactically impractical functional language, which [is] Turing-complete.\" \" Applications of C++ template metaprogramming includes the following: Complex constant computations Programmatic type constructions Code fragment generation and replication\" \" Those applications are usually backed up by some libraries, like Boost. MPL or Boost.Fusion, and a set of patterns including tag dispatching, recursive inheritance, and SFINAE .\" Chapter 2. C++ Metaprogramming in Practice The std::tuple<> pattern \" \"return a bunch of otherwise unrelated stuff.\" This is a common pattern in modern C++\" Returning auto` \" In template metaprogramming, there is no iterative construct.\" \" You can, however, use recursion to apply a callable on every member of the tuple.\" \" Whenever you can, you should use the ... operator to apply a callable to every member of a list. This is faster, it doesn't generate all the unneeded intermediate types, and the code is often more concise.\" std :: index_sequence : \"The trick is to create an index sequence - whose sole purpose is to give us an index on which to apply the ... operator - of the right size. This is done as follows:\" static const std :: size_t params_count = sizeof ...( Params ); std :: make_index_sequence < params_count > (); \" At compile time, when you need to know how many elements you have in your list, you use sizeof…().\" Chapter 3. C++ Metaprogramming and Application Design Meta-Axiom #1 \" Types are first-class values inside compile-time programs.\" Meta-Axiom #2 \" Any template class accepting a variable number of type parameters can be considered a type container.\"","tags":"Programming","url":"practical_cpp_metaprogramming.html","loc":"practical_cpp_metaprogramming.html"},{"title":"Rotation Matrix","text":"This page is work in progress Rotation The configuration of a point is fully described by a position, bodies additionally require a rotation to define their pose. A rotation \\(R\\) in 3D is specified by a rotation angle \\(\\theta\\) and a unit vector u (the rotation axis ). Frames Space frame: \\(\\{s\\}\\) Body frame: \\(\\{b\\}\\) Expressing orientation of \\(\\{b\\}\\) relative to \\(\\{s\\}\\) : writing the unit coordinate axis of \\(\\{b\\}\\) as column vectors in the coordinates of \\(\\{s\\}\\) Form a matrix from the column vectors Example: $$\\begin{matrix} \\hat{x}_b = \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix} & \\hat{y}_b = \\begin{bmatrix} -1\\\\ 0\\\\ 0 \\end{bmatrix} & \\hat{z}_b = \\begin{bmatrix} 0\\\\ 0\\\\ 1 \\end{bmatrix} \\end{matrix}$$ Where: \\(\\hat{x}_b\\) , \\(\\hat{y}_b\\) , \\(\\hat{z}_b\\) : Unit vectors of \\(\\{b\\}\\) represented in coordinates of \\(\\{s\\}\\) Combined unit vectors: $$R_{sb}=\\begin{bmatrix} \\hat{x}_b & \\hat{y}_b & \\hat{z}_b \\end{bmatrix} = \\begin{bmatrix} 0 & -1 & 0\\\\ 1 & 0 & 0\\\\ 0 & 0 & 1 \\end{bmatrix}$$ Where: \\(R_{sb}\\) : Subscript \\(s\\) : Reference frame Subscript \\(b\\) : Frame whose orientation is being represented Constraints There are only \\(3\\) dimensions for orientation of a rigid body in space. But the \\(3 \\times 3\\) rotation matrix has \\(9\\) numbers. So \\(6\\) constraints are required: All \\(3\\) column vectors are unit vectors Dot product of any \\(2\\) column vectors is zero (they are all mutually orthogonal to each other) These 6 constraints can also be written as: $$R&#94;TR = I$$ Where: \\(I\\) : Identity matrix $$I = \\begin{bmatrix} 1 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix}$$ Furthermore for right-handed frames the following must hold: $$\\det R = 1$$ This means that all rotation matrices are elements of the Special Orthogonal Group \\(SO(n)\\) . Special Orthogonal Group SO (2) and SO (3) The Special Orthogonal Group \\(SO(2)\\) and \\(SO(3)\\) is the set of all possible \\(2 \\times 2\\) or \\(3 \\times 3\\) real matrices \\(R\\) that satisfy: $$R&#94;TR = RR&#94;T = I$$ $$det R = 1$$ We refer to \\(SO(3)\\) as the rotation group of \\(\\mathbb{R}&#94;3\\) The rotation group \\(SO(3)\\) is referred to as the configuration space of the system and a trajectory of the system is a curve \\(R(t) \\in SO(3)\\) for \\(t \\in [0,T]\\) . Because a rotation matrix \\(R\\) lives in \\(SO(3)\\) , there is no numerical equivalent to a position such as angular position . Properties of Rotation Matrices These properties hold for \\(SO(2)\\) and \\(SO(3)\\) . Inverse: \\(R&#94;{-1} = R&#94;T \\in SO(n)\\) \\(R R&#94;{-1} = R&#94;{-1} R = I\\) Closure: \\(R_1 R_2 \\in SO(n)\\) Associative \\((R_1 R_2) R_3 = R_1 (R_2 R_3)\\) Not commutative: \\(R_1 R_2 \\neq R_2 R_1\\) Identity element: \\(R I = I R = R\\) Composition Rule for Rotations (Combining by matrix multiplication): \\(R_{ac} = R_{ab} R_{bc}\\) (subscript cancellation) Rotating a vector doesn't change its length: \\(x \\in \\mathbb{R}&#94;3, \\left \\| Rx \\right \\| = \\left \\| x \\right \\|\\) A rotation matrix (in 2D) has 3 eigenvectors: One real eigenvector corresponding to to eigenvalue \\(1\\) Two complex eigenvectors with eigenvalues: \\(\\lambda = \\cos \\theta \\pm j \\sin \\theta\\) where \\(\\theta\\) is the rotation angle Composition of Rotations The coordinates of a vector \\(\\mathbf{u}\\) can be mapped from frame \\(B\\) to frame \\(A\\) by writing: $${}_A\\mathbf{u} = R_{AB} \\cdot {}_B\\mathbf{u}$$ The vector \\(\\mathbf{u}\\) can also be mapped from frame \\(C\\) to frame \\(C\\) by writing: $${}_B\\mathbf{u} = R_{BC} \\cdot {}_C\\mathbf{u}$$ Combining these equations: $$\\begin{align*} {}_A\\mathbf{u} &= R_{AB} \\cdot (R_{BC} \\cdot {}_C\\mathbf{u}) \\\\ &= R_{AC} \\cdot {}_C\\mathbf{u} \\end{align*}$$ The resulting rotation matrix \\(R_{AC} = R_{AB} \\cdot R_{BC}\\) (s.a. subscript cancellation) can be interpreted as the rotation obtained by rotating frame \\(A\\) until it coincides with frame \\(B\\) , and then rotating frame \\(B\\) until it coincides with frame \\(C\\) . Uses of Roatation Matrices There are three uses for a rotation matrix: To represent an orientation To change the reference frame in which a vector or a frame is represented (operator for passive rotation) To rotate a vector or a frame (operator for active rotation) In the first use, \\(R\\) is thought of as representing a frame; in the second and third uses, \\(R\\) is thought of as an operator that acts on a vector or frame. Representing Orientation \\(R_c\\) means implicit the orientation of frame \\({c}\\) relative to the fixed reference frame \\({s}\\) . Explicit notation would be \\(R_{sc}\\) . \\(R_c = R_{sc}\\) Properties: \\(R_{ac} R_{ca} = I\\) \\(R_{ac} = R_{ca}&#94;{-1} = R_{ca}&#94;T\\) Changing the reference Frame (Passive Rotation) $$R_{ac} = \\underbrace{R_{ab}}_{operator} \\cdot \\overbrace{R_{bc}}&#94;{orientation}$$ Meaning: \\(R_{ac} = change\\_reference\\_frame\\_from\\_b\\_to\\_a(R_{bc})\\) \\(R_{bc}\\) can be viewed as a representation of the orientation of \\({c}\\) . \\(R_{ab}\\) can be viewed as a mathematical operator that changes the reference frame from \\({b}\\) to \\({a}\\) . Subscript cancellation rule: \\(R_{ab} R_{bc} = R_{a\\not{b}} R_{\\not{b}c} = R_{ac}\\) The reference frame of a vector can also be changed by a rotation matrix: \\(R_{ab} p_b = R_{a\\not{b}} p_{\\not{b}} = p_a\\) Rotating a vector or a frame (Active Rotation) \\(R\\) as a rotation operator can be written: $$R = Rot(\\hat{\\omega}, \\theta)$$ Elementary Rotations $$Rot(\\hat{x}, \\theta) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\cos \\varphi & -\\sin \\varphi \\\\ 0 & \\sin \\varphi & \\cos \\varphi \\end{pmatrix}$$ $$Rot(\\hat{y}, \\theta)= \\begin{pmatrix} \\cos \\varphi & 0 & \\sin \\varphi \\\\ 0 & 1 & 0 \\\\ -\\sin \\varphi & 0 & \\cos \\varphi \\end{pmatrix}$$ $$Rot(\\hat{z}, \\theta) = \\begin{pmatrix} \\cos \\varphi & -\\sin \\varphi & 0\\\\ \\sin \\varphi & \\cos \\varphi & 0 \\\\ 0 & 0 & 1 \\\\ \\end{pmatrix}$$ General form of Rotation $$Rot(\\hat{\\omega}, \\theta) = \\begin{bmatrix} \\cos \\theta + \\hat{\\omega}_1&#94;2(1-\\cos \\theta) & \\hat{\\omega}_1 \\hat{\\omega}_2(1-\\cos \\theta) - \\hat{\\omega}_3 \\sin \\theta & \\hat{\\omega}_1 \\hat{\\omega}_3(1-\\cos \\theta) - \\hat{\\omega}_2 \\sin \\theta \\\\ \\hat{\\omega}_1 \\hat{\\omega}_2(1-\\cos \\theta) - \\hat{\\omega}_3 \\sin \\theta& \\cos \\theta + \\hat{\\omega}_2&#94;2(1-\\cos \\theta) & \\hat{\\omega}_2 \\hat{\\omega}_3(1-\\cos \\theta) - \\hat{\\omega}_1 \\sin \\theta\\\\ \\hat{\\omega}_1 \\hat{\\omega}_3(1-\\cos \\theta) - \\hat{\\omega}_2 \\sin \\theta& \\hat{\\omega}_2 \\hat{\\omega}_3(1-\\cos \\theta) - \\hat{\\omega}_1 \\sin \\theta& \\cos \\theta + \\hat{\\omega}_3&#94;2(1-\\cos \\theta) \\end{bmatrix}$$ with: \\(\\hat{\\omega} = (\\hat{\\omega}_1, \\hat{\\omega}_2, \\hat{\\omega}_3)\\) Properties Any \\(R \\in SO(3)\\) can be obtained by rotating from the identity matrix \\(I\\) by some \\(\\theta\\) about some \\(\\hat{\\omega}\\) \\(Rot(\\hat{\\omega},\\theta) = Rot(-\\hat{\\omega},-\\theta)\\) Reference Frame for Rotation When rotating a frame \\({b}\\) by \\(Rot(\\hat{\\omega}, \\theta)\\) and \\(R_{sb}\\) represents the orientation of \\({b}\\) relative to \\({s}\\) , it's important to express if the rotation axis \\(\\hat{\\omega}\\) is defined in \\({s}\\) or in \\({b}\\) coordinates . Given \\(R = Rot(\\hat{\\omega}, \\theta)\\) : Rotate by \\(R\\) in \\({s}\\) frame: \\(R_{s{b}'} = RR_{sb}\\) Premultiplying by \\(R\\) : \\(\\hat{\\omega}\\) considered in the fixed frame Rotate by \\(R\\) in \\({b}\\) frame: \\(R_{s{b}''} = R_{sb}R\\) Postmulitplying by \\(R\\) : \\(\\hat{\\omega}\\) considered in the body frame. To rotate a vector \\(v\\) , there is only one frame involved. \\(\\hat{\\omega}\\) has to be interpreted as being in the frame in which \\(v\\) is represented: $${v}' = Rv$$ Literature Notes taken from: Modern Robotics: Mechanics, Planning, and Control by Kevin M. Lynch and Frank C. Park, Cambridge University Press, 2017 A Mathematical Introduction to Robotic Manipulation by Richard M. Murray, Zexiang Li and S. Shankar Sastry, CRC Press, 1994","tags":"Mechanics","url":"rotation_matrix.html","loc":"rotation_matrix.html"},{"title":"Mechanical Constraints","text":"Pfaffian Constraints A Pfaffian constraint is a set of \\(k\\) ( \\(k \\leq n\\) ) linearly independent constraints so that: $$A(q)\\dot{q} = 0$$ Where: \\(A(q) \\in \\mathbb{R}&#94;{k \\times n}\\) \\(\\dot{q}\\) : derivative of \\(q\\) with respect to time \\(k\\) : number of constraints \\(n\\) : number of variables needed to define robots configuration (implicit representation) Holonomic Constraints A holonomic mechanical system can move in arbitrary directions (in its C-space) Holonomic constraints can be defined independent of \\(\\dot{q}\\) (i.e. \\(f(q,t)= 0\\) ) Holonomic constraints reduce the dimension of the C-space (geometric limitation) C-space can be viewed as a surface of dimension \\(n-k\\) embedded in \\(\\mathbb{R}&#94;n\\) \\(n\\) : number of variables to define robots configuration \\(k\\) : independent holonomic constraints \\(n-k\\) : dimension of C-space, degree of freedom Integrable constraints: Kinematic constraints may be integrable. In this case, the constraints are geometric constraints the velocity constraints that they imply can be integrated to give equivalent configuration (geometric) constraints Holonomic constraints \\(g(q(t)) = 0\\) can be differentiated with respect to \\(t\\) to yield: $$\\frac{\\partial g}{\\partial q}(q)\\dot{q}=0$$ Nonholonomic Constraints A nonholonomic mechanical system cannot move in arbitrary directions (in its C-space) Constraints can not be integrated They reduce the dimension of the feasible velocities of the system (kinematic/velocity limitation) They do not reduce the dimension of the reachable C-space e.g. rolling without slipping, differential drive, car (Ackermann steering)","tags":"Mechanics","url":"mechanical_constraints.html","loc":"mechanical_constraints.html"},{"title":"Space Topology","text":"Space Topology Euclidian space: \\(\\mathbb{E}&#94;n\\) or \\(\\mathbb{R}&#94;n\\) Sphere: \\(S&#94;n\\) Torus: \\(T&#94;n\\) ( \\(T&#94;2 = S&#94;1 \\times S&#94;1\\) ) \\(\\mathbb{R}&#94;n\\) is the \\(n\\) -dimensional Euclidian space, \\(S&#94;n\\) the \\(n\\) -dimensional surface of a sphere and \\(T&#94;n\\) is the \\(n\\) -dimensional surface of a torus in a $(n + 1) dimensional space. \\(S&#94;1 \\times S&#94;1 \\times S&#94;1 \\times \\cdots \\times S&#94;1 = T&#94;n\\) for \\(n\\) copies of \\(S&#94;1\\) Some spaces ( c-spaces ) can be represented as Cartesian product of two or more lower dimensional spaces. C-space Cartesian product Explanation Rigid body in plane \\(\\mathbb{R}&#94;2\\times S&#94;1\\) x-y-coordinates and an angle PR robot arm \\(\\mathbb{R}&#94;1\\times S&#94;1\\) prismatic joint: \\(\\mathbb{R}&#94;1\\) , revolute joint: \\(S&#94;1\\) 2R robot arm \\(S&#94;1 \\times S&#94;1 = T&#94;2\\) 2 times revolute joint planar rigid body with 2R robot arm \\(\\mathbb{R}&#94;2 \\times S&#94;1 \\times T&#94;2 = \\mathbb{R}&#94;2 \\times T&#94;3\\) rigid body in 3-D \\(\\mathbb{R}&#94;3\\times S&#94;2\\times S&#94;1\\) 1 point in 3-D, a point on a 2-D sphere and a point an a circle Space Representation A numerical representation is not fundamental as the topology of a space. It involves always a choice. Explicit parametrisation Min. Number of coordinates needed For example Cartesian coordinates \\((x, y, z)\\) latitude/longitude Implicit representation Surface embedded in a higher dimensional space with constraints For example One constraint on three coordinates results in two degrees of freedom (2-D c-space) \\((x, y, z)\\) such that \\(x&#94;2 + y&#94;2 + z&#94;2 = 1\\) Implicit representations don't have singularities but have more numbers than the number of degrees of freedom. Unit quaternion Rotation matrix Literature Notes taken from: Modern Robotics: Mechanics, Planning, and Control by Kevin M. Lynch and Frank C. Park, Cambridge University Press, 2017","tags":"Mechanics","url":"space_topology.html","loc":"space_topology.html"},{"title":"Setup a new Machine","text":"Some notes on what to do to setup a new machine for my development projects. Linux Create new user with given (fixed) uid and gid only when needed for NFS Setup /etc/fstab for NFS mounts Setup /etc/exports and /etc/samba/smb.conf for exported mounts Install: with package manager: mc tmux vim zsh ranger docker kate gcc clang make cmake ninja python3 doxygen graphviz cgdb docker git htop icdiff meld direnv xdg-utils ack fzf: use git and install script (otherwithe key bindings are not setup properly) Restore keys in ~/.ssh/ Restore shell config files (.bashrc, .zshrc, .commonrc?) and oh-my-zsh Restore vim config (github) Restore tmux config (github) Resore .gitconfig , set git config --global --add user.useConfigOnly true (https://collectiveidea.com/blog/archives/2016/04/04/multiple-personalities-in-git)","tags":"Programming","url":"setup_a_new_machine.html","loc":"setup_a_new_machine.html"},{"title":"Configuration Space and Degrees of Freedoms","text":"Joints Every joint connects exactly two links. A joint: provides freedoms to allow one rigid body to move relative to another. provides constraints on the possible motions of the two rigid bodies it connects. \\(dof\\ (of\\ joint) = dof\\ (of\\ rigid\\ body) - number\\ of\\ constraints\\ (of\\ joint)\\) Typical Joints Joint type dof ( \\(f\\) ) Constraints \\(c\\) in 2D Constraints \\(c\\) in 3D Revolute (Scharnier) 1 2 5 Prismatic (Schubgelenk) 1 2 5 Cylindrical (Drehschubgelenk) 2 - 4 Universal (Kardangelenk) 2 - 4 Spherical (Kugelgelenk) 3 - 3 Configuration Space Configuration: a specification of the positions of all points of the robot. Configuration space (C-space): The space of all configurations. The n-dimensional space containing all possible configurations of the robot. The configuration of a robot is represented by a point in its C-space. Degrees of Freedom (dof) Degrees of freedom: smallest number of real-valued coordinates needed to represent its configuration. It's the dimension of the C-space or minimum number of real-valued numbers needed to represent the configuration. $$dof = (sum\\ of\\ freedoms\\ of\\ the\\ bodies) - (number\\ of\\ independent\\ constraints)$$ Degree of freedom depends on: Number of links Number of joints Types of joints Distribution of joints within mechanism It does not depend on length of links. Rigid body (number of degrees of freedom) 2D (planar bodies): 3 3D (spatial bodies): 6 Degrees of freedom in 3D: x y z roll pitch yaw Grübler's Formula \\(N\\) : Number of links (ground is also a link) \\(J\\) : Number of joints \\(m\\) : Degrees of freedom of rigid body (3 in 2D, 6 in 3D) \\(f_i\\) : Number of freedoms of joint \\(i\\) \\(c_i\\) : Number of constraints of joint \\(i\\) where \\(\\forall i;f_i + c_i = m\\) $$\\begin{align*} dof &= \\underbrace{m(N-1)}_{rigid\\ body \\ freedoms} - \\underbrace{\\sum_{i=1}&#94;Jc_i}_{joint\\ constraints}\\\\ &= m(N-1)-\\sum_{i=1}&#94;J(m-f_i)\\\\ &= m(N-1-J)+\\sum_{i=1}&#94;Jf_i \\end{align*}$$ The formula holds only if all constraints for the joints are independent. Literature Notes taken from: Modern Robotics: Mechanics, Planning, and Control by Kevin M. Lynch and Frank C. Park, Cambridge University Press, 2017","tags":"Mechanics","url":"configuration_space_and_degrees_of_freedoms.html","loc":"configuration_space_and_degrees_of_freedoms.html"},{"title":"tmux","text":"Prefix Key The prefix key combination needs to be pressed before any command for tmux (when tmux is running). By default the PREFIX is: CTRL + b Sessions Key Binding Function tmux new -s <name> Create a named session tmux ls List current sessions tmux a Attach to last session tmux a -t <name> Attach to named session tmux kill-session -t <name> Kill named session PREFIX d Detach from a running session PREFIX : Enter command mode PREFIX ) Change to next session PREFIX ( Change to previous session PREFIX s List all sessions Commands can be provided directly to tmux as arguments or can be entered in command line ( PREFIX : ). Commands: Check if a session is running: has-session -t : Target session The Target Flag ( -t ) To specify the target session, window and pane for a command the flag -t <session-name>:<window-id>.<pane-id> is used. <session-name> : The name of the session <window-id> and <pane-id> (optional): id number of window or pane Examples: tmux split-window -v -t mysession Grouped Sessions Grouped sessions allow different users to share a session but work in different windows. This is useful for pair-programming. Create main session: tmux new-session -s mainSession Create new session with main session as target tmux new-session -t mainSession -s subSession Server Sockets Sessions can be created with a custom server socket where other users can connect to. This can also be used for pair-programming. Create session with server socket: tmux -S /var/mysocket Attach to socket tmux -S /var/mysocket attach The socket is created if needed. Additional Flags for tmux new -d : Create in background (detached) -n <name> : Name the first windows Windows Like tabs in browser. Key Binding Function PREFIX c Create a new window PREFIX , Rename current window PREFIX n Move to next window PREFIX p Move to previous window PREFIX <num> Move to window by index PREFIX f Find named window PREFIX w Show menu with all windows PREFIX & Close current window Commands: Create window: new-window -n : name -t : Target session Change to a window: select-window -t : Target session and window Panes Key Binding Function PREFIX % Split pane vertically PREFIX \" Split pane horizontally PREFIX o Cycle through panes PREFIX ← , PREFIX ↑ , PREFIX → , PREFIX ↓ Navigate around panes PREFIX ALT ← , PREFIX ALT ↑ , PREFIX ALT → , PREFIX ALT ↓ Resize panes PREFIX x Close current pane PREFIX q Show number of each pane PREFIX z Maximize/resize pane (toggle) Commands: Create new pane: split-window -v or -h : vertically or horizontally -p Percent of split -t Target Pane Layouts Command: select-layout -t <session> <layout-type> There are following layout types: even-horizontal even-vertical main-horizontal main-vertical tiled Cycle through layouts: PREFIX SPACEBAR Panes and Windows Create new window form current pane: PREFIX ! Join a window (or pane) in other window (even from other session): join-pane -s <session-name>:<window-id>.<pane-id> Specify target: join-pane -s : . -t : . ` Move windows between sessions: PREFIX . Buffers vi mode needs to be activated for these key bindings ( setw -g mode-keys vi ) Key Binding (vi) Function PREFIX [ Enter Copy mode PREFIX ] Paste copied text (top of paste buffer stack) PREFIX = Show all copied buffers for selection Key Binding in Copy mode (vi) Function h , j , k , l Move curse (like in vi ) w , b Move word forward/backward CTRL + b , CTRL + f Page-up, page-down g , G Jump to top/bottom of buffer ? Search backward in buffer / Search forward in buffer n , N Jump to next/previous search result SPACE Select text (move around for selection) ENTER Copy selected text and leave Copy mode Commands: Copy visible content of pane: capture-pane Show the content of paste buffer: show-buffer Store the content of paste buffer in a file: save-buffer [-b buffer-index] <file-name> tmux maintains a stack of paste buffers: Paste buffer 0: PREFIX ] Show all buffers in stack: list-buffers Choose buffer to paste: choose-buffer Send Shell Commands Shell commands can be sent to tmux: tmux send-keys -t <session-name>:<window-id>.<pane-id> '<command>' C-m Arguments and Flags: -t : Target <command> : Can be any shell command C-m : Carriage return (enter, CTRL - M ) Config Personal config file: ~/.tmux.conf A custom config file can be supplied when tmux is started tmux -f <file-name> add source-file ~/.tmux.conf as first line to get settings from default file Reload file: source-file <file-name> Bind commands to keys: bind [-nr] <key> <command0> \\; <command1> ... -n : Don't use PREFIX -r : Command may repeat (hold key) Separate commands by \\; See Also Tmuxinator tmux Productive Mouse-Free Development by Brian P. Hogan","tags":"Programming","url":"tmux.html","loc":"tmux.html"},{"title":"Midnight Commander","text":"Panels Keys Function ALT + u Swap panels ALT + t Switch listing mode (full, brief, long …) ALT + i Show current directory in other panel (synchronize) ALT + , Toggle layout between left-right and top-bottom Navigation Keys Function CTRL + PgUp Change to parent directory ALT + o Open selected directory (or parent directory if file is selected) in other panel ALT + SHIFT + h Show directory history ALT + y Open previous directory from history ALT + u Open next directory from history Searching Search dialog: ALT + ? Quick search: ALT + s (or CTRL + s ) use same shortcut jump to next match wildcards ( * , ? ) can be used File Selections Keys (Alternative) Function INS ( CTRL + t ) Select current file + Select files by pattern \\ Un-select files by pattern * Reverse selection Shell Keys Function ESC Tab Shell completion, 2x for list of possibities, works also in dialog fields CTRL + O Toggle between shell and mc ALT + ENTER Copy current file name to shell Viewing and Editing Keys Function F3 Internal viewer SHIFT + F3 View raw content F4 Edit The editor can be changed: Disable Options > Configuration > Use internal edit Set envronment variable EDIT to preferred editor Function Keys The function keys ( F1 - F10 ) can be emulated by using ESC + 1 - ESC + 0 Wrapper Script ( mc-wrapper ) The mc-wrapper script allows that the shell changes to the current directory when exiting mc . It can be activated by setting an alias. On Linux: alias mc = '. /usr/libexec/mc/mc-wrapper.sh' On macOS (homebrew): alias mc = '. /usr/local/opt/midnight-commander/libexec/mc/mc-wrapper.sh' Terminal Emulator Settings GNOME Terminal (3.28.2) Disable Edit > Preferences > General > Enable the menu accelerator key (F10 by default) Change Edit > Preferences > Shortcuts > Help > Contetns to Shift+Ctrl+F1 Change Edit > Preferences > Shortcuts > Tabs > Switch to Previous Tab to Alt+Page Up Change Edit > Preferences > Shortcuts > Tabs > Switch to Next Tab to Alt+Page Down References Use Midnight Commander like a pro MC Tutorial","tags":"Programming","url":"midnight_commander.html","loc":"midnight_commander.html"},{"title":"Object Oriented Design Principles","text":"General Use Coding conventions Keep it simple, stupid ( KISS ) Boy Scout Rule: \"Always leave the campground cleaner than you found it\" - Robert C. Martin Broken Window Theory : Don't Live with Broken Windows Fix root cause (root cause analysis, no workaraounds): otherwise it will get you again You aren't gonna need it ( YAGNI ) : implement things when you need them Class Design SOLID S : Single responsibility principle (only one reason to change) O : Open/closed principle (open for extension, closed for modification) L : Liskov substitution principle I : Interface segregation principle D : Dependency inversion principle (dependency injection can be used) See SOLID Package Design See Package principles Package Cohesion Reuse-release Equivalence Principle ( REP ) A package must contain reusable classes All of the classes inside the package are reusable (or none of them are) The classes must be of the same family Common-Reuse Principle ( CRP ) Classes that are reused together belong in the same package Common-Closure Principle ( CCP ) A package should not have more than one reason to change Changes to an application shall occur only in one package If classes are tightly coupled, they belong to the same package Package Coupling Acyclic Dependencies Principle ( ADP ) No cycles are allowed in the dependency structure Dependencies form a tree (or DAG ) Stable-Dependencies Principle ( SDP ) Packages that are changed frequently shall not depend on packages that are difficult to change Stable-Abstractions Principle ( SAP ) Stable packages should be abstract, so that it can be easier extended Unstable packages should be concrete, it's easier to change Development Environment and Infrastructure Building the software needs to be possible with just one command Running a single test needs to be possible with just one command Running all tests needs to be possible with just one command Integrate unit tests into build Source control (e.g. git) for everything: source, docs, reference data, tools… Use static and dynamic analysis tools Set highest warning level of compilers, use multiple different compilers Write documentation: Wiki, Doxygen, Markdown… Apply CI / CD , pipeline as code References 97 Things Every Programmer Should Know Encapsulate what varies ( Encapsulation Is Not Information Hiding ) Prefer Composition to inheritance Program to Interface, not Implementation: Liskov substitution principle Cohesion : Objects should only interact with ‘friends' (objects in their neighborhood) Ineracting Objects should aim for loose coupling Tell don't ask : Tell Objects what to do with their data, don't ask for the data to operate on it List of Design Patterns","tags":"Programming","url":"object_oriented_design_principles.html","loc":"object_oriented_design_principles.html"},{"title":"Blender Basics","text":"Right-handed (Rechtshändiges) Cartesian coordinate system. Axes: \\(x\\) : width, red \\(y\\) : depth, green \\(z\\) : height, blue Mesh: vertices (3D points), edges (connects points) and faces (polygon formed by vertices). Texture: 2D images mapped to 3D objects. UV coordinates necessary to project texture onto mesh. Lights (rendering): usually 3 kind of lamps directional light, like sun, hard shadows omnidirectional light, diffuse, illuminating things arround it, soft shadows spots: simulate conical shaped light General Emulate 3 Button Mouse and Emulate Numpad File > User Preferences Emulate 3 Button Mouse: use ALT + LMB as MMB Emulate Numpad: use top row numbers as numpad Viewport (3D View) User Interface Toggle maximised and normal view: SHIFT + TAB Toggle property panel: N Toggle tools panel: T Navigation Mouse Action Shortcut Pan (shifting) Shift + MMB Orbit (rotatong) MMB Zoom Mouse wheel (or CTRL + MMB ) SHIFT + C : Reset 3D cursor to origin Shortcuts for Views Action Shortcut (num pad) Front 1 Back CTRL + 1 Right 3 Left CTRL + 3 Top 7 Bottom CTRL + 7 (De-)activate orthographic mode 5 Camera 0 Orbit View Action Shortcut (num pad) Orbit Right 4 Orbit Left 6 Orbit Up 8 Orbit Down 2 Orbit Opposite 9 Pan View Action Shortcut (num pad) Pan Right CTRL + 4 Pan Left CTRL + 6 Pan Up CTRL + 8 Pan Down CTRL + 2 General Shortcuts Action Shortcut (num pad) Show all objects HOME Zoom on selected object , Zoom in + Zoom out - Toggle local mode / Local mode : Zoom on selected object and hide all other objects Modeling Selection Select object: RMB Select multiple objects: SHIFT + RMB Select complete loop: ALT + RMB (can be combined with SHIFT ) Select (deselect) all: A Box (Border) selection: B LMB ( MMB for deselection) Circle (brush) select: C LMB , scroll for bigger brush ( MMB for deselection) 3D Cursor The 3D cursor is where new objects are placed. Set 3D cursor: LMB Reset 3D cursor to origin: SHIFT + C Add and Delete Objects Delete: x or DEL Add: SHIFT + A Duplicate Element SHIFT + D Use x , y or z to allow moving new object only on given axis. Tranformation Action Shortcut Grab G Rotation R ( R R for free rotation) Scale S The transformations can be reverted: Action Shortcut Revert Grab Alt + G Revert Rotatsion Alt + R Revert Scale Alt + S Each of the transformation hot-key can be combined with x , y or z to allow the trasformation only for this axis. With SHIFT + x , SHIFT + y or SHIFT + z the transformation is allow only for the other axes. General Shortcuts Set parent to: CTRL + P Clear parent: ALT + P Snap menu: SHIFT + S Modes Object Mode : Work with objects as whole Edit Mode : Work with Vertices , Edges and Faces of objects Toggle between Edit Mode and Object Mode : TAB Display Mode Toggle between Solid and Wireframe : Z Edit Mode Mesh select mode ( Vertex , Edge , Face ): CTRL + TAB","tags":"Misc","url":"blender_basics.html","loc":"blender_basics.html"},{"title":"Visualization of Functions","text":"Some notes how to visualize mathematical functions. Domain and Range Input space: domain Output space: range or codomain Graphs (single variable functions) Showing both ‘input space' and ‘output space' limited in dimension used for: single-variable functions multivariable functions with a tow-dimensional input and a one-dimensional output (represent all points of the form \\((x, y, f(x, y))\\) ) Contour Maps Show only the input space outputs values represent should be evenly spaced (easier to understand) useful for functions with a two-dimensional input and a one-dimensional output The Gradient is always perpendicular to the Contour lines Parametric Curves/Surfaces Show only the output space used for functions whose output space has more dimensions than the input space input information is lost Vector Fields Used for functions with same numbers of dimensions in the input space as in the output space Associates a vector with each point in space The vector lengths are usually not drawn to scale (but proportional to each other) Sometimes colours are used to indicate the length of the vectors Transformations Watch (or imagine) how each input point moves to its corresponding output space have to be represented as animations or a schematic drawing useful for gaining conceptual understanding impractical for representing functions precisely Can be used for functions with any dimensions in the input- and output space Plots The plots were made with Octave with this script .","tags":"Mathematics","url":"visualization_of_functions.html","loc":"visualization_of_functions.html"},{"title":"Docker","text":"See also: Docker Documentation . Setup Start the daemon (on Fedora): sudo systemctl start docker Start the daemon on boot (on Fedora): sudo systemctl enable docker Run as user: First create a docker group: sudo groupadd docker Then add the actual user to that group: sudo usermod -aG docker $USER After that log out and log in again. Images and Containers List Images Command: docker images Columns: REPOSITORY : name of the repository on the Docker Hub TAG : the tag (version) of the image IMAGE ID : first 12 digits of the unique (64 bit) image ID CREATED : the creation date SIZE : size of the image Search Images Command: docker search <image-name> Columns: NAME : name of the repository DESCRIPTION : Small description STARS : Showing how many people like the repository OFFICIAL : If it has been approved by the Docker team AUTOMATED : If the build is automanted (on Docker Hub) Pull Images Command: docker pull <image-name> Remove Images Command: docker rmi <image-id> or: docker rmi <image-name>:<tag> Run Image Command: docker run [flags] <image-name>:<tag> [command] Flags: -i : interactive shell -t : pseudo tty -d : run as daemon -p <host_port>:<container_port> : map ports If the interactive ( -i ) flag is provided a command sould be executed. This will override the default command that is run when the container is started. A random name is assigned to the image. Example: docker run -i -t fedora:latest /bin/bash Execute Command in running Container Command docker exec [flags] <image-name>:tag [command] -i : interactive shell -t : pseudo tty -d : run as daemon -w : use working directory Detach from running Container CTRL + p CTRL + q Show Running Images Command: docker ps To show also containers that are not running: docker ps -a Logs Command: docker logs <container-id-or-name> Stopping Containers Shutdown the container: Command: docker stop <container-id-or-name> Force quit the container: Command: docker kill <container-id-or-name> Renaming Containers Command: docker rename <old-name> <new-name> Getting Information from Containers Command: docker stats <container-name> Command: docker top <container-name> Removing Containers Command: docker rm <container-name> Registries There are three kinds of registries: Docker Hub: Hosted registry service by Docker Docker Trusted Registry: Hosted or on premise (backend maintained by Docker) Docker Registry: For running own Docker registry Docker Machine Create Machines Create and manage machines running Docker. Example (creating a VirtualBox machine running Docker): docker-machine create -d virtualbox node1 List Machines Command: docker-machine ls Columns: NAME : node name ACTIVE : docker commands are run on the active node DRIVER : which driver is used (e.g. virtualbox) STATE : if it's running URL : its URL SWARM : if it's part of a Docker Swarm cluster DOCKER : version of Docker ERRORS : any errors that occurred Restart a Node Commmand: docker-machine restart <node-name> Dockerfile Instructions FROM : The base image for this Docker image ( scratch for empty base image). MAINTAINER : Responsible maintainer (name and e-mail). RUN : Install packages and run other commands. ADD : Add files or folders to the Docker image. URLs can be provided. Automatically unpack or untar a compressed files. COPY : Same as ADD but without URL handling or unpacking/untarring. EXPOSE : Expose ports from the image to the outside world. LABEL : Additional information (version number, text …). Each label add a layer to the image. CMD : Execute commands and keep the container alive. ENTRYPOINT : Similar to CMD . Can be used in conjunction with CMD . USER : Specify which username to use. Influences RUN , CMD and ENTRYPOINT instructions that follow in the Dockerfile. WORKDIR : Specify the directory where to execute instructions. Influences RUN , CMD and ENTRYPOINT instructions that follow in the Dockerfile ONBUILD : Used if image is used as base in another Dockerfile. Executed as first statement after FROM . Use in conjunction with ADD or RUN . Label Informations Command: docker inspect <image-id> Best Practices Use a .dockerignore file. Install only necessary packages. Limit the number of layers. Every RUN command adds a layer. Execute only one process per container. Docker build docker build -t <docker-hub-username>/<repository-name> <directory> or docker build -f <path-do-dockerfile> -t <repository>:<tag> <repository> : repository name prefixed by Docker Hub username The .dockerignore file This file can be used to exclude files and folders from being sent to the Docker daemon for the build. It should be placed in the same folder as the Dockerfile .","tags":"Programming","url":"docker.html","loc":"docker.html"},{"title":"QEMU Quickstart","text":"This shows a basic use for QEMU . See also QEMU Documentation QEMU can emulate different processor architectures: qemu-system-arm : ARM qemu-system-mips : MIPS qemu-system-ppc : PowerPC qemu-system-x86 : x86 and x86_64 QEMU emulates lot of hardware for each architecture which can be shown with the option -machine help . Example use: qemu-system-arm -machine vexpress-a9 -m 256M -drive file=rootfs.ext4,sd -net nic -net use -kernel zImage -dtb vexpress-v2p-ca9.dtb -append \"console=ttyAMA0,115200 root=/dev/mmcblk0\" -serial stdio -net nic,model=lan9118 -net tap,ifname=tap0 The most important options are: -machine vexpress-a9 : emulation of ARM Versatile Express development board (with Cortex A-9 processor) -m 256M : 256 MiB RAM -drive file=rootfs.ext4,sd : connect the sd interface to the file rootfs.ext4 (filesystem image) -kernel zImage : loads the Linux kernel from the file zImage -dtb vexpress-v2p-ca9.dtb : loads the device tree from the file vexpress-v2p-ca9.dtb -append \"...\" : supplies this string as the kernel command line -serial stdio : connects the serial port to the terminal that launched QEMU , so you can log via the serial console -net nic,model=lan9118 : creates a network interface -net tap,ifname=tap0 : connects the network interface to the virtual network interface tap0 For configuring the network interface the command tunctl is used: sudo tunctl -u $(whoami) -t tap0 This creates a network interface named tap0 which is connected to the network controller in the emulated QEMU machine. The tunctl command is available from the User Mode Linux ( UML ) project (uml-utilites).","tags":"Programming","url":"qemu_quickstart.html","loc":"qemu_quickstart.html"},{"title":"Sanitizers","text":"Sanitizers Address Sanitizer (and Leak Sanitizer) Compile and link with the flag -fsanitize=address It might be necessary to link with -lasan with older gcc/ld versions. When linking shared libraries (dso's) the sanitizer run-time library ( libasan.so ) is not linked. This hapens only when executables are linked. It can lead to problems with some linker flags. For example -Wl,--no-undefined or -Wl,-z,defs don't work with with sanitizer because the compiler adds sanatizer related code into object files but the linker does not link the run-time library (leading to linker errors due to undefined symbols). To solve this problem either don't use the mentioned linker flags or compile only files needed in executables (and not in shared libraries) with the sanatizer flag. Run-time flags Flags for the sanitizers can be provided with the ASAN_OPTIONS and LSAN_OPTIONS environment variables. They take a colon ( : ) separated list and should be exported in the environment, before running a sanatized executable. Leak Sanitizer and Additional Checks The Leak Sanitizer can be enabled in ASan by setting ASAN_OPTIONS=detect_leaks=1 . This is not needed on Linux as it is enabled by default. Find dynamic initialization order problems (only on Linux): ASAN_OPTIONS=check_initialization_order=1 To find memory use after return add ASAN_OPTIONS=detect_stack_use_after_return=1 . To check for memory use after scope the compiler flag (not runtime oprion) -fsanitize-address-use-after-scope can be used. Symbolizing output To see nice call stacks with readable symbols when the sanitizer finds a problem a symbolizer might be needed. Add this to the sanitizer runtime flags: ASAN_OPTIONS=symbolize=1 and the path to the symbolizer ASAN_SYMBOLIZER_PATH=/usr/local/bin/llvm-symbolizer also the compiler flag to not omit the frame pointer can help: -fno-omit-frame-pointer Preloading In some cases the sanitizer library needs to be preloaded to an executable. Especially if other libraries are preloaded. Then the sanitizer library needs to be first in the list of preloaded libraries: LD_PRELOAD=libasan.so.3:libmylib.dso:$LD_PRELOAD ./myexecutable Note that the version of the sanitizer library (e.g. libasan.so.3 ) needs to be provided if there is no symbolic link libasan.so that points to the right version. To find out the right library name (and the path) one can run ldd path/to/an/executable | grep libasan on an executable that was built with the -fsanitize=address flag For debugging problems with LD_PRELOAD setting the environment variable LD_DEBUG=libs might help. It shows which DSO is loaded from which directory and the order of loading the DSO 's. Custom Signal Handlers If an executable uses custom signal handlers then the signal handlers of ASan need to be disabled: ASAN_OPTIONS=\"allow_user_segv_handler=1:handle_segv=0:handle_abort=0:handle_sigfpe=0\" Suppressions To suppress detection of concrete leaks foud by LSan a suppression file can be provided LSAN_OPTIONS=\"suppressions=$(realpath lsan_suppressions.txt)\" Example of lsan_suppressions.txt leak : FooBar leak : libmylibrary . so leak : myexecutable There is also a similar suppression mechanism for ASan. Virtual Memory Max Map Count When out of memory errors occur when using the address sanatizer it can be of help to increase the number of virtual memory maps that a process can have. sysctl -w vm.max_map_count = 1000000 The number of maps needs to be adjusted per use case (just experiment). The setting is also available at /proc/sys/vm/max_map_count See also: Elasticsearch: Virtual memory Docker To run an sanitized executable in a docker container the argument --cap-add SYS_PTRACE needs to be added to the docker run command. Help and Debugging It's possible to print debug information when using the sanitizer. Also the verbosity of any output can be increased. ASAN_OPTIONS=\"verbosity=2:debug=1:help=1\" For getting help ASAN_OPTIONS=\"help=1\"","tags":"Programming","url":"sanitizers.html","loc":"sanitizers.html"},{"title":"Zeit Management","text":"Allgemeines Bereiche Ziele Übersicht Prioritäten Plan Motivation Ziele Realisierare Ziele Kurz-, mittel- und langfristige Ziele Wo will ich in 5 Jahren sein? (Rückblick auf die letzten 5 Jahre) Übersicht Ziel-Mittel Analyse Welche Mittel stehen zur Verfügung Welche Vor- und Nachteile gibt es bei Alternativen Mind-Map Prioritäten ABC -Analyse A: Sehr wichtig, hohe Priorität B: Mittlere Priorität C: Geringe Priorität Mission Statement Prinzipien Wertvorstellungen längerfristige Ziele Prägnanter Leitsatz (fürs Leben) Aufschreiben! Pareto-Prinzip Mit 20% Aufand werden 80% der Aufgaben erfüllt Eisenhower-Prinzip Unterscheiden zwischen wichtig / unwichtig und dringend / nicht dringend : dringend nicht dringend wichtig sofort erledigen Termin festleden (selber erledigen) unwichtig delegieren => Papierkorb Salami-Taktik / Teile und Herrsche Aufgaben in überschaubare Schritte aufteilen Delegieren Falls jemand die Aufgabe besset/schneller erledigen kann Planen Agenda/Kalender To-Do-Listen Tages-, Wochen-, Monats- und Jahrespläne Schriftlich (auch Ziele) Am Morgen 5-8 min Tag planen Am Abend (vor dem nach Hause gehen): Aufgaben durchgehen, soll-ist Analyse Nächsten Tag planen, v. A. Hauptaufgaben Kleine, unerledigte Aufgaben durchführen (Mails, aufräumen…) ALPEN A ufgaben (Aktivitäten, Termine) in Tagesplan aufschreiben, optimal, wenn am Vortag, Reihenfolge nicht berücksichtigen L änge schätzen, möglichs realistisch, nicht zu knapp, Zeitlimit, genaue Uhrzeit für Termine P ufferzeit: maximal 60% der Zeit verplanen, 40% Puffer E ntscheidungen: Prioritäten setzen, Kürzen, Streichen, Weglassen, Delegieren N achkontrolle: Ende des Tages bzw. der Woche, einzelne Punkte und Planung kontrollieren, vergessene/unvollständige Punkte finden, Planung für nächsten Tag beginnen Leistungskurve Tägliche Leistungskurve einplanen Pausen und Snacks einplanen Goldene Stunde: 1 Stunde am Tag keine Störungen Entlastungsfragen: Warum gerade ich? (-> delegieren) Warum gerade jetzt? (-> verschieben) Warum so (-> rationalisieren/schlanke Lösung) Warum überhaupt (-> eliminieren) Motivation Positive Einstellung Erfolgskontrolle Erfolgsergebnis Belohnung! Motivationslöcher überbrücken 15 min Arbeit, 5 min Pause für harte Tage 3-2-1 Zählen, dann beginnen Einatmen/Ausatmen, dann anfangen Motivationskrisen als Anlass nehmen für Änderungen Wochen-, Monats-, Jahrespläne anpassen Projekte, \"Vielleicht\"/\"Irgendwann\" überarbeiten Prozesse aktualisieren und optimieren Getting Things Done Zuerst anstehende Tätigkeiten, Ziele, Projekte, Verpflichtungen notieren Alle Elemente, die aus mehr als einer Tätigkeit bestehen: Projekte Für Projekte immer den nächsten, machbaren Schritt formulieren auf Kontext (To-Do) Liste setzen Schritt für Schritt First things first Termine und Aufgaben trennen Termine in Kalender Aufgaben in To-Do Listen (thematisch sortiert) 2-Minuten-Regel: alle Tätigkeiten, die kürzer als 2 min dauern, sofort erledigen Projekte/Tätigkeiten, die nicht aktuell sind in \"Vielleicht/Irgendwann\" verschieben Wöchentlich Durchsicht für Termine, Aufgaben, Projekte… Vorgehensweise: Erfassen Alles zusammenlegen und in \"Eingang\" legen \" Eingang\" fortlaufend füllen, täglich leeren Mehrere Eingänge Mailbox Post Papierkram … aber auf so viele wie nötig reduzieren! Durcharbeiten Oben im \"Eingang\" beginnnen Immer ein Element (nur ein) ins System schleusen Nie ein Element zurück in den \"Eingang\" Falls Handlung erforderlich kürzer als 2 min: sofort erledigen delegieren? als nächster Schritt auf To-Do-Liste Falls keine Handlung erforderlich archivieren bzw. \"Vielleicht/Irgendwann\" weg werfen Organisieren: Nächste Schritte Elemente werden zu To-Do-Listen transportiert Projekte Immer nächsten Schritt in Kalender, To-Do-Liste, … \" Warten auf\" Tätigkeiten, die delegiert wurden \" Vielleicht/Irgendwann\" in Wöchentlicher Durchsicht ausmisten bzw. zu aktuellen Projekten hinzufügen Kalender Termine Alles, was sobald wie möglich erledigt werden soll: to-Do-Liste Zen to Done 10 Gewohnheiten Sammeln Durcharbeiten 1-3 Hauptaufgaben pro Tag, am Morgen beginen Immer nur 1 Aufgabe auf einmal! Listen Täglich durchsehen, Wöchentlicher Rück-/Ausblick Wenige Eingangsorte: E-Mail, Ablage, Briefkasten, Eingangs-Liste Wochenrückblick: Monats-/Jahres-Ziele kontrollieren und planen Vereinfachen: aktuelle Projekte sollen mit persönlichen Zielen zu tun haben Morgen- und Abend-Routinen durchführen, Wochen-/Monats-Routinen, Einkaufs-, Einzahluns-, Wasch-Tag… einplanen Leidenschaft finden Gewohnheiten Ändern Methoden zum ändern von Gewohnheiten: Selbstverpflichtung: Vorsatz fassen, Vorsatz öffentlich machen Trainig: ca. 30 Tage um eine Gewohnheit anzutrainieren Motivation. Nicht lange mit Rückschlägen und Motivations-Tiefs aufhalten. Einfach am nächsten Tag weiter machen Fortschritt Protokolieren, Tagesrückblick Unterstützung Belohnung! Oft! Fokus: während Gewohnheitsänderung (30 Tage) voll darauf konzentrieren (Erinnerungen, Poster, …). Ziel vor Augen führen. Positiv denken. Ziele positiv formulieren. Negative Gedanken durch positive ersetzen Alle E-Mails auf einen Account sammeln. Planen Jede Woche \"grosse Brocken\" auflisten \" Grosse Brocken\": wichtige Aufgaben, nicht umbedingt mit ‘grossem' Aufwand verbunden Auch an anderen Aufgaben arbeiten Jeden Tag 1-3 \"grosse Brocken\" am Morgen konsequent erledigen Zunächst 4-6 \"grosse Brocken\" pro Woche \" Grosse Brocken\" in Blöcken von 1-2 h einplanen Max. 60% der Zeit verplanen Jeden Morgen über die Wichtigen Aufgabe entscheiden Wichtige Aufgaben zu Ende bringen, bevor neue Aufgaben angefangen werden E-Mails (und so) erst nach den \"grossen Brocken\" checken Privat: Rückblick/Vorausschau am So Geschäft: Vorausschau am Mo, Rückblick am Do Belohnung, wenn wichtigste Aufgaben erledigt! Handeln Zuerst \"grosse Brocken\", festgelegte Zeitspanne (ca. 30 min, Timer) Vor Beginn Ablenkungen eliminieren: Tisch aufräumen E-Mail Programm zu Browser zu (zumindest unnötige Tabs) Timer stellen (auch zwischen Etappen) Bei Unterbrechungen: kurze Notiz, Dokumente in Inbox => weiter arbeiten an Vorherigem Zusammenreissen, Durchhalten Pausen einplanen Wenn fertig: Belohnen (kurze Pause, Länge vorher festlegen) Startprobleme kleine Teile (Teile und Herrsche) Einfach anfangen (mal 5 min an Aufgabe arbeiten) Belohnung Begeistern, negative Aspekte ignorieren, positive Aspekte und Gelegenheiten einer Aufgabe erkennen und nützen System Kontextlisten (Zuhause, Besorgungen, Telefonate …) Tägliche To-Do-Listen Kalender/ PIM Notizbuch mit 3 Abschnitten Tagesplanung Allgemeine Notizen (Inbox) To-Do-Listen (Kontextlisten, Inbox, Vielleicht/Irgendwann) Ordnung: immer Alles direkt ablegen/aufräumen flache Oberflächen (Tisch, Boden…) müssen leer bleiben Übergang von einer zur nächsten Aufgabe beachten Dinge versorgen, kurze Pause, nächste Aufgabe vorbereiten Ettiketten: Alles beschriften Wochenrückblick In Kalender eintragen: Wochenziele Wochenplanung-/Rückblick Tagesziele Ziele überprüfen Jahres-/Monats-/Wochenziel überprüfen, Notieren (nur 1 Jahresziel) Notizen/Listen durchgehen, neu sortieren Kalender überprüfen, neu ordnen (vergangene Woche, nächste Woche, was wurde verschoben/neu geplant, was kommt die nächste Woche) Listen durchgehen, unwichtigen streichen Wochenziel festlegen Vereinfachen Wir schaffen nie Alles was wir uns vorgenommen haben Überlange To-Do-Listen sind ein Problem Punkte auf Liste halbieren, ein paar Tage später nochmal halbieren Alles was einem seinem (Jahres-)Ziel nicht näher bringt streichen: was hat am meisten Wert Verpflichtungen streiche, „Nein\" sagen, Zusagen zurückziehen (wo möglich) Max. 3 wichtige Aufgaben pro Tag kleinere Aufgaben zusammenfassen, an einem Stück erledigen (ca. 30 min) Unwichtiges von Listen/Kalender… streichen (Ziele beachten) Verpflichtugnen reduzieren, nein sagen, Zusagen zurückziehen Routinen Tägliche und wöchentliche Routinen (Arbeit und Privat) Täglich In-Box leeren, System bearbeiten Listen morgens und abends durchgehen Privates einplanen: Sport… Einkaufstag, Wäschetag, Finanzen-Tag…, festlegen Evtl. Menü-Plan für Woche Kleine Aufgaben sinnvoll zusammenfassen Morgen-/Abendroutinen festlegen (privat & geschäftlich) Routinen (Privat und Geschäftlich) aufschreiben und planen, bis sie gefestigt sind Work/Life-Balance Körper/Gesundheit Sinn/Kultur Familie/Soziales Leistung/Arbeit","tags":"Misc","url":"zeit_management.html","loc":"zeit_management.html"},{"title":"Finance and Capital Markets","text":"This article is work in progress Present Value (Barwert): value of expected income stream as of the valuation date (i.e. present). Concept of time value of money Most common applied model uses compound interest: $$PV = \\frac{C}{(1+i)&#94;n}$$ \\(C\\) : Future amount of money \\(n\\) : Number of compounding periods \\(i\\) : Interest rate $$Assets = Liabilities + Equity$$ Assets: Aktiva (\"Vermögen\") , left side of balance sheet Liabilities: Passiva (\"Schulden\"), right side of balance sheet Equity: Eigenkapital Inflation Price inflation: goods and services get more expensive Monetary inflation: increase in money supply, leads usually to price inflation When people talk about inflation they usually mean price inflation. Bilanz (balance sheet): Aufstellung über Herkunft und Verwendung von Kapital Gegenüberstellung von -Aktiva (assets): Vermögen und Investitionen Passiva (liabilities): Schulden und Finanzierung Bilanzsumme Aktiva ist immer gleich Bilanzsumme Passiva Die Bilanz ist eine Stichtagsbetrachtung Mindestanforderung an eine Bilanz ( OR § 663) Aktiva Passiva Umlaufmittel Fremdkapital Anlagevermögen Eigenkapital Reinverlust Reingweinn => Bilanzsumme => Bilanzsumme Financial Statements There are 3 financial statements: Income statement Balance sheet Cashflow statement Betriebswirtschaftliche Kennzahlen: Kapitalrendite: \\(Gewinn/Eigenkapital x 100\\) EBIT (earnings before interest and tax): Gewinn ohne Zinsen, Steuern und ausserordentichen Anteilen Eigenkapitalquote: \\(Eigenkapital/Bilanzsumme x 100\\) Verschiedene Daten: Valuta: Festsetzung des Kalenderdatums, an dem eine Gutschrift oder Belastung auf einem Konto wirksam bzw. zinswirksam wird. Schlusstag (tradedate): der Tag, an welchem Käufer und Verkäufer ein Geschäft über ein Handelsobjekt (Basiswert) verbindlich an der Börse abschliessen Buchungsdatum: Termin, an dem eine Transaktion durchgeführt bzw. gebucht wird Das Valutadatum kann vor oder nach dem Buchungsdatum liegen Shares and Bonds Share/Stock (Aktie): part owner of company (security: equity) Shares are a split of the equities Market Capitalisation (market price): \\(number of shares x price per share\\) Bond (Anleihe): part lender to company (security: dept) Zero-coupon bond (wikipedia) Securities Security (Wertpapier): a tradable financial asset Going long: buying (buy low, sell high) Going short (Leerverkauf): borrowing the security and selling it (sell high, buy low) Various Terms Down Payment: Anzahlung Loan: Darlehen/Kredit Mortgage: Hypothek, Pfand Depretiation: Abschreibung Nutzenfunktion (utility) Grenznutzen (marginal utility) Umsatz/Erlös (revenue) Gewinn (profit) Investition (investment) Return on Investment","tags":"Finance","url":"finance_and_capital_markets.html","loc":"finance_and_capital_markets.html"},{"title":"GCC , binutils and other developer tools","text":"Work in Progress! ld Flags If the linker is called through the compler the linker flags need to be added with -Wl,<linker-flag> . -z now : Symbols are resolved at program initialization time (instead of lazy, dynamic binding through PLT ) -O1 (linker!) --no-undefined : gives an error if some symbols are not available in the object files at linking stage --no-allow-shlib-undefined : Similar to --no-undefined , but gives an error if symbols are not available in other shared libraries we link against --as-needed : Link only libraries that are needed See also: The new \"—as-needed\" option to the GNU linker Environment Variables LD_BIND_NOW : same as linker flag -z now LD_PRELOAD : Add libraries that are loaded first (can be used to override functionality) LD_DEBUG : Trace functionality of the linker/loader LD_DEBUG_OUTPUT readelf Show which libraries have been linked readelf -a <prog> | grep \"Shared library\" Show the run-time linker readelf -a <prog> | grep \"program interpreter\" objdump Flags -d : disassemble -C : demangle C++ symbols --no-show-raw-insn : Do not print the instruction bytes in disassembled code Link Time Optimization Add -flto flag to compile and link commands. Find Unused Code Add flags -ffunction-sections and -fdata-sections to the compiler invocation. For the linker use the flags --gc-sections and --print-gc-sections . These flags can prevent some optimizations. Output Link Map The link map contains information about object files, symbols, addresses… Add these flags to the linker invocation: --cref -Map name.map Dump Default Linker Script gcc -o /dev/null -xc /dev/null -Wl,--verbose Library Directories Print ld search pathes","tags":"Programming","url":"gcc,_binutils_and_other_developer_tools.html","loc":"gcc,_binutils_and_other_developer_tools.html"},{"title":"Language Implementation Patterns","text":"This page collects notes and citations from the book: Language Implementation Patterns by Terence Parr Some of my examples can be found here Getting Started with Parsing Basic Parsing Patterns \" The act of recognizing a phrase by computer is called parsing .\" \" You can think of grammars as functional specifications or design documents for parsers.\" \" Grammars are more than designs, though. They are actually executable ‘programs' written in a domain-specific language ( DSL ) specifically designed for expressing language structures.\" Identifying Phrase Structure \" Vocabulary symbols (tokens) play different roles like variable and operator. We can even identify the role of token subsequences like expression.\" \" Parse trees are important because they tell us everything we need to know about the syntax (structure) of a phrase.\" \" To parse [is to generate] a two-dimensional parse tree from a flat token sequence.\" Building Recursive-Descent Parsers \" A parser checks whether a sentence conforms to the syntax of a language.\" \" A language is just a set of valid sentences.\" \" A top-down parser […] starts at the top of the parse tree and works its way down to the token leaf nodes.\" \" Recursive-Descent Parser Descent refers to its top-down nature, and Recursive refers to the fact that its functions potentially call themselves.\" \" Nesting in a parse tree begets [(Kind zeugen)] recursion in a recursive-descent parser.\" \" LL (1) The first L means ‘read the input from left to right.' The second L means ‘descend into parse tree children from left to right.'\" Parser Construction Using a Grammar DSL \" [We can use] a DSL specifically designed for describing languages. ‘Programs' in this DSL are called grammars . Tools that translate grammars to parsers are called parser generators .\" \" Grammars are […] functional specifications for languages.\" \" Substructures in the parse tree and functions in the parser correspond to rules in a grammar.\" \" [Lexer:] combining input characters into vocabulary symbols (tokens).\" Tokenizing Sentences \" Recognizers that feed off character streams are called tokenizers or lexers \" \" At the character level, we refer to syntax as the lexical structure .\" \" Grammars describe language structures, and so we can also use them for lexical specifications.\" \" Lexical rules start with an uppercase letter\" \" Lexer and parser design patterns […] are nearly identical. […] The only difference lies in the type of their input symbols, characters or tokens.\" Pattern 1: Mapping Grammars to Recursive-Descent Recognizers \" Even when building lexers and parsers by hand, the best starting point is a grammar.\" \" Left recursion results in an infinite method invocation loop.\" \" The following rule yields a parser that does not terminate: \" r : r X ; \" we'd end up with a function that immediately called itself\" void r () { r (); match ( X ); } \" Besides left-recursive rules, there are other grammar constructs that yield nondeterministic recursive-descent recognizers. A nondeterministic recognizer cannot decide which path to take.\" \" A grammar, G, is a set of rules from which we generate a class definition (in any object-oriented programming language) containing a method for each rule\" \" For each rule, r, defined in a grammar, we build a method of the same name\" \" Token references for token type T become calls to match(T) . match is a support method in Parser that consumes a token if T is the current lookahead token. If there is a mismatch, match throws an exception.\" Alternatives \" Alternatives become either a switch or an if-then-else sequence\" switch ( < lookahead - token > ) { case < token1 - predicting - alt1 > : case < token2 - predicting - alt1 > : < match - alt1 > break ; case < token1 - predicting - alt2 > : case < token2 - predicting - alt2 > : < match - alt2 > break ; case < token1 - predicting - altN > : case < token2 - predicting - altN > : < match - altN > break ; default : < throw - exception > } Optional subrule (T)? if ( < lookahead - is - T > ) { match ( T );} // no error else clause One or more (...)+ subrules do { < code - matching - alternatives > } while ( < lookahead - predicts - an - alt - of - subrule > ); Zero or more (...)* subrules while ( < lookahead - predicts - an - alt - of - subrule > ) { < code - matching - alternatives > } Pattern 2: LL (1) Recursive-Descent Lexer \" Lexers derive a stream of tokens from a character stream by recognizing lexical patterns. Lexers are also called scanners , lexical analyzers , and tokenizers .\" \" This pattern can recognize nested lexical structures such as nested comments\" \" The goal of the lexer is to emit a sequence of tokens. Each token has two primary attributes: a token type (symbol category) and the text associated with it.\" Pattern 3: LL (1) Recursive-Descent Parser \" It's the weakest form of recursive-descent parser but the easiest to understand and implement.\" \" To make parsing decisions, the parser tests the current lookahead token against the alternatives' lookahead sets. A lookahead set is the set of tokens that can begin a particular alternative.\" \" Formally, we compute lookahead sets using two computations: FIRST and FOLLOW . In practice, though, it's easier to simply ask ourselves, ‘What tokens can possibly start phrases beginning at this alternative?'\" Deterministic Parsing Decisions \" LL parsing decisions work only when the lookahead sets predicting the alternatives are disjoint\" \" If the lookahead sets overlap, though, the parser is nondeterministic -it cannot determine which alternative to choose.\" \" Building an LL (1) parser is the easiest way to learn about parsers. In practice, though, we really need more than a single token of lookahead.\" Pattern 4: LL (k) Recursive-Descent Parser \" The strength of a recursive-descent parser depends entirely on the strength of its lookahead decisions.\" \" Having more lookahead is like being able to see farther down multiple paths emanating from a fork in a maze. The farther we can see ahead, the easier it is to decide which path to take. More powerful parsing decisions make it easier to build parsers.\" \" For example, we want to recognize input such as [a, b=c, [d,e]] \" list : '[' elements ']' ; elements : element ( ',' element )* ; element : NAME '=' NAME | NAME | list ; \" element [is] non- LL (1) since the first two alternatives start with the same NAME token.\" \" The lookahead depth k in LL (k) is really a maximum not the exact, fixed amount of lookahead each parsing decision uses.\" Enhanced Parsing Patterns Parsing with Arbitrary Lookahead \" [Some] language constructs […] only differ on the right side. For example, C++ function definitions and declarations are identical until the parser sees ; or { :\" void bar () {...} // a function definition void bar (); // a function declaration \" function headers can be arbitrarily long, the distinguishing token does not appear at a fixed lookahead position from the left side of the statement.\" \" The parser can speculatively parse as far ahead as it needs.\" \" Speculatively matching the alternatives of a rule effectively orders them. The first alternative that matches wins. This is great because we can use ordering to specify precedence.\" \" With ordered alternatives, there is no ambiguity because the parser consistently chooses the first of two ambiguous alternatives. By having the parser pay attention to the order of alternatives\" \" Though speculative parsing has a lot of advantages, there are two drawbacks. First, it can make debugging more difficult. When the parser speculatively parses ahead, it's easy to get lost with all of the scanning ahead and rewinding. Second, backtracking can be extremely slow. Fortunately, we can fix the efficiency issue.\" Parsing like a Pack Rat \" Almost by definition, we use backtracking parsers only when we need to distinguish between similar language constructs. If the constructs are similar, the associated grammar likely contains repeated references to the same rule.\" Directing the Parse with Semantic Information \" The parsers we're working with in this book recognize context-free languages . A context-free language is a language whose constructs don't depend on the presence of other constructs. Unfortunately, some programming languages have context-sensitive phrases. To handle context-sensitive phrases with a context-free parser, we have to predicate alternatives. In effect, a predicate is just a run-time boolean test that says when it's OK to match an alternative. The predicates gate the associated alternatives in and out.\" \" In C++, the expression T(6) is either a function call or a constructor-style typecast depending on whether T is a function or type name. A C++ parser literally does not know how to interpret T(6) without seeing the definition of T . Such a construct is context sensitive and, in this case, ambiguous from a purely syntactic point of view.\" \" Ambiguous grammars lead to nondeterministic parsers \" Pattern 5 Backtracking Parser \" This pattern adds speculative parsing support (arbitrary lookahead) to any recursive-descent recognizer.\" \" […] we can't map all grammars to recursive-descent parsers. Only non-left-recursive grammars work (no rule can directly or indirectly invoke itself without consuming a token).\" \" […] we can't always get properly functioning (deterministic) parsers even from non-left-recursive grammars. The problem is that fixed lookahead LL parsers need the lookahead sets predicting alternatives to be disjoint.\" \" This pattern overcomes this lookahead issue by allowing arbitrary lookahead […]. To look arbitrarily ahead, we need infrastructure to support backtracking. Backtracking also gives us a way to specify the precedence of ambiguous rule alternatives (alternatives that can match the same input). Backtracking parsers, by definition, try the alternatives in order.\" \" Syntactic predicates are grammar fragments that specify the lookahead language predicting an alternative.\" \" ANTLR 's notion of grammars plus syntactic predicates [are called] Parsing Expression Grammars (PEGs)\" \" In the functional language world, syntactic predicates are called parser combinators \" \" Syntactic predicates and speculative parsing are extremely useful when parsing phrases that look the same from the left edge. Distinguishing between C++ function definitions and declarations is a prime example\" Dealing with Actions While Backtracking \" Either we disallow actions or disallow actions with side effects, or we parse winning alternatives twice.\" \" During speculation, all actions are off. Once the parser knows an alternative will match, however, it can match the alternative again \"with feeling\" to do the actions.\" Pattern 6 Memoizing Parser \" This pattern records partial parsing results during backtracking to guarantee linear parsing performance, at the cost of a small amount of memory.\" \" Memoizing is a form of dynamic programming\" \" Another name for memoizing recursive-descent parser is packrat parser \" \" Memoization only helps us, though, if we invoke the same rule at the same input position more than once.\" \" For example, upon input (3+4); , a backtracking parser derived from the following rule invokes expr twice:\" s : expr '!' // assume backtracking parser tries this alternative | expr ';' // and then this one ; expr : ... ; // match input such as \"(3+4)\" \" Rule s invokes expr to speculatively match the first alternative. expr succeeds, but s finds that the next input symbol is ; and not ! . Rule s rewinds the input and tries the second alternative. The parser immediately calls expr again and at the same input position. […] To avoid reparsing, all we have to do is remember that expr succeeded the last time we tried it at this position.\" \" Packrat parsers are guaranteed to have linear performance [and also] linear space complexity.\" Pattern 7 Predicated Parser \" This pattern augments any top-down parser with arbitrary boolean expressions that help make parsing decisions.\" \" These boolean expressions are called semantic predicates and specify the semantic applicability of an alternative. Predicates that evaluate to false effectively \"turn off\" a parser decision path. From a grammar point of view, false predicates make alternatives invisible.\" \" We need semantic predicates when the parser cannot use syntax alone to make parsing decisions, that is, when the parser cannot distinguish between alternatives without using run-time information. The most common case is when we need to use symbol table information to guide the parse.\" \" Predicates are also useful when a parser must recognize multiple versions of an input language. For example, the GCC C compiler adds a number of extensions beyond C. Java 5.0 introduced the enum keyword to support enumerated types.\" Analyzing Languages Chapter 4 Building Intermediate Form Trees \" Only the simplest language applications get away with reading input and directly generating output. Such applications are called syntax-directed applications\" \" The key characteristic of syntax-directed applications is that they translate input phrase by phrase using a single pass over the input.\" \" Most language applications, however, need to build an intermediate representation ( IR ) or intermediate form.\" \" The goal of an application's reader component is to fill an IR data structure with elements of interest from the input stream.\" \" to get a computer to understand a nontrivial sentence, we have to break it down into a series of operations and operands.\" \" Once we identify the operators and operands among the input tokens, we need to build an IR data structure. For most language applications, that means building a tree data structure. In particular, we'll build an abstract syntax tree ( AST ).\" \" Abstract syntax tree ( AST ) […] hold the key tokens from the input stream and record grammatical relationships discovered during the parse.\" \" ASTs are the lingua franca spoken by the various stages in a language application. Each stage performs a computation, rewrites the tree, or creates another data structure before passing the tree along to the next stage.\" \" The four most common IR tree patterns:\" \" Parse trees record how a parser recognizes an input sentence. The interior nodes are rule names, and the leaves are tokens. Although parse trees are less suitable than ASTs for most language applications, parsers can create them automatically.\" \" Homogeneous AST […]: If all the nodes have the same type, we say that they are homogeneous. With a single node type, there can be no specialty fields to reference child subtrees. Nodes track children with lists of child pointers.\" \" Normalized Heterogeneous AST […]: Trees with a multitude of node types are called heterogeneous trees. Normalized het erogeneous trees use a normalized list of children like homogeneous trees.\" \" Irregular Heterogeneous AST […]: When we refer to an AST as heterogeneous, we also assume that the nodes have irregular children. Instead of a normalized child list, the nodes have named fields, one per child.\" Why We Build Trees \" Many languages have subphrases and nested structures.\" \" trees are the perfect data structure to represent ordered and nested structures. There are two general kinds of trees we're going to look at: parse trees and abstract syntax trees .\" \" Parse trees record the sequence of rules a parser applies as well as the tokens it matches. Interior parse tree nodes represent rule applications, and leaf nodes represent token matches.\" \" a parser execution trace isn't really the best IR . Certainly we need to pinpoint the various substructures, but we don't need to name them explicitly.\" Building Abstract Syntax Trees \" An IR tree should be the following: Dense : No unnecessary nodes Convenient : Easy to walk Meaningful : Emphasize operators, operands, and the relationship between them rather than artifacts from the grammar\" \" it should be easy and fast to identify patterns in the tree. Language applications that use intermediate trees usually make multiple passes over the trees in order to analyze or build other data structures. The structure of intermediate trees should be brain-dead simple.\" \" the tree structure should be insensitive to changes in the grammar\" \" Computers only care about operators and operands.\" \" By condensing the input to its essential elements, we decouple it from the original syntax. So, for example, assignment syntax boils down to an assignment operator and two operands. Decoupling does two things. First, it gets us closer to the operator-operand model of the CPU . Second, we can have different languages share a common intermediate form.\" \" The key idea behind AST structure is that tokens representing operators or operations become subtree roots. All other tokens become operands (children of operator nodes).\" Representing Pseudo-operations in ASTs \" Not all programming language constructs map directly to executable code.\" \" In some cases, there is no reasonable input token to use as a subtree root. We must invent an imaginary token , a token for which there is no corresponding input token. For example, variable declarations in languages derived from C usually need an imaginary token.\" Implementing ASTs in Java \" technically, we need only one data type: a generic tree node with a list of children. What we really care about is the tree structure (relationships between nodes), not necessarily the node implementation type(s) themselves.\" \" Trees built from a single data type are called homogeneous trees .\" Enforcing Tree Structure with the Type System \" To avoid creating improperly structured ASTs, we can co-opt the implementation language's static type system to enforce structure.\" \" The best way to create ASTs and to verify their structure is with a formal mechanism.\" Constructing ASTs with ANTLR Grammars \" The key is that we are declaring what the AST should look like, not how to build it. It is analogous to using a grammar to specify syntax rather than building a parser.\" \" We looked at two different ways to structure intermediate representations (parse trees and ASTs) and three different ways to implement ASTs.\" \" Pattern 8, Parse Tree […]: Pros: Parser generators can automatically build these for us. Cons: Parse trees are full of noise (unnecessary nodes). They are sensitive to changes in the grammar unrelated to syntax. If a parser generator generates heterogeneous node types, there can be literally hundreds of class definitions. \" \" Pattern 9, Homogeneous AST […]: Pros: Homogeneous trees are very simple. Cons: It's cumbersome to annotate AST nodes because the single node type has the union of all needed fields. There is no way to add methods specific to a particular kind of node.\" \" Pattern 10, Normalized Heterogeneous AST […]: Pros: It's easy to add operator or operand-specific data and methods. Cons: Large grammars like Java's need about 200 class definitions to be fully heterogeneous. That's a lot of files to read and write.\" \" Pattern 11, Irregular Heterogeneous AST […]. Pros: It's easy to add operator- or operand-specific data and methods. Sometimes code operating on nodes is more readable because the children (operands) have names rather than positions like children[0] . Building tree-walking methods for a small set of heterogeneous nodes is quick and easy. Cons: As with Pattern 10, Normalized Heterogeneous AST […], there are lots of AST classes to read and write. Having irregular children makes building external visitors difficult. Most of the time we have to build tree walkers by hand using Pattern 12, Embedded Heterogeneous Tree Walker […]\" \" If you're in doubt about which is best in your situation, choosing Pattern 10, Normalized Heterogeneous AST […] is a safe bet.\" Pattern 8: Parse Tree Purpose \" A parse tree describes how a parser recognized an input sentence. A parse tree is sometimes called a syntax tree (as opposed to an abstract syntax tree). Despite not being that useful for building interpreters and translators [they] are heavily used by development environments and text rewriting systems.\" Discussion \" Parse trees record the sequence of rules a parser applies as well as the tokens it matches. Interior parse tree nodes represent rule applications, and leaf nodes represent token matches.\" \" Parse trees are really easy to build by hand and are so regular that tools like ANTLR can automate the process for us.\" \" Parse trees are full of noise because of all the interior rule nodes. They are also very sensitive to changes in the grammar.\" \" An AST captures just the essential information from the input: all of the input tokens and the appropriate structure. The interior nodes are operators or operations rather than rule names.\" \" Parse trees mirror the function call graphs of a recursive-descent parser\" Pattern 9: Homogeneous AST Purpose \" A homogeneous tree implements an abstract syntax tree ( AST ) using a single node data type and a normalized child list representation.\" Discussion \" The key idea behind an AST is the operator-operand tree structure, not the node data type.\" \" We don't need to use the type system of our implementation language to distinguish between nodes.\" \" In fact, homogeneous ASTs are the only convenient choice for non-object-oriented languages like C.\" \" Homogeneous ASTs necessarily use a normalized child representation: List<AST> . This makes it particularly easy to build external visitors.\" Pattern 10: Normalized Heterogeneous AST Purpose \" This pattern implements an abstract syntax tree ( AST ) using more than a single node data type but with a normalized child list representation.\" Discussion \" This pattern makes the most sense when we need to store node-specific data\" \" The normalized child list makes it much easier to build external visitors.\" Pattern 11: Irregular Heterogeneous AST Purpose \" This pattern implements an abstract syntax tree ( AST ) using more than a single node data type and with an irregular child list representation.\" Discussion \" Instead of a uniform list of children, each node data type has specific (named) child fields. In this sense, the child pointers are irregular. In some cases, named fields lead to more readable code.\" \" It's very natural to name the fields of a class, in this case naming the children of a node. The big downside to using nodes with irregular children is that it's much less convenient to build tree walkers\" Walking and Rewriting Trees \" Tree walking is one of the key processes going on in a large language application.\" \" In real applications, though, tree walking gets surprisingly complicat- ed. There are a number of different variations, sometimes even within the same application.\" \" The variation we choose depends on whether we have the source code for our tree nodes, whether the trees have normalized children, whether the trees are homogeneous or heterogeneous, whether we need to rewrite trees while walking, and even in which order we need to walk the nodes.\" Walking Trees and Visitation Order \" Preorder traversal or top-down traversal: \\(+ 1 2\\) . Visit a (parent) node before visiting its children. Inorder traversal: \\(1 + 2\\) . Visit a node in between visiting children. Postorder traversal or bottom-up traversal: \\(1 2 +\\) . Visit a node after visiting its children.\" Pattern 12: Embedded Heterogeneous Tree Walker Purpose \" This pattern walks heterogeneous ASTs using a set of recursive methods defined within the node class definitions.\" Discussion \" This is the easiest tree-walking pattern to understand, but, ultimately, this approach doesn't scale well. Because it distributes tree-walking code across all node definitions, it works best when there are only a few node definitions.\" Pattern 13: External Tree Visitor Purpose \" This pattern encapsulates all tree-walking code associated with a particular task into a single visitor class. Visitors combine tree walking and action execution code outside the AST node definitions. Consequently, we can change the functionality of the tree walker without having to change the AST class definitions and can even switch visitors on the fly. An external visitor can walk either heterogeneous or homogeneous AST nodes.\" Discussion \" The visitor pattern is the workhorse of choice for tree walking in most language applications. Ultimately you might get tired of manually building visitors, though\" Implementation \" There are two ways to implement this pattern. The first is more traditional and relies on the node types themselves. The second relies on the node's token type instead.\" Visitor Switching on Node Type \" The traditional implementation of the visitor pattern originally specified in ‘Design Patterns: Elements of Reusable Object-Oriented Software' relies on a ‘double-dispatch' method within each AST node. The double-dispatch method redirects visit() calls on a node to an appropriate method in a visitor servicing that node type. The visitor is like a set of callback methods.\" Switching on the Token Type to Build Independent Visitors \" For language applications, we build trees from tokens. Since we can distinguish between tokens using the token type, we can also distinguish between AST nodes using the token type. By switching on the token type rather than the AST node type, we can avoid the visit() method in each AST node. In its place, we use just one dispatch method inside the visitor.\" Pattern 14: Tree Grammar Purpose \" Tree grammars are a terse and formal way of building an external visitor.\" Discussion \" Tree grammars look just like conventional parser grammars except that we can match subtree patterns as well. As with parser grammars, we can embed actions to extract information or reorganize the input (a tree, in this case).\" \" ANTLR generates tree walkers from tree grammars that literally act like parsers.\" \" Tree grammars do not care about the implementation language classes used to represent AST nodes (they work with both homogeneous and heterogeneous AST nodes).\" Pattern 15: Tree Pattern Matcher Purpose \" This pattern walks trees, triggering actions or tree rewrites as it encounters tree patterns of interest. The process of matching and rewriting trees is formally called term rewriting.\" Discussion \" Using a tree pattern matcher differs from using a tree grammar in two important ways: We have to specify patterns only for the subtrees we care about. We don't need to direct the tree walk.\" \" A tree pattern matcher is analogous to text rewriting tools such as awk , sed , and perl .\" Translating and Generating Languages Translating Computer Languages \" Translation involves fully understanding each input phrase, picking an appropriate output construct, and filling it with elements from the input model.\" \" We usually have to create an input model like an AST because we can't always do semantic analysis properly as we parse.\" \" According to the needs of the task at hand, we compute everything we need to know about the input and then make a decision about mapping an input phrase to an output phrase.\" Rule-Based Translation \" Rule-based systems are particularly good at legacy code conversions because we want the translated code to be natural.\" \" These rule engines let us say what to do, not how to do it. They are powerful, implementation-language independent, expressive, formal, and beautiful.\" \" These systems tend to be complex beasts. With a large number of translation rules, translations can be slow.\" \" The rule engines themselves are black boxes, which can make it hard to understand what's gone wrong in a translation. Some of these systems were also designed to be the center of the universe, making them tricky to integrate into applications.\" Model-Driven Translation \" In a model-driven translator, everything centers around an input model created by the parser.\" \" From the AST input model, we're going to learn how to derive an appropriate output model instead of immediately generating output.\" \" As we walk the input model, we'll match subtrees and create output objects to represent translated phrases\" \" Using a hierarchy of output objects to represent the output makes sense because it's equivalent to a syntax tree.\" Decoupling Input Model Traversal from Output Order \" There are two ways to solve input-output ordering mismatches. First, we can walk the tree [multiple times].\" \" This works but is inefficient because we have to walk the (potentially very large) tree [multiple times]. This is an output-driven approach because it ‘pulls' information from the input model according to the output order.\" \" The second choice is to walk the input model a single time, collecting the declarations and definitions in lists instead of directly emitting text\" \" This input-driven approach lets us decouple the input and output order, albeit at the cost of buffering up the various output pieces. It's totally worth it, though.\" \" We know that the translation process is about creating an input model, enriching it with semantic information, and then creating an appropriate output model. Target-specific generator classes are familiar and well-structured, but building them is a lot of work, and they're often a hassle to use. Visitors that generate text directly are much more convenient. Unfortunately, print statements lock the order of the output elements to the input model traversal order. Besides, computing output strings in a general-purpose programming language ain't exactly pretty.\" Organizing Translated Phrases into a Nested Model \" In general, translators need to track a few locations in the output model. These locations are typically things such as the current file object, the current class object, or the current method object.\" Pattern 29: Syntax-Directed Translator Purpose \" This pattern generates text using a grammar, or equivalent hand-built parser, embedded with actions.\" Discussion \" Syntax-directed translators are little more than grammars salted with actions (code snippets). They don't build internal models and then walk them to generate output. Putting actions directly in a grammar sometimes makes the grammar difficult to read. It also locks the grammar into a single bit of translator functionality.\" Pattern 30: Rule-Based Translator Purpose \" A rule-based translator expresses a translation with a set of ‘x becomes y' rules, written in the DSL of a pattern-matching engine.\" Discussion \" To use a rule-based system, we have to feed it two things: a grammar that describes input sentences and a set of translation rules.\" \" Although it looks like we're doing text-to-text transformations, the underlying engine is actually doing tree rewrites. For complicated translators, we need to build a lot of ancillary data structures beyond trees such as symbol tables and control-flow graphs.\" Pattern 31: Target-Specific Generator Classes Purpose \" This pattern describes a class library whose sole purpose is to represent and generate output constructs in a particular language.\" Discussion \" Rather than use print statements to generate programs or data, generator classes let us insulate application code from the exact syntax of an output language. […] we're defining a special class for each output element.\"","tags":"Programming","url":"language_implementation_patterns.html","loc":"language_implementation_patterns.html"},{"title":"Make as multi-paradigm language","text":"Make can be seen as a multi-paradigm programming language. It supports at least three paradigms: Declarative Imperative Functional Declarative Rules in makefiles are written in a declarative way: %.o : %. c gcc -c $< -o $@ Imperative Conditionals can be used as an imperative way to describe alternative actions in makefiles . ifeq ($(MSG),) $( warning \" No message provided \" ) endif Functional Functions in make are evaluated in a functional way: Example: Calculate the factorial with help of bc fact = $(if $( filter 0 , $( 1 )) , 1 , $( shell echo \" $( 1 ) * \\ $( call fact, $( shell echo \" $( 1 ) - 1\" | bc )) \" | bc)) result = $( call fact, 4 ) # 24","tags":"Programming","url":"make_as_multi-paradigm_language.html","loc":"make_as_multi-paradigm_language.html"},{"title":"Linking and Loading","text":"The different forms of Linking There are two major forms of linking: static and dynamic. But there are some subtle details in each form of linking. This explanations are mainly for Linux. On other OS 's the same concepts apply but with some minor differences. See also: Executable Loader Static Linking Combine object files and archives to binaries (executables and shared libraries) Usually a step in the build process (after compiling) Not very flexible Not very suitable for libraries Libraries should be linked dynamically to executables Libraries are usually created by statically linking object files together Order of symbol resolution can be tricky (e. g. same symbol in multiple object files) Dynamic Linking Shared libraries (so, dll, dylib) are loaded at runtime. Loading Often just called: dynamic linking Shared libraries are passed to linker in build process (like with static linking) Dynamic loader loads library to process address space when executable is started (or can be lazy loaded) Mostly used for libraries that are available at build time Manual loading (dlopen) Same mechanism as dynamic linking But linking happens manually (programmatically) ‘dlopen' on Linux/ UNIX Can load library at run time Used for plug-in mechanism Preloading The dynamic linker looks for libraries in ‘LD_PATH' Possible to hook into loading process with ‘LD_PRELOAD' Libraries provided in ‘LD_PRELOAD' are loaded instead of libraries in ‘LD_PATH' Can be used for overwriting or extending system functions (e. g. allocators: ‘malloc', ‘free', ‘realloc' …) Special checks (security) are performed before library is preloaded","tags":"Programming","url":"linking_and_loading.html","loc":"linking_and_loading.html"},{"title":"Linux System Calls","text":"A system call is the basic interface between applications and the kernel. Linux has more than 300 system calls . The syscalls are implemented by the syscall function . Intel x86-64 System calls use the default calling convention (System V AMD64 ABI ). Stack based arguments are not used. This limits the number of arguments to 6. Each system call has an unique call code. The call code are provided in the rax register. Call codes and arguments can be found here . Registers The registers are only used when needed. The call code is necessary. Register Usage rax Call code, return value after call rdi 1st argument rsi 2nd argument rdx 3rd argument rcx 4th argument r8 5th argument r9 6th argument SYSCALL Instruction After the call code and the arguments are placed in the registers the syscall instruction is executed. Then the current process will pause and control is passed to the kernel. When the system call returns the process will be resumed.","tags":"Programming","url":"linux_system_calls.html","loc":"linux_system_calls.html"},{"title":"x86 Calling Conventions","text":"There are a lot of different calling conventions fir x86 processors. See also Wikipedia:x86 calling conventions Unix System V AMD64 ABI : Linux macOS FreeBSD Solaris This calling conventions are also used for C/C++ programs. Instructions call : Save return address, implicitely pushing RIP to stack ret : Pop's corrent top of stack ( RSP ) into RIP Argument Passing The first 6 arguments (integer or pointer) are passed in registers: Register Argument RDI 1st argument RSI 2nd argument RDX 3rd argument RCX 4th argument R8 5th argument R9 6th argument The first 8 floating point arguments are in the registers xmm0 - xmm7 . Additional arguments are located on the stack in reverse order (from right to left) R10 is used as static chain pointer for nested functions Variadic functions: Number of floating point arguments: RAX The arguments are passed in the vector registers Return Value The return value is passed in registers RAX and if needed also in RDX . If the return value is a floating point number it is passed in xmm0 . Call by Reference (out params) Output parameters need two steps to return a value: Get the address from the stack Use that address to return the value Registers Calle: must save and restore RBP , RBX and R12 - R15 if they are used Caller: all other registers must be saved and restored if their content is needed after the function cal No XMM registers are preserved Register Usage Notes rax Return Value rbx Callee Saved rcx 4th Argument Not preserved during function call rdx 3rd Argument Not preserved during function call rsi 2nd Argument Not preserved during function call rdi 1st Argument Not preserved during function call rbp Callee Saved rsp Stack Pointer r8 5th Argument Not preserved during function call r9 6th Argument Not preserved during function call r10 Temporary Not preserved during function call r11 Temporary Not preserved during function call r12 Callee Saved r13 Callee Saved r14 Callee Saved r15 Callee Saved xmm0 - xmm15 Not preserved during function call Clean-Up The caller is responsible for clearing the argument from the stack. This is usually donne by adjusting the RSP , instead of POP instructions. add rsp , numberOfArguments * 8 ; number of arguments placed on stack Stack Frame The stack frame is the data on the stack that belong to one function call. It's also called activation record or call frame. Here is a good explanation. Arguments that are not passed in registers Return address (saved by call ) Saved rbp from last stack frame. rbp needs to point now to this address Local variables. rsp points to last used address After the last stack frame there are 128 bytes red zone The System V AMD64 ABI doesn't require to use the base pointer To save and adjust rbp : ; prologue push rbp ; save the 'old' `rbp` on the stack mov rbp , rsp ; adjust `rbp` to point to the just saved 'old' `rbp` The Red Zone The 128 bytes after the last stack frame are reserved for compiler optimization This range is not allowed to be changed asynchonically (e.g by signals or IRQ handlers) Some functions (i.e. leaf functions) don't need a stack frame at all. They can use the red zone","tags":"Programming","url":"x86_calling_conventions.html","loc":"x86_calling_conventions.html"},{"title":"Intel Assembler Overview","text":"Program Format Typical sections of an assembler program are: Data section: declaration and definition of initialized data BSS section: declaration of uninitialized data Text section: where the code is placed Comments Code comments are written using the semicolon ( ; ). It can be placed anywhere. Everything to the end of the line is ignored. Numeric Literals Radix Format Example Decimal default 127 Hex 0x prefix 0x7F Octal q postfix 177q Constants < name > equ < value > Constants are substituted with their value during the assembly process. They are not assigned a memory location Example: SIZE equ 127 Initialized Variables (data) Declared in section .data Name, data type and initial value Supported data types: db : 8-bit(byte) dw : 16-bit (word) dd : 32-bit (double word) dq : 64-bit (quad word) ddq : 128-bit integer dt : 128-bit float Arrays are Initialized with comma separated values Format: < varName > < dataType > < initialValue > Examples: bVal db 2 ; byte cVal db \"H\" ; char str db \"Hello World\" ; string wVal dw 5000 ; 16-bit word dVal dd 50000 ; 32-bit (double word) arr dd 100 , 200 , 300 ; 3 element array flt1 dd 3.14159 ; 32-bit float qVar dq 1000000000 ; 64-bit (quad word) Uninitialized Variables ( BSS ) Declared in section .bss Name, data type and count Supported data types: resb 8-bit (byte) resw 16-bit (word) resd 32-bit (double-word) resq 64-bit (quad-word) resdq 128-bit (double quad-word) Format: < varName > < resType > < count > Examples: bArr resb 5 ; 5 element byte array wArr resw 20 ; 20 element word array dArr resd 30 ; 30 element double array qArr resq 25 ; 25 element quad array Code (text) Placed in `section .text initial program entry point needs to be defined For the standard linker (Linux) the entry point is defined: global _start _start: Assembler Directives Assembler directives are instrutions for the assembler that are not directly translated to CPU instrutions. Labels Labels are used as targets for jumps Can contain letters, numbers and _ Terminated with colon ( : ) Case sensitive (at least in yasm ) May be definied only once Examples: loopBegin: end:","tags":"Programming","url":"intel_assembler_overview.html","loc":"intel_assembler_overview.html"},{"title":"Git Reset","text":"The git reset command is not very easy and can be dangerous in some cases. The git checkout command does similar things. This overview is taken from the Pro Git book. The meanings in the following tables are: HEAD : Means that either HEAD itself is moved or the branch that HEAD points to ( REF ) Index and Workdir : indicate what is changed by the command Safe : If the command can overwrite uncommited changes it is not safe (work can be lost) Commit Level Command HEAD Index Workdir Safe Notes reset --soft [commit] REF ✗ ✗ ✓ reset [commit] REF ✓ ✗ ✓ reset --hard [commit] REF ✓ ✓ ✘ just replaces and overwrites files without waring checkout <commit> HEAD ✓ ✓ ✓ merges files, aborts and warns in case of conflicts File Level Command HEAD Index Workdir Safe Notes reset [commit] <file> ✗ ✓ ✗ ✓ checkout [commit] <file> ✗ ✓ ✓ ✘ would be same as git reset --hard [branch] file but this is not allowed HEAD is never changed with this commands. Patch Option Both commands, git reset and git checkout support the --patch option to allow to reset or checkout only part of changes.","tags":"Programming","url":"git_reset.html","loc":"git_reset.html"},{"title":"FreeRTOS","text":"This page contains some random notes for FreeRTOS. See also: FreeRtosExamples and FreeRtos Configuration The configuration of the kernel is set in FreeRTOSConfig.h configTOTAL_HEAP_SIZE : This must be enough space to allocate all tasks including the idle task. If the kernel does't start it could be that the heap is too small. vTaskStartScheduler() returns only if there is not enough memory to create the idle task. Tasks Defined as free functions Return type: void Argument: void * Usally endless loop Will never exit (not allowed by kernel) A task function can be used to create multiple tasks It's possible to create a task from another task Task Stack and TCB Each task has its own stack (allocated by the kernel when the task is created). The stack size is given in number of words and not number of bytes The stack depth multiplied by the stack width must not be bigger than the maximum value that can be stored in a variable of type size_t. The stack size of the idle task is defined by configMINIMAL_STACK_SIZE . The scheduler maintains a Task Control Block ( TCB ) for each task. Each TCB has the same size. Creating a task results in two allocations ( pvPortMalloc() ): Task stack TCB Stack Overflow FreeRTOS has multiple mechanisms that allow to handle and debug stack overflows. High Water Mark The function uxTaskGetStackHighWaterMark() returns the minumum of avaliable stack space since the task has been started. Runtime Checks There are runtime checks for stack overflow available. Checking at task switch Fill stack on creation with a pattern, check top of stack for the pattern Method 1 After a context switch the kernel checks that the stack pointer remains within the valid stack range. This method is quick but can miss stack overflows that happen between context switches. Method 2 This method performes additional checks to the ones of method 1. At task creation the stack is filled with a known pattern. The last 20 bytes of the stack are checked for that pattern. If it was overwritten a stack overflow occured. This method is not as quick as method 1. But it will likely find most (if not all) stack overflows. Configuration Set the desired method with configCHECK_FOR_STACK_OVERFLOW (value 1 or 2 ) Implement a hook function: vApplicationStackOverflowHook() The hook function has the prototype: void vApplicationStackOverflowHook ( xTaskHandle * pxTask , signed char * pcTaskName ); Real recovery ofter a stack overflow is not possible. Since since the TCB may be corrupted the data privided to the hook function may be useless. Task States Only the FreeRTOS kernel is allowed to change task state. Running Not running Suspended Ready Blocked Events and system calls can influence the scheduler for allowing different tasks to run. If more than one task of the same priority is able to run, the scheduler will switch each task in and out of the Running state, in turn. Task Priorities Can be assigned a valu form 0 (lowest) to configMAX_PRIORITIES - 1 (highest). Priority can be changed after the scheduler has started but are never changed by the scheduler itself. Selecting Task Priority Rule of thumb: Tasks for hard real-time functionalities are assigned priorities above tasks for soft real-time functionalities. Rate Monotonic Scheduling ( RMS ) See Wikipedia:Rate-monotonic scheduling Each task is assiged a unique static priority. The priorities are assigned according to the cycle duration of the task. Task with the highest execution frequency (shorter cycle) is assigned highest priority is assigned Task with the lowest execution frequency is assigned lowest priority is assigned RMS maximizes the schedulability of an application. Blocking a Task A task can be blocked if it currently has nothing to do vTaskDelay() is usually called for a task to enter the blocked state vTaskDelayUntil() should be used when a fixed execution period is required (e.g. DSP ) Idle Task Hook It's possible to add a callback to the idle task. It is called automatically once per iteration by the idle task loop. Following rules must be followed: The hook function must never block or suspend itself If the application make use of vTaskDelete() the hook function needs to return to its caller within reasonable time The idle task is responsible to clean up resources after a task has been deleted. Scheduler The scheduler is executed after each time slice to select the next task. A periodic (tick) interrupt is used for switching. The tick interrupt frequency is set by configTICK_RATE_HZ . To convert the number of ticks into milliseconds the constant portTICK_RATE_MS can be used. Scheduling Algorithm FreeRTOS uses a fixed priority pre-emptive scheduling algorithm. But it can be configured to use a cooperative scheduling algorithm. Each task has a priority assigned Each task has a state Only one task is in running state The scheduler selects always the highest priority task (in ready state) to enter running state Synchronization Tasks or ISRs send information to a queue or to a semaphore. These synchronization events are used to signal asynchronous activity. Semaphores: communicate events Queues: communicate events and data Queues Fixed number and size of data items FIFO Writing and reading: byte-for-byte copy Accessible from multiple tasks multiple writers is common multiple readers is not used often Semaphores Handles to all types of semaphores are stored in variables of type xSemaphoreHandle . Binary Semaphores See Wikipedia:Semaphore Unblock a task each time an IRQ occurs Synchronization of tasks with interrupts Deferring processing to handler task Terminology: \\(P()\\) operation: ‘taking' a semaphore \\(V()\\) operation: ‘giving' a semaphore API function xSemaphoreGiveFromISR(..) : All types of semapores except recursive semaphores can be ‘given' with xSemaphoreGiveFromISR() . Output argument pxHigherPriorityTaskWoken : if set to pdTRUE a context switch should be performed before ISR exits this context switch needs to be done manually in ISR code the scheduler will switch to the highes prioriy task (in ready state) Counting Semaphores Use cases: Counting events Count value: difference between number of occured events and number of processed events Created with an initial count value of zero Resource management Count value: indicates number of available resources Created with an initial count value equal to number of available resources Critical Sections FreeRTOS provides several mechanisms to implement mutual exclusion. But best is if resources are not shared and each resource is accessed only by one single task. Basic Critical Sections Critical sections can be wrapped in calls to taskENTER_CRITICAL() and taskEXIT_CRITICAL() . This is a very rudimentary form of mutual exclusion. IRQs with priority above configMAX_SYSCALL_INTERRUPT_PRIORITY are still allowed to execute these interrupts are not allowed to call FreeRTOS API functions Critical sections can be nested. Locking the Scheduler The scheduler can be suspended by vTaskSuspendAll() and resumed by xTaskResumeAll() . This protects the critical section only from access by other tasks. It leaves interrupts enabled. If an interrupt requests an context switch the request is performed when the scheduler is resumed. Do not call FreeRTOS API functions while the scheduler is suspended Locking the scheduler can be nested. Mutexs and Binary Semaphores Token that is associated to a shared resource Acquire the token before access to the resource is given After using the resource the token has to be released Mutual exclusion: the semaphore must be returned Synchronization: the semaphore is not returned (discarded) IRQs Event processing: Detect event IRQs polling IRQs how much processing in ISR how much in main code Communication of IRS to main code Only API functions and macros ending in FromISR or FROM_ISR are allowed to be called from interrupt service routines. The Cortex-M3 allows up to 256 (8 bits) different interrupt priorities. High numeric priority numbers mean low interrupt priority. There are CMSIS functions that provide access to the interrupt controller. Interrupt priorities have nothing to do with task priorities. Interrupt priorities are provided by the microcontoller architecture. Interrupts with higher priorites than configMAX_SYSCALL_INTERRUPT_PRIORITY (lower value) must not call any FreeRTOS API functions IRQ Nesting Interrupt nesting is achieved by setting configMAX_SYSCALL_INTERRUPT_PRIORITY to a higher priority (lower value) than configKERNEL_INTERRUPT_PRIORITY . Resources Used interrupts on Cortex-M3: SysTick PendSV SVC These interrupts are not available for the application. Memory (typical sizes): Flash: 6 kB RAM : 300-500 bytes Reentrancy A function that is safe to be called from multiple tasks or ISRs is reentrant. Each task has its own stack and its own set of (pushed) registers. If a function only accesses data from its own stack and registers then it is reentrant. General Real-Time Requirements Typically, applications of this type include a mix of both hard and soft real-time requirements Soft real-time requirements: time deadline but breaching deadline does not render the system useles i.e. GUI (user interaction) Hard real-time: time deadline breaching the deadline: absolute failure of the system i.e. security related code in cars Applications typically combine both types of real-time rocessing Coding Standard FreeRTOS has its own coding standard. Data Types portBASE_TYPE : Most efficient data type on the architecture portTickType : Tick count value to specify block times (16-bit or 32-bit) Variable and Function Names Variable Prefixes: c : char s : short l : long x : portBASE_TYPE and others (structs, handles …) u : unsigend (combined with other prefixes) p : pointer (combined with other prefixes) Function Names: Prefixed with return type and file where the function is defined. Defines: Macro Value pdTRUE 1 pdFALSE 0 pdPASS 1 pdFAIL 0","tags":"Programming","url":"freertos.html","loc":"freertos.html"},{"title":"Investment Management (Coursera Specialization)","text":"Contents Understanding Financial Markets General Introduction and Key Concepts Investment Management in a nutshell Investment management is about managing risk ! The key to successful Investment Management is to constantly be aware of the underlying risks: Country risk Market risk Currency risk Liquidity risk Inflation risk Shortfall risk The key is diversification! The role of financial markets Financial Market Participants Firms Investors Government Financial Intermediaries Primary vs. Secondary Market Investopedia Primary market: investors buy securities directly from the company issuing them Secondary market: investors trade securities among themselves (the company does not participate in the transaction) Functions of the financial system \" the primary function of any financial system is to facilitate the allocation and deployment of economic resources, both across borders and across time, in an uncertain environment\" - Merton & Bodie (1995) Pooling resources & subdividing shares: allow big investors (firms) to invest lot of money but also allow small investors Transferring resources across time & space: invest in different countries, invest for the future Managing Risk: diversified portfolio. derivative products that allow you to insure your portfolio against a drop securitization Providing Information: prices, values … Desirable criteria of financial markets: transparent: all participants have all the relevant information on the prices, the volume traded, the beta spreads, the order book … fair: e.g traditional traders vs. high frequency traders avoid insider trading: some people could have privileged information prevent market abuse: fixing prices … Well functioning financial markets should provide access to the following and to everybody: trading information (price, volume, order book, …) trading technology (higher speed access, …) company information (insider trading, …) market opportunities (fixing FOREX & LIBOR markets) Basic concepts in finance Return (rate of return): $$r={\\frac {V_{f}-V_{i}}{V_{i}}}$$ where \\(V_{f}\\) : final value, including dividends and interest \\(V_{i}\\) : initial value often in % average return of a time period is important A risk-free asset: future returns are certain (return doesn't change) tradeoff between return and risk: ratio between average return and risk Histogram: horizontal axis is divided into intervals divide the number of observations in an interval by the total number of observations vertical axis indicates this probability or (relative) frequency Major Financial Markets Equities How much is a company worth on the stock market? Net Present Value ( NVP , Kapitalwert, Nettobarwert) \\(NPV(i,N)=\\sum _{t=0}&#94;{N}{\\frac {R_{t}}{(1+i)&#94;{t}}}\\) \\(t\\) : time of the cash flow \\(i\\) : discount rate, i.e. the return that could be earned per unit of time \\(R_t\\) : net cash flow (cash inflow - cash outflow) at time \\(t\\) invest in projects with a positive NPV do not invest in projects with negative NPV Investopedia Wikipedia","tags":"Finance","url":"investment_management_(coursera_specialization).html","loc":"investment_management_(coursera_specialization).html"},{"title":"Git: The .git Directory","text":"Some notes about the contents of the .git directory that resides in the root of a git repository. config file: contains project-specific configurations info/exclude file: ignore patterns in addition to .gitignore files but not tracked hooks directory: client- or server-side hook scripts HEAD file: points to the currently checked out branch objects directory: stores the content of the git database refs directory: stores pointers to commit objects (branches) index file: staging area information description file: only used by the GitWeb program For submodules the directory is in the .git directory of the main repository (\"supermodule\"). The .git directory of a repository can be shown with git rev-parse --git-dir","tags":"Programming","url":"git__the__git_directory.html","loc":"git__the__git_directory.html"},{"title":"C++ on Embedded Systems","text":"This page is still work in progress No Exceptions (-fno-exception) What do std functions that throw exeptions What happens with library functions that throw No Run Time Type Information ( RTTI ) (-fno-rtti) check size of modules (linker map files) Find weakness (according code size) of compiler and avoid this kind of code Use Templates with care (can generate big code, hard to write, but nice to use) Use STL only if enough space available, use allocators (memory pools) new/delete can be overloaded Avoid new/delete, malloc/free: Memory Fragmentation Use Memory Pools instead Allocate at start time Don't use any garbage collector Use smart pointers if possible Programm Procedural when OOP is not needed Don't use ‘Embedded C++', use coding rules and forbid some C++ features","tags":"Programming","url":"cpp_on_embedded_systems.html","loc":"cpp_on_embedded_systems.html"},{"title":"Software Development Priorities","text":"This page is still work in progress When developping software the priorities are as following: correctness maintainability efficiency Correct SW Defensive Programming Unit Tests Contracts Static Analysis Maintainable SW Architecture Design Patterns Idioms Readable/intuitive code Correct and Actual Documentation Optimal, efficient (Optimized) SW Speed Size (Program Size, Memory Use) Dynamic Analysis No premature optimization Often code gets less readable/maintainable let the compiler/linker do the hard work","tags":"Programming","url":"software_development_priorities.html","loc":"software_development_priorities.html"},{"title":"Smart Pointers","text":"From: Smart Pointers to boost your code unique_ptr unique ownership copying prevented use std::move to transfer ownership const prevents transfer can't be used as elements in containers use as replacement for auto_ptr shared_ptr reference counting referenced object is destroyed when (and only when) all copies of the shared_ptr have been destroyed can be used as elements in containers weak_ptr created as copy of shared_ptr copying/destroying have no effect on ref counting of shared_ptr after all copies of shared_ptr have been destroyed all weak_ptr copies become empty scoped_ptr (Boost) use const unique_ptr instead (or unique_ptr if you have to) Other smart pointers (Boost) intrusive_ptr shared_array scoped_array Rules for smart pointers Assign and keep: Assign a newly constructed instance to a smart pointer immediately Keep the management of the referenced instance in the smart pointer(s). The smart pointer(s) own the object. Don't delete the owned instance manually. You can't (shouldn't) take the instance away from the smart pointer. ..._ptr<T> is not a T* There are no implicit conversions You can not assign a T* to a ..._ptr<T> You can not write ptr = NULL use ptr.reset() To retrieve the raw pointer use ptr.get() You must not delete it! You must not use this pointer after the smart pointer(s) it comes from is/are destroyed, resetted or reassigned Avoid circular references Use weak_ptr to break such cycles No temporary shared_ptr Always construct named smart pointer variables. No anonymous (i.e. as param in function call) smart pointer constuction Smart pointers are thread safe. But not necessary the pointee objects","tags":"Programming","url":"smart_pointers.html","loc":"smart_pointers.html"},{"title":"Unix System Files","text":"/etc/passwd Contains user informations. The information fields are separated by a colon. user-name:password:UID:GID:info:home-dir:shell user-name : The name that the user needs to log in. password : Usually the entry x , meaning that the password is saved in /etc/shadow . UID : The user-id (usually greater than 1000). GID : The group-id. info : Additional information such as the real name. Subfields separated by coma. home-dir : The home dir of the user. shell : The login-shell of the user (needs to be listed in /etc/shells ). /etc/shadow Saves the user password and related information. The file can be read only by root. name:password:DOC:minDay:maxDay:warn:deaktIn:deaktSince:unused name : User name (same as in /etc/passwd ). password : Encrypted password. DOC : Day of last change (number of days since 1.1.1970). minDay : Earliest possible day to change password. maxDay : Latest possible day to change password. warn : Number of days before maxDay that a warning occures. deaktIn : Day when password expires. deaktSince : Number of days since password expired. unused : For feature use. /etc/group Stores informations about groups. groupName:password:groupId:member1,memger2,... groupName : Name of the group. password : Usually x , meaning that the password is saved in /etc/passwd . groupId : Group ID ( GID ). member1,memger2,... : User names of the members of the group (comma separated). Mounts /etc/fstab : Lists drives to mount at startup /etc/exports : Folders provided to clients over NFS Network Files /etc/services : Services provided on the network, mapping of ports to processes (see getservbyname() and getservbyport() ). etc/networks : Informations about connected networks (see getnetbyname() and getnetbyaddr() ) /etc/protocols : Information about network portocols (see getprotobyname() and getprotobynumber() ) /etc/hosts : Mapping from hostnames to IP 's, mainly replaced by DNS (see gethostbyname() and gethostbyaddr() ) Logging /etc/syslog.conf : Configuration of applications that use syslog /etc/asl.conf : Additional config on macOS cron Jobs The config file is usually: /var/spool/cron use crontab -e to edit the right file Shell Configurations: bash: use ~/.bashrc and source it in ~/.bash_profie zsh: use ~/.zshrc References Zsh/Bash startup files loading order","tags":"Programming","url":"unix_system_files.html","loc":"unix_system_files.html"},{"title":"Unix Device Files","text":"Device files provide access to drivers through files. They are listed under /dev . There are two kinds of device drivers (as shown with ls -l ) c : character device b : block device Character devices provide a stream of bytes (e. g. serial ports, USB , system consoles …) Block devices allow random access of blocks of information (e. g. HD , SSD …). The device files have a major and a minor number. The same major number means that the same device driver is responsible for the file/device. The minor number is used to distinguish different devices of the same type that use the same device driver. Functions A file descriptor for a device file can be aquired by the common open() function. Afer use it can be closed with close() . Reading and writing to the file is donne with read() and write() . Contol of device drivers is acomplished with the ioctl() function. Man page: man ioctl and man ioctl_list (only Linux). Special Device Files /dev/null : used to redirect unused streams to ‘nowhere' /dev/zero : ‘infinite' sized file filled with 0 (s.a. mmap() ) /dev/pts/ : virtual consoles (s.a. stackexchange ) /dev/random and /dev/urandom : provide random numbers ( /dev/random might block (high entropy), /dev/urandom doesn't block) /dev/full : can be used to simulate writing to a file when the disk is full (sets errno to ENOSPC )","tags":"Programming","url":"unix_device_files.html","loc":"unix_device_files.html"},{"title":"Complexity","text":"This page is work in progress \" Halving the size of the problem at each step is the distinguishing characteristic of logarithmic growth\" SICP : 2.3.3 Example: Representing Sets Main complexity names: Running time Name O(1) constant O(n) linear O(log(n)) logarithmic O(n*log(n)) linearithmic O(n&#94;2) quadratic O(2&#94;n) exponential Algorithm with exponential run time are considered as not feasible . Amortized time means that there is no guarantee that the algorithm is run every time with that complexy but that an average value of the run time is taken.","tags":"Programming","url":"complexity.html","loc":"complexity.html"},{"title":"Parsers and Lexers","text":"Most material on this page is from Udacity: Programming Languages Source --[Lexer (break up in words)]--> Tokens --[Parser (understand the structure)]--> AST --[Find meaning]-> ... Lexical Analysis (Lexing): String -> Token List Syntatical Analysis (Parsing): Token List -> Valid in Grammar? Grammars Terminals Non-terminals Rewrite rule / derivations Recursion (recursive rewrite rules) in a context-free grammar can allow an infinite number of valid sentences in the language. Grammars and Regular Expressions Grammars are more powerful than Regular Expressions Regular Expressions describe Regular Languages Grammars describe Context Free Languages \" A language L is a context-free language if there exists a context-free grammar G such that the set of strings accepted by G is exactly L . Context-free languages are strictly more powerful than regular languages. (i.e., context-free grammars are strictly more powerful than regular expressions.)\" Programming Languages Analogy Expressions := Noun Prases Operators := Verbs Statements := Setences Formal Grammar Concepts If a language ‘L' is regular , then that language ‘L' is also context free . Language: a set of strings Regular language: represented by FSM or Regexp Context-free language: represented by context-free Grammar Using Rewrite Rules Closure Shift Reduce Function calls: 1. Create a new environment. It's parent is the current environment. 2. Create storage places in the new environment for each formal parameter . 3. Fill these placeswith the values of the actual arguments 3. Evaluate the function body in the new environment. Comparison of LL - and LR -Parsers This overview is assembled mostly from LL and LR Parsing Demystified LL Parser LR Parser traversal pre-order (visit the parent node before the children) post-order (visit the parent node after the children) derivations leftmost derivation reversed rightmost derivation direction top-down bottom-up alternative description predictive parser shift-reduce parser","tags":"Programming","url":"parsers_and_lexers.html","loc":"parsers_and_lexers.html"},{"title":"Modern C++ Design","text":"Part I. Techniques Policy-Based Class Design The Benefit of Templates \" Templates are a good candidate for coping with combinatorial behaviors because they generate code at compile time based on the types (and/or constant values) provided by the user. Class templates are customizable in ways not supported by regular classes.\" \" Furthermore, for class templates you can use partial template specialization […]. Partial template specialization gives you the ability to specialize a class template for only some of its arguments.\" \" […] several problems that are not self-evident: You cannot specialize structure. Using templates alone, you cannot specialize the structure of a class (its data members). You can only specialize functions. Partial specialization of member functions does not scale. You can specialize any member function of a class template with one template parameter, but you cannot specialize individual member functions for templates with multiple template parameters. \" The library writer cannot provide multiple default values. At best, a class template implementer can provide a single default implementation for each member function. You cannot provide several defaults for a template member function.\" Member functions can be only fully specialized: template < class T > class Widget { void Fun () { .. generic implementation ... } }; // ok: specialization allowed template <> void Widget < char >:: Fun () { ... specialized implementation ... } Partial specialization is not allowed: template < class T , class U > class Gadget { void Fun () { .. generic implementation ... } }; // error: member functions can't be specialized partially template < class U > void Gadget < char , U >:: Fun () { ... specialized implementation ... } Policies and Policy Classes \" A policy defines a class interface or a class template interface. The interface consists of one or all of the following: inner type definitions, member functions, and member variables.\" \" Policies have much in common with traits but differ in that they put less emphasis on type and more emphasis on behavior. Also, policies are reminiscent of the Strategy design pattern , with the twist that policies are bound at compilation time.\" \" For a given policy, there can be an unlimited number of implementations. The implementations of a policy are called policy classes \" \" Policy classes are not intended for stand-alone use […]\" \" Policies are syntax oriented, not signature oriented.\" \" [A] policy does not specify that [a member function] must be static or virtual—the only requirement is that the class template define a [coresponding (with the expected signature)] member function.\" \" The classes that use one or more policies are called hosts or host classes \" Implementing Policy Classes with Template Template Parameters \" Library code can use template template parameters for specifying policies\" template < template < class Created > class CreationPolicy > class WidgetManager : public CreationPolicy < Widget > { // ... }; \" The Created symbol does not contribute to the definition of WidgetManager . You cannot use Created inside WidgetManager - it is a formal argument for CreationPolicy (not WidgetManager ) and can be simply omitted.\" \" First, you can change policies from the outside as easily as changing a template argument […]. Second, you can provide your own policies that are specific to your concrete application.\" \" [The author might] provide a default template argument for the policy that's most commonly used:\" template < template < class > class CreationPolicy = OpNewCreator > class WidgetManager ... \" Policies are quite different from mere virtual functions. […] policies come with […] static binding.\" \" policies' features also make them unsuitable for dynamic binding and binary interfaces , so in essence policies and classic interfaces do not compete.\" Destructors of Policy Classes \" Defining a virtual destructor for a policy […] works against its static nature and hurts performance.\" \" The lightweight, effective solution that policies should use is to define a nonvirtual protected destructor \" Optional Functionality Through Incomplete Instantiation \" If a member function of a class template is never used, it is not even instantiated - the compiler does not look at it at all, except perhaps for syntax checking. This gives the host class a chance to specify and use optional features of a policy class.\" Combining Policy Classes \" The greatest usefulness of policies is apparent when you combine them. Typically, a highly configurable class uses several policies for various aspects of its workings. Then the library user selects the desired high-level behavior by combining several policy classes.\" Decomposing a Class into Policies \" [To] decompose the functionality of a class in policies. The rule of thumb is to identify and name the design decisions that take part in a class's behavior. Anything that can be done in more than one way should be identified and migrated from the class to a policy. Don't forget: Design constraints buried in a class's design are as bad as magic constants buried in code.\" \" When you decompose a class in policies, it is very important to find an orthogonal decomposition. An orthogonal decomposition yields policies that are completely independent of each other. You can easily spot a nonorthogonal decomposition when various policies need to know about each other.\" Summary \" The mechanics of policies consist of a combination of templates with multiple inheritance. A class that uses policies - a host class - is a template with many template parameters (often, template template parameters), each parameter being a policy. The host class \"indirects\" parts of its functionality through its policies and acts as a receptacle that combines several policies in a coherent aggregate.\" \" Policy-based classes support flexibility when it comes to conversions. If you use policy-by-policy copying, each policy can control which other policies it accepts, or converts to, by providing the appropriate conversion constructors, conversion operators, or both.\" \" Two important guidelines.\" \" One is to localize, name, and isolate design decisions in your class - things that are subject to a trade-off or could be sensibly implemented in various ways.\" \" The other guideline is to look for orthogonal policies, that is, policies that don't need to interact with each other and that can be changed independently.\" Techniques Partial Template Specialization \" In a partial specialization of a class template, you specify only some of the template arguments and leave the other ones generic.\" \" When you instantiate a template, the compiler does a pattern matching of existing partial and total specializations to find the best candidate; this gives you enormous flexibility.\" \" Unfortunately, partial template specialization does not apply to functions - be they member or nonmember - which somewhat reduces the flexibility and the granularity of what you can do.\" \" Although you can totally specialize member functions of a class template, you cannot partially specialize member functions .\" \" You cannot partially specialize namespace-level (nonmember) template functions. The closest thing to partial specialization for namespace-level template functions is overloading. For practical purposes, this means that you have fine-grained specialization abilities only for the function parameters - not for the return value or for internally used types.\" Overloading is the closest to partial specialization for functions: // primary template template < class T , class U > T Fun ( U obj ); // illegal partial specialization template < class U > void Fun < void , U > ( U obj ); // specialization legal (overloading) template < class T > T Fun ( Window obj ); Local Classes Local (nested) classes can be used (defined) inside of template functions: \" What makes local classes truly interesting is that you can use them in template functions. Local classes defined inside template functions can use the template parameters of the enclosing function.\" \" Local classes do have a unique feature, though: They are final . Outside users cannot derive from a class hidden in a function.\" Detecting Convertibility and Inheritance at Compile Time See also SFINAE \" How can we write a function that accepts ‘anything else'? […] We need a match that's ‘worse' than an automatic conversion - that is, a conversion that kicks in if and only if there's no automatic conversion. A quick look through the conversion rules applied for a function call yields the ellipsis match, which is the worst of all - the bottom of the list\" \" Passing a C++ object to a function with ellipses has undefined results, but this doesn't matter. Nothing actually calls the function. It's not even implemented. Recall that sizeof does not evaluate its argument.)\" See also: stackoverflow \" […] how much you can do with functions […], that not only don't do anything but don't even really exist at all [(just declared but not defined)]?\" \" If template code applies const twice (to a type that's already const), the second const is ignored.\" Optimized Parameter Types \" A detail that must be carefully handled is that C++ does not allow references to references. Thus, if T is already a reference, you should not add one more reference to it.\" Typelists \" templates cannot have a variable number of parameters\" \" virtual functions cannot be templates\" Intermezzo About \"meta functions\" (like Length ) for Typelist s that are implemented in a functional way: \" Couldn't we develop a version of Length that's iterative, instead of recursive? After all, iteration is more natural to C++ than recursion.\" \" template specialization [..] provide the equivalent of if statements at compile time.\" \" All compile-time values are immutable . After you've defined an integral constant, say an enumerated value, you cannot change it (that is, assign another value to it).\" \" Type definitions ( typedef s) can be seen as introducing named type constants. Again, after definition, they are frozen - you cannot later redefine a typedef d symbol to hold another type.\" Erasing a Type from a Typelist \" [If] there is no default version of [a] template […] you can instantiate [it] only with certain types.\" Components Generalized Functors \" Generalized functors, [are] a powerful abstraction that allows decoupled interobject communication.\" \" Encapsulates any processing invocation\" \" Is typesafe \" \" Is an object with value semantics : copying, assignment, and pass by value, does not expose virtual member functions\" \" Can store state and invoke member functions\" \" Two important aspects of the Command pattern: Interface separation. The invoker is isolated from the receiver. Time separation. Command stores a ready-to-go processing request that's to be started later. \" C++ Callable Entities \" In addition to simple callbacks [function pointers], C++ defines many more entities that support the function-call operator. Let's enumerate all the things that support operator() in C++.\" \" C-like functions\" \" C-like pointers to functions\" \" References to functions (which essentially act like const pointers to functions)\" \" Functors, that is, objects that define an operator() \" \" The result of applying operator.* or operator->* having a pointer to a member function in the right-hand side of the expression\" \" You can add a pair of parentheses to the right of any of the enumerated items, put an appropriate list of arguments inside, and get some processing done. No other objects in C++ allow this except the ones just listed.\" (before C++11) The Functor Class Template Skeleton \" In C++ a bald pointer to a polymorphic type does not strictly have first-class semantics because of the ownership issue.\" \" C++ does not instantiate member functions for templates until they are actually used .\" \" Good C++ libraries sport this interesting feature: Whenever something ambiguous may appear, they allow the user to disambiguate it by writing some explicit code. At the other end of the spectrum are libraries that misuse silent C++ features (especially conversions and pointer ownership). They allow the user to type less, but at the cost of making dubious assumptions and decisions on the user's behalf.\" Handling Functors template < typename R , class TList > template < typename Fun > Functor < R , TList >:: Functor ( const Fun & fun ) : spImpl_ ( new FunctorHandler < Functor , Fun > ( fun )) { } \" The two template parameter sets are necessary: The template <typename R, class TList> stands for the class template Functor , and template <typename Fun> stands for the parameter that the constructor itself takes. […] is known as an ‘out-of-class member template definition'.\" Argument and Return Type Conversions \" Template processing predates compiling, allowing you to operate at source-code level. In object-oriented programming, in contrast, the power comes from late (after compilation) binding of names to values. Thus, object-oriented programming fosters reuse in the form of binary components, whereas generic programming fosters reuse at the source-code level. […] The two techniques complement each other.\" \" pointers to member functions and their two related operators - .* and ->* - reveals strange features. There is no C++ type for the result of geronimo.*pActivity and pGeronimo->*pActivity . Both are binary operators [that] return something to which you can apply the function-call operator immediately, but that ‘something' does not have a type.\" \" The standard says, ‘If the result of .* or ->* is a function, then that result can be used only as the operand for the function call operator() .\" \" You cannot store the result of operator.* or operator->* in any way, although there is an entity that holds the fusion between your object and the pointer to a member function\" \" pointers to member functions and the two related operators are a curiously half-baked concept in C++. And by the way, you cannot have references to member functions (although you can have references to regular functions).\" Real-World Issues II : Heap Allocation \" You might expect a pointer to a member function to occupy 4 bytes, just as pointers to functions do. However, pointers to methods are actually little tagged unions. They deal with multiple virtual inheritance and virtual/nonvirtual functions.\" Implementing Singletons Addressing the Dead Reference Problem ( II ): Singletons with Longevity \" The concept emerging here is that of longevity control and is independent of the concept of a singleton: The greater longevity an object has, the later it will be destroyed. It doesn't matter whether the object is a singleton or some global dynamically allocated object.\" The Double-Checked Locking Pattern \" Very experienced multithreaded programmers know that even the Double-Checked Locking pattern, although correct on paper, is not always correct in practice. In certain symmetric multiprocessor environments (the ones featuring the so-called relaxed memory model), the writes are committed to the main memory in bursts, rather than one by one. The bursts occur in increasing order of addresses, not in chronological order.\" \" Thus, sadly, the Double-Checked Locking pattern is known to be defective for such systems.\" \" Usually the platform offers alternative, nonportable concurrency-solving primitives, such as memory barriers, which ensure ordered access to memory.\" \" A reasonable compiler should generate correct, nonspeculative code around volatile objects.\" Smart Pointers \" Smart pointers are C++ objects that simulate simple pointers by implementing operator-> and the unary operator* . In addition to sporting pointer syntax and semantics, smart pointers often perform useful tasks - such as memory management or locking - under the covers, thus freeing the application from carefully managing the lifetime of pointed-to objects.\" The Deal \" Smart pointers have value semantics, whereas some simple pointers do not. An object with value semantics is an object that you can copy and assign to. A plain int is the perfect example of a first-class object. You can create, copy, and change integer values freely. A pointer that you use to iterate in a buffer also has value semantics - you initialize it to point to the beginning of the buffer, and you bump it until you reach the end. Along the way, you can copy its value to other variables to hold temporary results. With pointers that hold values allocated with new , however, the story is very different. Once you have written Widget * p = new Widget ; the variable p not only points to, but also owns , the memory allocated for the Widget object. This is because later you must issue delete p to ensure that the Widget object is destroyed and its memory is released.\" \" In short, in the smart pointers' world, ownership is an important topic. By providing ownership management, smart pointers are able to support integrity guarantees and full value semantics. Because ownership has much to do with constructing, copying, and destroying smart pointers, it's easy to figure out that these are the most vital functions of a smart pointer.\" Storage of Smart Pointers \" Each type that's hardcoded in a piece of generic code decreases the genericity of the code. Hardcoded types are to generic code what magic constants are to regular code.\" \" When you apply operator-> to a type that's not a built-in pointer, the compiler does an interesting thing. After looking up and applying the user-defined operator-> to that type, it applies operator-> again to the result. The compiler keeps doing this recursively until it reaches a native pointer, and only then proceeds with member access. It follows that a smart pointer's operator-> does not have to return a pointer. It can return an object that in turn implements operator-> , without changing the use syntax.\" \" If you return an object of type PointerType by value from operator-> , the sequence of execution is as follows: Constructor of PointerType PointerType::operator-> called; likely returns a pointer to an object of type PointeeType Member access for PointeeType - likely a function call Destructor of PointerType \" In a nutshell, you have a nifty way of implementing locked function calls. This idiom has broad uses with multithreading and locked resource access. You can have PointerType ‘ s constructor lock the resource, and then you can access the resource; finally, PointerType ‘ s destructor unlocks the resource.\" Ownership-Handling Strategies \" A smart pointer is a first-class value that takes care of deleting the pointed-to object under the covers. The client can intervene in the pointee object's lifetime by issuing calls to helper management functions.\" \" self-ownership, smart pointers must carefully track the pointee object, especially during copying, assignment, and destruction.\" Copy on Write \" The idea that underlies COW is to clone the pointee object at the first attempt of modification; until then, several pointers can share the same object. Smart pointers, however, are not the best place to implement COW , because smart pointers cannot differentiate between calls to const and non- const member functions of the pointee object.\" \" Function invocations for the pointee object happen somewhere beyond the reach of the smart pointer.\" Reference Counting \" You should not keep dumb pointers and smart pointers to the same object.\" \" The actual counter must be shared among smart pointer objects\" \" Reference management - be it counting or linking - is a victim of the resource leak known as cyclic reference .\" Destructive Copy \" C++ etiquette calls for the right-hand side of the copy constructor and the assignment operator to be a reference to a const object. Classes that foster destructive copy break this convention for obvious reasons. Because etiquette exists for a reason, you should expect negative consequences if you break it.\" \" Because they do not support value semantics, smart pointers with destructive copy cannot be stored in standard containers and in general must be handled with almost as much care as raw pointers.\" \" On the bright side, smart pointers with destructive copy have significant advantages: They incur almost no overhead. They are good at enforcing ownership transfer semantics. They are good as return values from functions. They are excellent as stack variables in functions that have multiple return paths. \" The Address-of Operator \" There are two reasons why overloading unary operator& is not a very good idea. One reason is that exposing the address of the pointed-to object implies giving up any automatic ownership management. […] The second reason, a more pragmatic one, is that overloading unary operator& makes the smart pointer unusable with STL containers. Actually, overloading unary operator& for a type pretty much makes generic programming impossible for that type, because the address of an object is too fundamental a property to play with naively. Most generic code assumes that applying & to an object of type T returns an object of type T* […] address-of is a fundamental concept. If you defy this concept, generic code behaves strangely either at compile time or - worse - at runtime.\" Putting It All Together \" A rule for all policies is that they must have value semantics; that is, they must define a proper copy constructor and assignment operator.\" Object Factories \" […] subject to the paradox of ‘virtual constructors'. You need virtual constructors when the information about the object to be created is inherently dynamic and cannot be used directly with C++ constructs.\" \" This marks a fundamental difference between creating objects and invoking virtual member functions in C++. Virtual member functions are fluid, dynamic - you can change their behavior without changing the call site. In contrast, each object creation is a stumbling block of statically bound, rigid code.\" The Need for Object Factories \" […] an object factory may be needed. When you save an object to a file, you must save its actual type in the form of a string, an integral value, an identifier of some sort. Thus, although the type information exists, its form does not allow you to create C++ objects.\" Object Factories in C++: Classes and Objects \" In C++, classes and objects are different beasts. Classes are what the programmer creates, and objects are what the program creates. You cannot create a new class at runtime, and you cannot create an object at compile time. Classes don't have first-class status: You cannot copy a class, store it in a variable, or return it from a function.\" \" In C++ there is a fracture between types and values: A value has a type attribute, but a type cannot exist on its own. If you want to create an object in a totally dynamic way, you need a means to express and pass around a ‘pure' type and build a value from it on demand. Because you cannot do this, you somehow must represent types as objects - integers, strings, and so on. Then, you must employ some trick to exchange the value for the right type, and finally to use that type to create an object.\" Abstract Factory \" However, the more you reduce dependencies, the more you also reduce type knowledge, and consequently the more you undermine the type safety of your design. This is yet another instance of the classic dilemma of better type safety versus lesser dependencies that often appears in C++\" \" Type2Type is a simple template whose unique purpose is to disambiguate overloaded functions.\" Visitor \" Visitor gives you a surprising amount of flexibility in a certain area: You can add virtual functions to a class hierarchy without recompiling them or their existing clients. However, this flexibility comes at the expense of disabling features that designers take for granted: You cannot add a new leaf class to the hierarchy without recompiling the hierarchy and all its clients.\" \" Visitor's operational area is limited to very stable hierarchies (you seldom add new classes) and heavy processing needs (you often add new virtual functions).\" \" Visitor goes against programmers' intuition; therefore, a careful implementation and rigorous discipline are essential to using it successfully.\" Visitor Basics \" In a nutshell, from a dependency standpoint, new classes are easy to add, and new virtual member functions are difficult to add.\" \" Visitor applies best when operations on objects are distinct and unrelated.\" \" A type switch occurs whenever you query a polymorphic object on its concrete type and perform different operations with it depending on what that concrete type is.\" Back to the \"Cyclic\" Visitor \" If you use dynamic_cast against some object, the runtime support has quite a few things to do. The RTTI code must figure out whether the conversion to the target type is legal and, if it is, must compute a pointer to that target type.\" \" Let's detail a bit how a compiler writer can achieve this. One reasonable solution is to assign a unique integral identifier to each type in the program. The integral identifier also comes in handy when it comes to exception handling, so it's quite a wise integrating solution. Then in each class's virtual table, the compiler puts (a pointer to) a table of identifiers of all its subtypes. Together with these identifiers, the compiler has to store the offsets of the relative positions of the subobjects within the big object. This would be enough information to perform a dynamic cast correctly.\" \" Details - such as multiple inheritance - render the dynamic cast code even more complicated and slower.\" \" dynamic_cast does have a cost, which is unpredictable and can become unacceptable for some particular needs of an application.\" Summary \" Essentially, Visitor allows you to add virtual functions to a class hierarchy without modifying the classes in that hierarchy. In some cases, Visitor can lead to a clever, extensible design.\" Multimethods \" The C++ virtual function mechanism allows dispatching of a call depending on the dynamic type of one object. The multimethods feature allows dispatching of a function call depending on the types of multiple objects. A universally good implementation requires language support, which is the route that languages such as CLOS , ML , Haskell, and Dylan have taken. C++ lacks such support, so its emulation is left to library writers.\" What Are Multimethods? \" Two types of polymorphism are implemented in C++: Compile-time polymorphism, supported by overloading and template functions Runtime polymorphism, implemented with virtual functions \" \" Overloading and template functions scale to multiple objects naturally.\" \" Unfortunately, virtual functions - the only mechanism that implements runtime polymorphism in C++ - are tailored for one object only. Even the call syntax - obj.Fun(arguments) - gives obj a privileged role over arguments \" The Logarithmic Dispatcher and Casts \" A template can accept a pointer to a function as a nontype template parameter. […] A template is allowed to accept pointers to global objects, including functions, as nontype template parameters. The only condition is that the function whose address is used as a template argument must have external linkage.\" \" You can easily transform static functions into functions with external linkage by removing static and putting them into unnamed namespaces.\" Converting Arguments: static_cast or dynamic_cast ? \" Virtual inheritance provides a means for several derived classes to share the same base class object.\" \" you must use dynamic_cast if you have a hierarchy using virtual inheritance.\" \" The `dynamic_cast operator is designed to reach the right object in a class hierarchy, no matter how intricate its structure is.\" \" dynamic_cast is much slower than static_cast . Its power comes at a cost.\" \" What is double dispatching? You can see it as finding a handler function (or functor) in a two-dimensional space. On one axis are the types of the left-hand operator. On the other axis are the types of the right-hand operator. At the intersection between two types, you find their respective handler function.\" Summary \" Multimethods are dispatched depending on multiple classes simultaneously. This allows you to implement virtual functions for collections of types instead of one type at a time.\" \" Multimethods are needed in applications that call algorithms that depend on the type of two or more objects. Typical examples include collisions between polymorphic objects, intersections, and displaying objects on various target devices.\"","tags":"Programming","url":"modern_cpp_design.html","loc":"modern_cpp_design.html"},{"title":"Structure and Interpretation of Computer Programs","text":"Notes to the book Structure and Interpretation of Computer Programs, Second Edition Harold Abelson, Gerald Jay Sussman, Julie Sussman MIT Press My Github repository with examples See also Scheme (Lisp) 1 Building Abstractions with Procedures 1.1 The Elements of Programming \" [The] language provides for combining simple ideas to form more complex ideas. Every powerful language has three mechanisms for accomplishing this: primitive expressions , which represent the simplest entities the language is concerned with, means of combination , by which compound elements are built from simpler ones, and means of abstraction , by which compound elements can be named and manipulated as units.\" 1.1.3 Evaluating Combinations \" To evaluate a combination, do the following: Evaluate the subexpressions of the combination. Apply the procedure that is the value of the leftmost subexpression (the operator) to the arguments that are the values of the other subexpressions (the operands). \" \" […] first perform the evaluation process on each element of the combination. Thus, the evaluation rule is recursive in nature\" 1.1.5 The Substitution Model for Procedure Application Applicative order versus normal order \" [The] interpreter first evaluates the operator and operands and then applies the resulting procedure to the resulting arguments.\" \" [An] alternative evaluation model would not evaluate the operands until their values were needed. Instead it would first substitute operand expressions for parameters until it obtained an expression involving only primitive operators, and would then perform the evaluation.\" \" This alternative ‘fully expand and then reduce' evaluation method is known as normal-order evaluation , in contrast to the ‘evaluate the arguments and then apply' method that the interpreter actually uses, which is called applicative-order evaluation .\" \" Normal-order and applicative-order evaluation produce the same value.\" \" Lisp uses applicative-order evaluation.\" \" Lisp obeys the convention that every expression has a value.\" 1.2.1 Linear Recursion and Iteration \" In the iterative case, the program variables provide a complete description of the state of the process at any point.\" \" In contrasting iteration and recursion, we must be careful not to confuse the notion of a recursive process with the notion of a recursive procedure . When we describe a procedure as recursive, we are referring to the syntactic fact that the procedure definition refers (either directly or indirectly) to the procedure itself. But when we describe a process as following a pattern that is, say, linearly recursive, we are speaking about how the process evolves, not about the syntax of how a procedure is written.\" \" One reason that the distinction between process and procedure may be confusing is that most implementations of common languages (including Ada, Pascal, and C) are designed in such a way that the interpretation of any recursive procedure consumes an amount of memory that grows with the number of procedure calls, even when the process described is, in principle, iterative. As a consequence, these languages can describe iterative processes only by resorting to special-purpose ‘looping constructs' such as do , repeat , until , for , and while .\" \" any iterative process can be realized ‘in hardware' as a machine that has a fixed set of registers and no auxiliary memory. In contrast, realizing a recursive process requires a machine that uses an auxiliary data structure known as a stack .\" 1.3 Formulating Abstractions with Higher-Order Procedures 1.3.2 Constructing Procedures Using lambda Using let to create local variables \" we could use a lambda expression to specify an anonymous procedure for binding our local variables The general form of a let expression is ( let (( <var 1 > <exp 1 > ) ( <var 2 > <exp 2 > ) ... ( <var n> <exp n> )) <body> ) the let expression is interpreted as an alternate syntax for (( lambda ( <var 1 > ... <var n> ) <body> ) <exp 1 > ... <exp n> ) No new mechanism is required in the interpreter in order to provide local variables. A let expression is simply syntactic sugar for the underlying lambda application.\" \" Let allows one to bind variables as locally as possible to where they are to be used.\" Abstractions and first-class procedures \" programming languages impose restrictions on the ways in which computational elements can be manipulated. Elements with the fewest restrictions are said to have first-class status.\" \" Some of the ‘rights and privileges' of first-class elements are: They may be named by variables. They may be passed as arguments to procedures. They may be returned as the results of procedures. They may be included in data structures.\" 2 Building Abstractions with Data 2.1.3 What Is Meant by Data? \" In general, we can think of data as defined by some collection of selectors and constructors, together with specified conditions that these procedures must fulfill in order to be a valid representation.\" \" we could implement cons , car , and cdr without using any data structures at all but only using procedures. Here are the definitions: ( define ( cons x y ) ( define ( dispatch m ) ( cond (( = m 0 ) x ) (( = m 1 ) y ) ( else ( error \"Argument not 0 or 1: CONS\" m )))) dispatch ) ( define ( car z )( z 0 )) ( define ( cdr z ) ( z 1 )) 2.2 Hierarchical Data and the Closure Property \" The ability to create pairs whose elements are pairs is the essence of list structure's importance as a representational tool. We refer to this ability as the closure property of cons. In general, an operation for combining data objects satisfies the closure property if the results of combining things with that operation can themselves be combined using the same operation.\" 2.2.3 Sequences as Conventional Interfaces Sequence Operations \" The key to organizing programs so as to more clearly reflect the signal-flow structure is to concentrate on the ‘signals' that flow from one stage in the process to the next. If we represent these signals as lists, then we can use list operations to implement the processing at each of the stages.\" \" The value of expressing programs as sequence operations is that this helps us make program designs that are modular, that is, designs that are constructed by combining relatively independent pieces. We can encourage modular design by providing a library of standard components together with a conventional interface for connecting the components in flexible ways.\" \" The Lisp community also (unfortunately) uses the word \"closure\" to describe a totally unrelated concept: A closure is an implementation technique for representing procedures with free variables.\" \" Unlike Lisp with its pairs, these languages have no built-in general-purpose glue that makes it easy to manipulate compound data in a uniform way.\" 3 Modularity, Objects, and State 3.1 Assignment and Local State 3.1.1 Local State Variables \" The set! special form, whose syntax is:\" set! <name> <new-value> \" Here <name> is a symbol and <new-value> is any expression. Set! changes <name> so that its value is the result obtained by evaluating <new-value> .\" \" The value of a set! expression is implementation-dependent. Set! should be used only for its effect, not for its value.\" 3.1.2 The Benefits of Introducing Assignment \" From the point of view of one part of a complex process, the other parts appear to change with time. They have hidden time-varying local state. If we wish to write computer programs whose structure reflects this decomposition, we make computational objects whose behavior changes with time. We model state with local state variables, and we model the changes of state with assignments to those variables.\" 3.1.3 The Costs of Introducing Assignment Sameness and change \" A language that supports the concept that \"equals can be substituted for equals\" in an expression without changing the value of the expression is said to be referentially transparent. Referential transparency is violated when we include set! in our computer language. This makes it tricky to determine when we can simplify expressions by substituting equivalent expressions. Consequently, reasoning about programs that use assignment becomes drastically more difficult.\" \" The phenomenon of a single computational object being accessed by more than one name is known as aliasing.\" Pitfalls of imperative programming \" programs written in imperative style are susceptible to bugs that cannot occur in functional programs.\" 3.2 The Environment Model of Evaluation \" Once we admit assignment into our programming language […], a variable can no longer be considered to be merely a name for a value. Rather, a variable must somehow designate a \"place\" in which values can be stored. In our new model of evaluation, these places will be maintained in structures called environments.\" \" An environment is a sequence of frames. Each frame is a table (possibly empty) of bindings, which associate variable names with their corresponding values.\" \" Each frame also has a pointer to its enclosing environment […]. The value of a variable with respect to an environment is the value given by the binding of the variable in the first frame in the environment that contains a binding for that variable. If no frame in the sequence specifies a binding for the variable, then the variable is said to be unbound in the environment.\" \" Indeed, one could say that expressions in a programming language do not, in themselves, have any meaning. Rather, an expression acquires a meaning only with respect to some environment in which it is evaluated.\" 3.2.1 The Rules for Evaluation \" In the environment model of evaluation, a procedure is always a pair consisting of some code and a pointer to an environment. Procedures are created in one way only: by evaluating a \\(\\lambda\\) -expression. This produces a procedure whose code is obtained from the text of the \\(\\lambda\\) -expression and whose environment is the environment in which the \\(\\lambda\\) -expression was evaluated to produce the procedure.\" \" In general, define creates definitions by adding bindings to frames.\" \" Now that we have seen how procedures are created, we can describe how procedures are applied. The environment model specifies: To apply a procedure to arguments, create a new environment containing a frame that binds the parameters to the values of the arguments. The enclosing environment of this frame is the environment specified by the procedure. Now, within this new environment, evaluate the procedure body.\" \" The environment model of procedure application can be summarized by two rules: A procedure object is applied to a set of arguments by constructing a frame, binding the formal parameters of the procedure to the arguments of the call, and then evaluating the body of the procedure in the context of the new environment constructed. The new frame has as its enclosing environment the environment part of the procedure object being applied. A procedure is created by evaluating a \\(\\lambda\\) -expression relative to a given environment. The resulting procedure object is a pair consisting of the text of the \\(\\lambda\\) -expression and a pointer to the environment in which the procedure was created.\" \" We also specify that defining a symbol using define creates a binding in the current environment frame and assigns to the symbol the indicated value.\" \" Evaluating the expression ( set! <variable> <value> ) in some environment locates the binding of the variable in the environment and changes that binding to indicate the new value. That is, one finds the first frame in the environment that contains a binding for the variable and modifies that frame. If the variable is unbound in the environment, then set! signals an error.\" 3.3 Modeling with Mutable Data 3.3.1 Mutable List Structure Sharing and identity \" One way to detect sharing in list structures is to use the predicate eq? , […] as a way to test whether two symbols are equal. More generally, ( eq? x y ) tests whether x and y are the same object (that is, whether x and y are equal as pointers).\" Mutation is just assignment \" We can implement mutable data objects as procedures using assignment and local state.\" \" Assignment is all that is needed, theoretically, to account for the behavior of mutable data. As soon as we admit set! to our language, we raise all the issues, not only of assignment, but of mutable data in general.\" \" On the other hand, from the viewpoint of implementation, assignment requires us to modify the environment, which is itself a mutable data structure. Thus, assignment and mutation are equipotent: Each can be implemented in terms of the other.\" 4 Metalinguistic Abstraction 4.1 The Metacircular Evaluator \" The model has two basic parts: To evaluate a combination (a compound expression other than a special form), evaluate the subexpressions and then apply the value of the operator subexpression to the values of the operand subexpressions. To apply a compound procedure to a set of arguments, evaluate the body of the procedure in a new environment. To construct this environment, extend the environment part of the procedure object by a frame in which the formal parameters of the procedure are bound to the arguments to which the procedure is applied.\" \" The job of the evaluator is not to specify the primitives of the language, but rather to provide the connective tissue - the means of combination and the means of abstraction - that binds a collection of primitives to form a language. Specifically: The evaluator enables us to deal with nested expressions. The evaluator allows us to use variables. […] We need an evaluator to keep track of variables and obtain their values before invoking the primitive procedures. The evaluator allows us to define compound procedures. This involves keeping track of procedure definitions, knowing how to use these definitions in evaluating expressions, and providing a mechanism that enables procedures to accept arguments.* The evaluator provides the special forms, which must be evaluated differently from procedure calls.\" 4.1.1 The Core of the Evaluator \" The evaluation process can be described as the interplay between two procedures: eval and apply .\" Eval \" Eval takes as arguments an expression and an environment. It classifies the expression and directs its evaluation. Eval is structured as a case analysis of the syntactic type of the expression to be evaluated.\" \" Each type of expression has a predicate that tests for it and an abstract means for selecting its parts. This abstract syntax makes it easy to see how we can change the syntax of the language by using the same evaluator.\" Primitive expressions \" For self-evaluating expressions, such as numbers, eval returns the expression itself. Eval must look up variables in the environment to find their values. For quoted expressions, eval returns the expression that was quoted. An assignment to (or a definition of) a variable must recursively call eval to compute the new value to be associated with the variable. The environment must be modified to change (or create) the binding of the variable. An if expression requires special processing of its parts, so as to evaluate the consequent if the predicate is true, and otherwise to evaluate the alternative. A lambda expression must be transformed into anapplicable procedure by packaging together the parameters and body specified by the lambda expression with the environment of the evaluation. A begin expression requires evaluating its sequence of expressions in the order in which they appear. A case analysis ( cond ) is transformed into a nest of if expressions and then evaluated.\" 4.1.4 Running the Evaluator as a Program \" Our evaluator program reduces expressions ultimately to the application of primitive procedures.\" \" We thus set up a global environment that associates unique objects with the names of the primitive procedures that can appear in the expressions we will be evaluating.\" \" The global environment also includes bindings for the symbols true and false .\" 4.1.5 Data as Programs \" Turing presented a simple computational model - now known as a Turing machine - and argued that any \"effective process\" can be formulated as a program for such a machine. (This argument is known as the Church-Turing thesis .) Turing then implemented a universal machine, i.e., a Turing machine that behaves as an evaluator for Turing-machine programs.\" \" An evaluator, which is implemented by a relatively simple procedure, can emulate programs that are more complex than the evaluator itself. The existence of a universal evaluator machine is a deep and wonderful property of computation.\" 4.2.1 Normal Order and Applicative Order \" We noted that Scheme is an applicativeorder language, namely, that all the arguments to Scheme procedures are evaluated when the procedure is applied. In contrast, normal-orde r languages delay evaluation of procedure arguments until the actual argument values are needed. Delaying evaluation of procedure arguments until the last possible moment (e.g., until they are required by a primitive operation) is called lazy evaluation \" 4.2.2 An Interpreter with Lazy Evaluation \" The basic idea is that, when applying a procedure, the interpreter must determine which arguments are to be evaluated and which are to be delayed. The delayed arguments are not evaluated; instead, they are transformed into objects called thunks .\" \" The word thunk was invented by an informal working group that was discussing the implementation of call-by-name in Algol 60. They observed that most of the analysis of (\"thinking about\") the expression could be done at compile time; thus, at run time, the expression would already have been \"thunk\" about\" 4.3 Variations on a Scheme - Nondeterministic Computing \" With nondeterministic evaluation, an expression represents the exploration of a set of possible worlds, each determined by a set of choices. Some of the possible worlds lead to dead ends, while others have useful values. The nondeterministic program evaluator supports the illusion that time branches, and that our programs have different possible execution histories. When we reach a dead end, we can revisit a previous choice point and proceed along a different branch.\" 4.3.1 Amb and Search \" Abstractly, we can imagine that evaluating an amb expression causes time to split into branches, where the computation continues on each branch with one of the possible values of the expression. We say that amb represents a nondeterministic choice point.\" \" It is better to systematically search all possible execution paths. The amb evaluator […] implements a systematic search as follows: When the evaluator encounters an application of `amb, it initially selects the first alternative. This selection may itself lead to a further choice. The evaluator will always initially choose the first alternative at each choice point. If a choice results in a failure, then the evaluator automagically backtracks to the most recent choice point and tries the next alternative.\" 4.3.3 Implementing the Amb Evaluator \" The evaluation of an ordinary Scheme expression may return a value, may never terminate, or may signal an error. In nondeterministic Scheme the evaluation of an expression may in addition result in the discovery of a dead end, in which case evaluation must backtrack to a previous choice point. The interpretation of nondeterministic Scheme is complicated by this extra case.\" Execution procedures and continuations \" Recall that the execution procedures for the ordinary evaluator take one argument: the environment of execution. In contrast, the execution procedures in the amb evaluator take three arguments: the environment, and two procedures called continuation procedures . The evaluation of an expression will finish by calling one of these two continuations: If the evaluation results in a value, the success continuation is called with that value; if the evaluation results in the discovery of a dead end, the failure continuation is called.\" \" The failure continuation in hand at that point will cause the most recent choice point to choose another alternative. If there are no more alternatives to be considered at that choice point, a failure at an earlier choice point is triggered, and so on.\" \" In addition, if a side-effect operation (such as assignment to a variable) occurs on a branch of the process resulting from a choice, it may be necessary, when the process finds a dead end, to undo the side effect before making a new choice.\" \" When the failure continuation for an amb runs out of choices, it calls the failure continuation that was originally given to the amb , in order to propagate the failure back to the previous choice point or to the top level.\" 4.4 Logic Programming \" Computer science deals with imperative (how to) knowledge, whereas mathematics deals with declarative (what is) knowledge.\" \" In a nondeterministic language, expressions can have more than one value, and, as a result, the computation is dealing with relations rather than with single-valued functions. Logic programming extends this idea by combining a relational vision of programming with a powerful kind of symbolic pattern matching called unification .\" 5 Computing with Register Machines 5.1 Designing Register Machines \" To design a register machine, we must design its data paths (registers and operations) and the controller that sequences these operations.\" 5.1.4 Using a Stack to Implement Recursion \" Since there is no a priori limit on the depth of nested re- cursive calls, we may need to save an arbitrary number of register values. These values must be restored in the re- verse of the order in which they were saved, since in a nest of recursions the last subproblem to be entered is the first to be finished. This dictates the use of a stack .\" \" Although in principle the factorial computation requires an infinite machine, the machine in […] is actually finite except for the stack, which is potentially unbounded.\" \" When a recursive sub- problem is encountered, we save on the stack the registers whose current values will be required after the subproblem is solved, solve the recursive subproblem, then restore the saved registers and continue execution on the main problem.\" \" [The] registers that need to be saved depends on the particular machine, since not all recursive computations need the original values of registers that are modified during solution of the subproblem.\" 5.2.2 The Assembler \" The assembler transforms the sequence of controller expressions for a machine into a corresponding list of machine instructions, each with its execution procedure.\" 5.3 Storage Allocation and Garbage Collection 5.3.1 Memory as Vectors \" Memory addresses can be incremented to support sequential access to some set of the cubbyholes. More generally, many important data operations require that memory addresses be treated as data, which can be stored in memory locations and manipulated in machine registers. The representation of list structure is one application of such address arithmetic .\" \" In order to describe memory operations, we use two primitive Scheme procedures for manipulating vectors:\" (vector-ref <vector> <n>) returns the \\(n&#94;{th}\\) element of the vector. (vector-set! <vector> <n> <value>) sets the \\(n&#94;{th}\\) element of the vector to the designated value. Representing Lisp data \" Let us imagine that computer memory is divided into two vectors: the-cars and the-cdrs . We will represent list structure as follows: A pointer to a pair is an index into the two vectors. The car of the pair is the entry in the-cars with the designated index, and the cdr of the pair is the entry in the-cdrs with the designated index. We also need a representation for objects other than pairs (such as numbers and symbols) and a way to distinguish one kind of data from another. […] using typed pointers , that is, to extending the notion of \"pointer\" to include information on data type.\" 5.3.2 Maintaining the Illusion of Infinite Memory \" Garbage collection is based on the observation that, at any moment in a Lisp interpretation, the only objects that can affect the future of the computation are those that can be reached by some succession of car and cdr operations starting from the pointers that are currently in the machine registers.\" \" We assume here that the stack is represented as a list […], so that items on the stack are accessible via the pointer in the stack register.\" 5.4.1 The Core of the Explicit-Control Evaluator Evaluating simple expressions \" Numbers and strings (which are self-evaluating), variables, quotations, and lambda expressions have no subexpressions to be evaluated.\" Evaluating procedure applications \" A procedure application is specified by a combination containing an operator and operands. The operator is a subexpression whose value is a procedure, and the operands are subexpressions whose values are the arguments to which the procedure should be applied.\" \" Saving registers whose contents will not be needed later may also hold onto useless data that could otherwise be garbage-collected, freeing space to be reused.\" 5.4.4 Running the Evaluator \" we have explored successively more precise models of the evaluation process. We started with the relatively informal substitution model, then extended this […] to the environment model, which enabled us to deal with state and change. In the metacircular evaluator […], we used Scheme itself as a language for making more explicit the environment structure constructed during evaluation of an expression. Now, with register machines, we have taken a close look at the evaluator's mechanisms for storage management, argument passing, and control. At each new level of description, we have had to raise issues and resolve ambiguities that were not apparent at the previous, less precise treatment of evaluation. To understand the behavior of the explicit-control evaluator, we can simulate it and monitor its performance.\" \" There are two common strategies for bridging the gap between higher-level languages and register-machine languages.\" \" An interpreter written in the native language of a machine configures the machine to execute programs written in a language (called the source language ) that may differ from the native language of the machine performing the evaluation. The primitive procedures of the source language are implemented as a library of subroutines written in the native language of the given machine. A program to be interpreted (called the source program ) is represented as a data structure. The interpreter traverses this data structure, analyzing the source program. As it does so, it simulates the intended behavior of the source program by calling appropriate primitive subroutines from the library.\" \" […] the alternative strategy of compilation . A compiler for a given source language and machine translates a source program into an equivalent program (called the object program ) written in the machine's native language.\" \" In view of the complementary advantages of compi-lation and interpretation, modern program-development environments pursue a mixed strategy. Lisp interpreters are generally organized so that interpreted procedures and compiled procedures can call each other.\" 5.5.1 Structure of the Compiler Targets and linkages \" Compile and the code generators that it calls take two arguments in addition to the expression to compile. There is a target , which specifies the register in which the compiled code is to return the value of the expression. There is also a linkage descriptor , which describes how the code resulting from the compilation of the expression should proceed when it has finished its execution. The linkage descriptor can require that the code do one of the following three things: continue at the next instruction in sequence (this is specified by the linkage descriptor next ), return from the procedure being compiled (this is spec- ified by the linkage descriptor return ), or jump to a named entry point (this is specified by using the designated label as the linkage descriptor).\" Instruction sequences and stack usage \" An instruction sequence will contain three pieces of information: the set of registers that must be initialized before the instructions in the sequence are executed (these registers are said to be needed by the sequence), the set of registers whose values are modified by the instructions in the sequence, and * the actual instructions (also called statements ) in the sequence.\" 5.5.6 Lexical Addressing \" One of the most common optimizations performed by compilers is the optimization of variable lookup.\" \" Because our language is lexically scoped, the runtime environment for any expression will have a structure that parallels the lexical structure of the program in which the expression appears.\" \" This is not true if we allow internal definitions, unless we scan them out.\" \" We can exploit this fact by inventing a new kind of variable-lookup operation, lexical-address-lookup , that takes as arguments an environment and a lexical address that consists of two numbers: a frame number , which specifies how many frames to pass over, and a displacement number , which specifies how many variables to pass over in that frame.\" \" In order to generate such code, the compiler must be able to determine the lexical address of a variable it is about to compile a reference to.\" 5.5.7 Interfacing Compiled Code to the Evaluator Interpretation and compilation \" An interpreter raises the machine to the level of the user program; a compiler lowers the user program to the level of the machine language. We can regard the Scheme language (or any programming language) as a coherent family of abstractions erected on the machine language.\" \" We incur significant overhead if we insist that errors encountered in execution of a user program be detected and signaled, rather than being allowed to kill the system or produce wrong answers.\"","tags":"Programming","url":"structure_and_interpretation_of_computer_programs.html","loc":"structure_and_interpretation_of_computer_programs.html"},{"title":"Scheme: Frequently used Functions","text":"Pairs ( cons , car and cdr ) (cons a b) : Construct a pair (car p) : Get first element of pair (or list) (cdr p) : Get second element of pair (or rest of list) Lists (list a0 a1 a2 ... aN) : create a list Equivalent to (cons (cons a0 (cons a1 (cons a2 (cons ... (cons (cons aN nil))...))))) nil : end of list marker (equivalent to the empty list '() ) (null? p) : checks if the given element is nil (pair? e) : checks if the given element is a pair the empty list satisfies null? and also is not a pair (cons a lst) makes a list like the original one ( lst ), but with an additional item ( a ) at the beginning Example: ( define a ( list 1 2 3 )) ( cons 10 a ) >> ( 10 1 2 3 ) (list-ref lst i) : retrieves element at index i (index is zero-based) (length lst) : returns the length of a list (append lst1 lst2) : combines the elements of two lists into one new list (map fun lst) : applies the function fun to each element of lst and returns a new list with the results The map function can be applied on multiple lists: \" This more general map takes a procedure of n arguments, together with n lists, and applies the procedure to all the first elements of the lists, all the second elements of the lists, and so on, returning a list of the results. For example: \" ( map + ( list 1 2 3 ) ( list 40 50 60 ) ( list 700 800 900 )) >>> ( 741 852 963 ) ( map ( lambda ( x y ) ( + x ( * 2 y ))) ( list 1 2 3 ) ( list 4 5 6 )) >>> ( 9 12 15 ) SICP :2.2.1 Representing Sequences:Mapping over lists (for-each fun lst) : applies a function fun to each element in the list lst but doesn't create a new list. The return value of the function is ignored. \" The procedure for-each is similar to map. It takes as arguments a procedure and a list of elements. However, rather than forming a list of the results, for-each just applies the procedure to each of the elements in turn, from left to right. The values returned by applying the procedure to the elements are not used at all - for-each is used with procedures that perform an action, such as printing.\" SICP :2.2.1 Representing Sequences:Exercise 2.23 (flatten t) : flattens a tree structure into a list (filter p lst) : Creates a new list with the elements of lst for which the predicate p is satisfied (foldr f b lst) : applies the function f to the next element of the list and the result of the previous element. b is the initial begining value. The list is traversed from right to left. Also known as acumulate ( SICP :2.2.3 ) (foldl f b lst) : Same as foldlr but the list is traversed from left to right. (reduce f lst) : Same as foldl but take the first (leftmost) element as starting element. If reduce is not available (e.g in Racket) it can be defined in terms of foldl : ( define ( reduce f xs ) ( and ( not ( empty? xs )) ( foldl f ( first xs ) ( rest xs )))) Stack Overflow (memq v lst) : find element v in list lst and return the rest of the list starting at the found element. If element is not found #f is returned. Similar functions: member , memv , memf Local Binding let is syntactic sugar for a procedure call: ( let (( <var> <exp> )) <body> ) is interpreted as an alternate syntax for (( lambda ( <var> ) <body> ) <exp> ) Delayed Evaluation delay is a special form that returns a delayed object . It is a promise to evaluate the given expression at some future time. force takes a delayed object and performs the evaluation. It forces the delay to fulfill its promise. Delay can be a special form such that: ( delay <exp> ) is syntactic sugar for ( lambda () <exp> ) Force simply calls the procedure (of no arguments) pro- duced by delay, so we can implement force as a proce- dure: ( define ( force delayed-object ) ( delayed-object )) Dotted-tail Notation Used for functions with arbitrary number of arguments Example: ( define ( x a b . c ) ... ) ( x 1 2 3 4 5 ) ;; in the body of x: a=1, b=2, c='(3 4 5)","tags":"Programming","url":"scheme__frequently_used_functions.html","loc":"scheme__frequently_used_functions.html"},{"title":"Closure","text":"There are 2 concepts that are called closure Lambdas Closures are (anonymous) functions with free variables that capture the context of the calling function (stack). Wikipedia:Closure The Closure Property From SICP : 2.2 Hierarchical Data and the Closure Property : \" The ability to create pairs whose elements are pairs is the essence of list structure's importance as a representational tool. We refer to this ability as the closure property of cons . In general, an operation for combining data objects satisfies the closure property if the results of combining things with that operation can themselves be combined using the same operation.\" \" The use of the word ‘closure' here comes from abstract algebra, where a set of elements is said to be closed under an operation if applying the operation to elements in the set produces an element that is again an element of the set. The Lisp community also (unfortunately) uses the word \"closure\" to describe a totally unrelated concept: A closure is an implementation technique for representing procedures with free variables.\" Cons Cells Related to the closure property is the ability to create compound data (e.g pairs) only with functions: ( define ( cons a b ) ( lambda ( p ) ( p a b ))) ( define ( car p ) ( p ( lambda ( a b ) a ))) ( define ( cdr p ) ( p ( lambda ( a b ) b ))) Wikipedia:Cons","tags":"Programming","url":"closure.html","loc":"closure.html"},{"title":"OS X Keymap","text":"Swiss German keyboard layout for OS X, useful for programming. Download the keymap from github","tags":"Programming","url":"os_x_keymap.html","loc":"os_x_keymap.html"},{"title":"Regular Expressions","text":"See also the Python Standard Library Documentation My Repository on GitHub Metacharacters Twelve Metacharacters: Backslash \\ Caret &#94; Dollar $ Dot . Pipe | Question mark ? Asterisk * Plus + Opening and closing parenthesis ( and ) Character Classes Also known as character sets Grouped in [ and ] Example: Hex number [0-9a-fA-F] Invert character set with &#94; Predefined Character Classes Available in Python Element Description . Any character except newline \\n \\d Decimal digit, equivalent to [0-9] \\D Any non-digit character, equivalent to [&#94;0-9] \\s Any whitespace character, equivalent to [ \\t\\n\\r\\f\\v] \\S Any non-whitespace character, equivalent to [&#94; \\t\\n\\r\\f\\v] \\w Any alphanumeric character, equivalent to [a-zA-Z0-9_] \\W Any non-alphanumeric character, equivalent to [&#94;a-zA-Z0-9_] POSIX Character Classes POSIX defines some classes: Wikibooks not available in Python Alternation Alternation (or) is marked with | Quantifiers Quantifiers can be applied to characters, character sets, and to groups Optional (0 or 1 repetition): ? Zero (0) or more times: * One (1) or more times: + Exact repetition and ranges: {} Exactly n times: {n} Between n and m times (both inclusive): {n,m} At least n times: {n,} At most n times: {,n} Greedy and Reluctant Quantifiers greedy quantifiers will try to match as much as possible default behavior biggest possible result reluctant (non-greedy, lazy) will try to have smalles match possible extra ? to quantifier: ?? , *? and +? Boundary Matchers Identifiers Match &#94; Beginning of a line $ End of a line \\b At word boundary \\B Anything that is not word boundary (opposite of \\b ) \\A Beginning of the input \\Z End of the input Python Regex Functions RegexObject class in re module wrapper functions in re module match tries to match at beginning of string pos and slicing can have different results search is like match in most languages (e.g Perl) tries to match at any position in string Compilation Flags Grouping Subexpressions are grouped within ( and ) Used for different purposes: Creating subexpressions for applying quantifiers Limiting scope of an alternation Extract parts of the matched pattern (capturing) Using captured parts again in the regex Look Around Add subpatterns that are not in the result (not consuming characters) positive: subpattern needs to match negative: subpattern must not match Also called zero-width assertions Python re module allows look behind only with fixed size (sub-) patterns positive negative Look ahead (?=regex) (?!regex) Look behind (?<=regex) (?<!regex) Benchmarking General for Python: import cProfile cProfile . run ( \"myFunction\" )","tags":"Programming","url":"regular_expressions.html","loc":"regular_expressions.html"},{"title":"Unix Directory Structure","text":"Linux Directory Content /bin/ Essential commands and programs /sbin/ System binaries: mainly system administration programs /boot/ Files for bootloader /dev/ Device files /usr/ System utilities (Unix System Resources) /opt/ Optional software: mainly for users /etc/ System configuration files /home/ User home directories /root/ Root home directory /tmp/ Temporary files, deleted at reboot /kernel/ The operating system /srv/ Data for services /var/ Variable files (logs) /usr/lib/ Precompiled libraries (shared objects) /lib/ Precompiled libraries (shared objects) /media/ Removable devices are mounted here /mnt/ Temporary filesystems are mounted here /proc/ Virtual file system with live kernel information Any directory ending with bin/ contains binary executable files (or links to them) Procfs The /proc file system is a pseudo filesystem that allows to get informations from the kernel. The man page can be found with: man 5 proc There are also informations available in the /sys file system. Processes Each running process has a subfolder in /proc which is named after its PID . Directory Content /proc/$pid/cmdline The command line that was used to start the process. /proc/$pid/environ The environment of the process. /proc/self A link to the current process (can be used prom the process to get information about itself). /proc/$pid/fd/ A directory that contains references to file descriptors that the process uses. /proc/$pid/statm Information about the process memory. /proc/$pid/status General formated information: PID , real and effective UID and GID , memory use, bitmasks … /proc/$pid/stat Same as /proc/$pid/status . But not formated. /proc/$pid/cwd A symbolic link to the current working directory of the process. /proc/$pid/exe A reference to the executable file of the process. /proc/$pid/root A link to the root directory of the process (s.a. chroot() ). /proc/$pid/maps Contains memory mappings to files and libraries used by the process. This file can become very large. Kernel The command that was used to run the kernel at boot time can be found with cat /proc/cmdline More information about kernel parameters can be found with man bootparam The current locks can be found under /proc/locks Loaded kernel modules are listed under /proc/modules Information about filesystems are found under /proc/filesystems and /proc/mounts Hardware Directory Content /proc/devices List devices (character- and block-devices), HD 's, SSD 's, ports … /proc/dma List of used DMA channesl and the driver that uses it. /proc/interrupts List of used interrupts, type of IRQ , using modules and number of occureces /proc/ioports List of all used I/O port ( HD 's, ethernet, USB …). /proc/iomem Mapped hardware memory regions. /proc/stat General informations about processorS states (s.a. procinfo ) /proc/uptime Number of seconds that the system is running and idle time of CPU 's. /proc/scsi/ Directory with informations about SCSI devices /proc/scsi/scsi List of all SCSI devices. /proc/net/ Directory with information to networks /proc/loadavg Average workload (1 minute, 5 minutes, 15 minutes, active processes / number of processes, last used PID ) /proc/meminfo Information about memory. /proc/cpuinfo Information about CPU 's.","tags":"Programming","url":"unix_directory_structure.html","loc":"unix_directory_structure.html"},{"title":"Understanding Computation","text":"This page collects notes and citations from the book: Understanding Computation by Tom Stuart I. Programs and Machines \" To create an environment where […] computation can occur, we need three basic ingredients:\" \" A machine capable of performing the computation\" \" A language for writing instructions that the machine can understand\" \" A program written in that language, describing the exact computation that the machine should perform\" - page 18 The Meaning of Programs \" But computer programming isn't really about programs , it's about ideas \" - page 20 The Meaning of \"Meaning\" \" semantics is the study of the connection between words and their meanings\" - page 21 Operational Semantics Expression and Statements: \" The purpose of an expression is to be evaluated to produce another expression; a statement, on the other hand, is evaluated to make some change to the state of the abstract machine.\" - page 35 \" difference between expressions and statements. For expressions, we pass an environment into #reduce and get a reduced expression back; no new environment is returned\" - page 37 \" [ SIMPLE 's] expressions are pure and its statements are impure\" - page 37 \" conditional statements like « if (x) { y = 1 } else { y = 2 } » , which contain an expression called the condition ( « x » ), and two statements that we'll call the consequence ( « y = 1 » ) and the alternative ( « y = 2 » )\" - page 39 \" the latest R6RS standard for the Scheme programming language uses small-step semantics to describe its execution\" - page 45 \" small-step semantics has a mostly iterative flavor, requiring the abstract machine to repeatedly perform reduction steps\" - page 45 \" [Big-step semantics:] recursive rather than an iterative process\" - page 46 Small-step semantics: iterative Big-step semantics: recursive \" operational semantics is about explaining a language's meaning by designing an interpreter for it. By contrast, the language-to-language translation of denotational semantics is like a compiler\" - page 60 \" [It's] possible to compare two programs written in different languages, if a denotational semantics exists to translate both languages into some shared representation\" - page 62 \" Small-step semantics is also known as structural operational semantics and transition semantics\" - page 63 \" big-step semantics is more often called natural semantics or relational semantics\" - page 63 \" denotational semantics is also called fixed-point semantics or mathematical semantics\" - page 63 \" One alternative is axiomatic semantics\" - page 64 Design by Contract (pre-/post-conditions) \" Reducing an expression and an environment gives us a new expression, and we may reuse the old environment next time; reducing a statement and an environment gives us a new statement and a new environment.\" - page 70 \" alternative style of operational semantics, called reduction semantics , which explicitly separates these \"what do we reduce next?\" and \"how do we reduce it?\" phases by introducing so-called reduction contexts\" - page 71 The Simplest Computers \" each finite automaton has a hardcoded collection of rules that determine how it should move from one state to another in response to input\" - page 73 \" finite automata also have a rudimentary way of producing output\" - page 74 \" [Deterministic finite automaton:] it's always absolutely certain which state it will end up in\" - page 75 \" a string is accepted if there's some way for the NFA to end up in an accept state by following some of its rules—that is, if finishing in an accept state is possible , even if it's not inevitable.\" - page 81 \" The collection of strings that are accepted by a particular machine is called a language : we say that the machine recognizes that language.\" - page 82 \" those languages that can be recognized by finite automata are called regular languages\" - page 82 \" […] introducing another machine feature called free moves. These are rules that the machine may spontaneously follow without reading any input\" - page 88 \" The characters read by finite automata are usually called symbols , the rules for moving between states are called transitions , and the collection of rules making up a machine is called a transition function (or sometimes transition relation for NFAs)\" - page 91 \" NFA with free moves is known as an NFA -ε, and free moves themselves are usually called ε-transitions.\" - page 91 \" it's possible to convert any regular expression into an equivalent NFA —every string matched by the regular expression is accepted by the NFA , and vice versa — and then match a string by feeding it to a simulation of that NFA to see whether it gets accepted.\" - page 92 \" here are two kinds of extremely simple regular expression that are not built out of anything simpler: An empty regular expression. This matches the empty string and nothing else. A regular expression containing a single, literal character. For example, a and b are regular expressions that match only the strings ‘ a ‘ and ‘ b ‘ respectively.\" - page 92 \" combine them to build more complex expressions: Concatenate two patterns. We can concatenate the regular expressions a and b to get the regular expression ab , which only matches the string ‘ ab ‘ . Choose between two patterns, written by joining them with the | operator. We can join the regular expressions a or b to get the regular expression a|b , which matches the strings ‘ a ‘ and ‘ b ‘ . Repeat a pattern zero or more times, written by suffixing it with the * operator. We can suffix the regular expression a to get a* , which matches the strings ‘ a ‘ , ‘ aa ‘ , ‘ aaa ‘ , and so on, as well as the empty string \" (i.e., zero repetitions).\" - page 92 \" the * operator to bind more tightly than concatenation , which in turn binds more tightly than the | operator.\" - page 94 \" Any two NFAs can be concatenated by turning every accept state from the first NFA into a nonaccept state and connecting it to the start state of the second NFA with a free move\" - page 97 \" The start state of the first NFA \" \" The accept states of the second NFA \" \" All the rules from both NFAs\" \" Some extra free moves to connect each of the first NFA 's old accept states to the second NFA 's old start state\" - page 98 \" We can use a similar strategy to convert a Choose expression into an NFA .\" - page 98 \" A new start state\" \" All the accept states from both NFAs\" \" All the rules from both NFAs\" \" Two extra free moves to connect the new start state to each of the NFA 's old start states\" - page 100 \" We can do the same for any NFA […]. This time we need: A new start state, which is also an accept state All the accept states from the old NFA All the rules from the old NFA Some extra free moves to connect each of the old NFA 's accept states to its old start state Another extra free move to connect the new start state to the old start state - page 101 \" Free moves are useful for this conversion because they provide an unobtrusive way to glue together smaller machines into larger ones without affecting the behavior of any of the components.\" - page 102 \" Nondeterminism and free moves make it easier to design finite state machines to perform specific jobs\" - page 105 \" it's possible to convert any nondeterministic finite automaton into a deterministic one that accepts exactly the same strings\" - page 105 Just Add Power \" A finite state machine with a built-in stack is called a pushdown automaton ( PDA ), and when that machine's rules are deterministic, we call it a deterministic pushdown automaton ( DPDA ).\" - page 122 \" […] a PDA rule into five parts: The current state of the machine The character that must be read from the input (optional) The next state of the machine The character that must be popped off the stack The sequence of characters to push onto the stack after the top character has been popped off\" page 123 \" The assumption is that a PDA will always pop the top character off the stack, and then push some other characters onto the stack, every time it follows a rule. Each rule declares which character it wants to pop, and the rule will only apply when that character is on the top of the stack; if the rule wants that character to stay on the stack instead of getting popped, it can include it in the sequence of characters that get pushed back on afterward.\" - page 123 \" […] mark the bottom of the stack—the dollar sign, $ , is a popular choice\" - page 124 \" there are two important things to know about a pushdown automaton at each step of its computation: what its current state is, and what the current contents of its stack are. If we use the word configuration to refer to this combination of a state and a stack, we can talk about a pushdown automaton moving from one configuration to another as it reads input characters\" - page 126 \" there isn't an NPDA -to- DPDA algorithm.\" - page 139 \" Lexical analysis Read a raw string of characters and turn it into a sequence of tokens. Each token represents an individual building block of program syntax, like \"variable name,\" \"opening bracket,\" or \" while keyword.\" A lexical analyzer uses a language-specific set of rules called a lexical grammar to decide which sequences of characters should produce which tokens. This stage deals with messy character-level details like variable-naming rules, comments, and whitespace, leaving a clean sequence of tokens for the next stage to consume.\" - page 139 \" Syntactic analysis Read a sequence of tokens and decide whether they represent a valid program according to the syntactic grammar of the language being parsed. If the program is valid, the syntactic analyzer may produce additional information about its structure (e.g., a parse tree).\" - page 140 \" context-free grammar ( CFG ) Each rule has a symbol on the lefthand side and one or more sequences of symbols and tokens on the right\" - page 143 \" The technique for converting a CFG into a PDA works like this: Pick a character to represent each symbol from the grammar Use the PDA 's stack to store characters that represent grammar symbols and tokens. When the PDA starts, have it immediately push a symbol onto the stack to represent the structure it's trying to recognize. Translate the grammar rules into PDA rules that expand symbols on the top of the stack without reading any input. Each grammar rule describes how to expand a single symbol into a sequence of other symbols and tokens Give every token character a PDA rule that reads that character from the input and pops it off the stack These token rules work in opposition to the symbol rules. The symbol rules tend to make the stack larger, sometimes pushing several characters to replace the one that's been popped; the token rules always make the stack smaller, consuming input as they go. 5. Finally, make a PDA rule that will allow the machine to enter an accept state if the stack becomes empty - pages 143-145 \" the symbol rules repeatedly expand the symbol on the top of the stack until it gets replaced by a token, then the token rules consume the stack (and the input) until they hit a symbol. This back and forth eventually results in an empty stack as long as the input string can be generated by the grammar rules.\" \" This algorithm is called LL parsing. The first L stands for \"left-to-right,\" because the input string is read in that direction, and the second L stands for \"left derivation,\" because it's always the leftmost (i.e., uppermost) symbol on the stack that gets expanded.\" - page 146 \" The unlimited storage provided by a stack lets a PDA remember arbitrary amounts of information during a computation and refer back to it later.\" - page 148 \" There's a feedback loop between the rules and the stack—the contents of the stack affect which rules the machine can follow, and following a rule will affect the stack contents—which allows a PDA to store away information on the stack that will influence its future execution.\" - page 148 The Ultimate Machine Turing machine: \" […] unified rule format has five parts: The current state of the machine The character that must appear at the tape head's current position The next state of the machine The character to write at the tape head's current position The direction (left or right) in which to move the head after writing to the tape\" - page 156 \" we don't have to worry about free moves, because Turing machines don't have them.\" - pages 160 \" A Turing machine's next action is chosen according to its current state and the character currently underneath its tape head, so a deterministic machine can only have one rule for each combination of state and character—the \"no contradictions\" rule—in order to prevent any ambiguity over what its next action will be.\" - page 160 \" there's an implicit stuck state that the machine can go into when no rule applies \" - page 160 \" does adding nondeterminism make a Turing machine more powerful? In this case the answer is no: a nondeterministic Turing machine can't do any more than a deterministic one. Pushdown automata are the exception here, because both DFAs and DTMs have enough power to simulate their nondeterministic counterparts. A single state of a finite automaton can be used to represent a combination of many states, and a single Turing machine tape can be used to store the contents of many tapes, but a single pushdown automaton stack can't represent many possible stacks at once.\" - page 166 \" can we design a single machine that can read a program from its input and then do whatever job the program specifies? Perhaps unsurprisingly, a Turing machine is powerful enough to read the description of a simple machine from its tape - a deterministic finite automaton, say - and then run a simulation of that machine to find out what it does.\" - page 176 \" we are able to design a machine that can simulate any other DTM by reading its rules, accept states, and initial configuration from the tape and stepping through its execution, essentially acting as a Turing machine rulebook interpreter. A machine that does this is called a universal Turing machine ( UTM ).\" - page 177 \" We can write software - an encoded description of a Turing machine - onto a tape, feed that tape to the UTM , and have our software executed to produce the behavior we want.\" - page 177 \" One challenge is that every Turing machine has a finite number of states and a finite number of different characters it can store on its tape, with both of these numbers being fixed in advance by its rulebook, and a UTM is no exception. \" - page 178 II . Computation and Computability \" As programmers we work with languages and machines that are designed to fit our mental models of the world, and we expect them to come equipped with features that make it easy to translate our ideas into implementations. These human-centered designs are motivated by convenience rather than necessity\" - page 182 \" […] hard theoretical constraints: certain problems just can't be solved by any computer, no matter how fast and efficient it is.\" - page 182 Programming with Nothing Chruch numerals: \" Each number corresponds to a unique way of repeating an action: the number one corresponds to just performing the action; the number two corresponds to performing it and then performing it again; and so on. The number zero, unsurprisingly, corresponds to not performing the action at all.\" - page 189 \" […] Church encoding after Alonzo Church, the inventor of the lambda calculus\" - page 190 \" [Church numeral to integer] conversion: def to_integer ( proc ) proc [-> n { n + 1 } ][ 0 ] end This method takes a proc that represents a number and calls it with another proc (which just increments its argument) and the native Ruby number 0. \" - page 191 Church to boolean: def to_boolean ( proc ) proc [ true ][ false ] end \" This works by taking a proc that represents a Boolean and calling it with true as its first argument and false as its second. TRUE just returns its first argument, so to_boolean(TRUE) will return true , and likewise for FALSE […]\" - 193 \" In languages like Ruby, the if - else statement is nonstrict (or lazy ): we give it a condition and two blocks, and it evaluates the condition to decide which of the two blocks to evaluate and return — it never evaluates both.\" - page 200 \" Ruby […] evaluates both arguments before IF gets a chance to decide which one to return.\" - page 200 \" we can easily implement lists that calculate their contents on the fly, also known as streams . In fact, there's no reason why streams even need to be finite, because the calculation only has to generate the list contents as they're consumed\" - page 215 \" defining a data structure in terms of itself might seem weird and unusual; in this setting, they're exactly the same thing, and the Z combinator makes both completely legitimate.\" - page 216 \" Function calls are the only thing that actually happens when a lambda calculus program is evaluated\" - page 225 \" function calls are the only kind of syntax that can be reduced.\" - page 225 \" You might protest that 3 - 5 = 0 isn't called \"subtraction\" where you come from, and you'd be right: the technical name for this operation is \"monus\", because the nonnegative integers under addition form a commutative monoid instead of a proper abelian group.\" - page 229 Universality Is Everywhere \" Even though any individual Turing machine has a hardcoded rulebook, the universal Turing machine demonstrates that it's possible to design a device that can adapt to arbitrary tasks by reading instructions from a tape. These instructions are effectively a piece of software that controls the operation of the machine's hardware, just like in the general-purpose programmable computers we use every day.\" - page 231 \" The term Turing complete is often used to describe a system or programming language that can simulate any Turing machine.\" \" a Turing machine can act as an interpreter for the lambda calculus by storing a representation of a lambda calculus expression on the tape and repeatedly updating it according to a set of reduction rules\" - page 234 \" Since every Turing machine can be simulated by a lambda calculus program, and every lambda calculus program can be simulated by a Turing machine, the two systems are exactly equivalent in power.\" - page 234 Partial Recursive Functions \" partial recursive functions are programs that are constructed from four fundamental building blocks in different combinations. […] The first two building blocks are called zero and increment ** […] third building block, recurse […] recurse is just a template for defining a certain kind of recursive function. The programs that we can assemble out of zero , increment , and recurse are called the primitive recursive functions. All primitive recursive functions are total : regardless of their inputs, they always halt and return an answer. This is because recurse is the only legitimate way to define a recursive method, and recurse always halts: each recursive call makes the last argument closer to zero, and when it inevitably reaches zero, the recursion will stop. However, we can't simulate the full execution of an arbitrary Turing machine with primitive recursive functions, because some Turing machines loop forever, so primitive recursive functions aren't universal. To get a truly universal system we have to add a fourth fundamental operation, minimize : minimize takes a block and calls it repeatedly with a single numeric argument. For the first call, it provides 0 as the argument, then 1, then 2, and keeps calling the block with larger and larger numbers until it returns zero. By adding minimize to zero , increment , and recurse , we can build many more functions—all the partial recursive functions—including ones that don't always halt. With minimize , it's possible to fully simulate a Turing machine by repeatedly calling the primitive recursive function that performs a single simulation step. The simulation will continue until the machine halts - and if that never happens, it'll run forever. - pages 235 - 238 SKI Combinator Calculus \" The SKI calculus is even simpler, with only two kinds of expression - calls and alphabetic symbols - and much easier rules. All of its power comes from the three special symbols S , K , and I (called combinators ), each of which has its own reduction rule: Reduce S[a][b][c] to a[c][b[c]] , where a , b , and c can be any SKI calculus expressions. Reduce K[a][b] to a . Reduce I[a] to a . page 239 \" The SKI calculus can produce surprisingly complex behavior with its three simple rules—so complex, in fact, that it turns out to be universal.\" - page 243 \" Although the SKI calculus has three combinators, the I combinator is actually redundant. There are many expressions containing only S and K that do the same thing as I […] S[K][K] has the same behavior as I , and in fact, that's true for any SKI expression of the form S[K][whatever] . The I combinator is syntactic sugar that we can live without; just the two combinators S and K are enough for universality.\" - pages 245-246 Iota \" iota ( ɩ ) is an extra combinator that can be added to the SKI calculus. Here is its reduction rule: Reduce ɩ[a] to a[S][K] .\" - page 246 \" […] a language called Iota whose programs only use the ɩ combinator. Although it only has one combinator, Iota is a universal language \" - page 246 \" We can convert an SKI expression to Iota by applying these substitution rules: Replace S with ɩ[ɩ[ɩ[ɩ[ɩ]]]] . Replace K with ɩ[ɩ[ɩ[ɩ]]] . Replace I with ɩ[ɩ] . page 246 Tag Systems \" a tag system operates on a string by repeatedly adding new characters to the end of the string and removing them from the beginning.\" - page 248 \" A tag system's description has two parts: first, a collection of rules, where each rule specifies some characters to append to the string when a particular character appears at the beginning […] and second, a number, called the deletion number, which specifies how many characters to delete from the beginning of the string after a rule has been followed.\" - page 249 \" Having a deletion number greater than 1 is essential for making this tag system work. Because every second character triggers a rule, we can influence the system's behavior by arranging for certain characters to appear (or not appear) in these trigger positions.\" - page 254 \" Cyclic tag systems are extremely limited - they have inflexible rules, only two characters, and the lowest possible deletion number - but surprisingly, it's still possible to use them to simulate any tag system.\" - page 260 Impossible Programs \" The practical purpose of a computing machine is to perform algorithms. An algorithm is a list of instructions describing some process for turning an input value into an output value, as long as those instructions fulfill certain criteria: Finiteness: There are a finite number of instructions. Simplicity: Each instruction is simple enough that it can be performed by a person with a pencil and paper without using any ingenuity. Termination: A person following the instructions will finish within a finite number of steps for any input. Correctness: A person following the instructions will produce the right answer for any input. page 274 \" can any algorithm be turned into instructions suitable for execution by a machine?\" - page 276 \" there's a real difference between the abstract, intuitive idea of an algorithm and the concrete, logical implementation of that algorithm within a computational system. Could there ever be an algorithm so large, complex, and unusual that its essence can't be captured by an unthinking mechanical process?\" - page 276 \" the question is philosophical rather than scientific\" - page 276 \" The idea that any algorithm can be performed by a machine - specifically a deterministic Turing machine - is called the Church–Turing thesis , and although it's just a conjecture rather than a proven fact, it has enough evidence in its favor to be generally accepted as true. \" - page 277 Code Is Data \" programs can be represented as data so that they can be used as input to other programs; it's the unification of code and data that makes software possible in the first place.\" - page 279 Universal Systems Can Loop Forever \" any system that's powerful enough to be universal will inevitably allow us to construct computations that loop forever without halting.\" - page 281 \" So why must every universal system bring nontermination along for the ride?\" - page 283 \" it's impossible to remove features (e.g., while loops) from a programming language in a way that prevents us from writing nonhalting programs while keeping the language powerful enough to be universal.\" - page 287 \" Languages that have been carefully designed to ensure that their programs must always halt are called total programming languages , as opposed to the more conventional partial programming languages whose programs sometimes halt with an answer and sometimes don't. Total programming languages are still very powerful and capable of expressing many useful computations, but one thing they can't do is interpret themselves.\" - page 287 \" a fundamental mathematical result called Kleene's second recursion theorem , which guarantees that any program can be converted into an equivalent one that is able to calculate its own source code.\" - page 288 Decidability \" A decision problem is any question with a yes or no answer\" \" A decision problem is decidable (or computable ) if there's an algorithm that's guaranteed to solve it in a finite amount of time for any possible input. The Church-Turing thesis claims that every algorithm can be performed by a Turing machine, so for a problem to be decidable, we have to be able to design a Turing machine that always produces the correct answer and always halts if we let it run for long enough.\" - page 293 \" There are many decision problems - infinitely many - and it turns out that a lot of them are undecidable: there is no guaranteed-to-halt algorithm for solving them. Each of these problems is undecidable not because we just haven't found the right algorithm for it yet, but because the problem itself is fundamentally impossible to solve for some inputs, and we can even prove that no suitable algorithm will ever be found.\" - page 294 The Halting Problem \" the halting problem , is the task of deciding whether the execution of a particular Turing machine with a particular initial tape will ever halt.\" - page 295 \" This is Rice's theorem : any nontrivial property of program behavior is undecidable, because the halting problem can always be reduced to the problem of deciding whether that property is true; if we could invent an algorithm for deciding that property, we'd be able to use it to build another algorithm that decides the halting problem, and that's impossible.\" - page 304 \" Any system with enough power to be self-referential can't correctly answer every question about itself.\" - page 308 \" every pushdown automaton has an equivalent context-free grammar and vice versa; any CFG can be rewritten in Chomsky normal form; and any CFG in that form must take exactly 2n − 1 steps to generate a string of length n.\" - page 312 Programming in Toyland Abstract Interpretation \" The main idea of abstract interpretation is to use an abstraction , a model of the real problem that discards enough detail to make it manageable - perhaps by making it smaller, simpler, or by eliminating unknowns - but that also retains enough detail to make its solution relevant to the original problem.\" - page 315 \" A lot of the time, it's fine for a result to be imprecise, but for an abstraction to be useful, it's important that this imprecision is safe . Safety means that the abstraction always tells the truth: the result of an abstract computation must agree with the result of its concrete counterpart. If not, the abstraction is giving us unreliable information and is probably worse than useless.\" - page 321 Static Semantics \" […] dynamic semantics of programming languages, a way of specifying the meaning of code when it's executed; a language's static semantics tells us about properties of programs that we can investigate without executing them. The classic example of static semantics is a type system \" - page 327 \" From the perspective of someone designing the static semantics, it's also more difficult to handle a language where variables can change their types.\" - page 334 \" Fundamentally, there is a tension between the restrictiveness of a type system and the expressiveness of the programs we can write within it.\" - page 334 \" A good type system finds an acceptable compromise between restrictiveness and expressiveness, ruling out enough problems to be worthwhile without getting in the way, while being simple enough for programmers to understand.\" - page 334 \" Any information we get from the type system has to be taken with a pinch of salt, and we have to pay attention to its limitations when deciding how much faith to put in it. A successful execution of a program's static semantics doesn't mean \"this program will definitely work,\" only \"this program definitely won't fail in a particular way\" It would be great to have an automated system that can tell us that a program is free of any conceivable kind of bug or error, but as we saw […], the universe just isn't that convenient.\" - page 338 \" Formally, abstract interpretation is a mathematical technique where different semantics for the same language are connected together by functions that convert collections of concrete values into abstract ones and vice versa, allowing the results and properties of abstract programs to be understood in terms of concrete ones.\" - page 338 \" Java has a type and effect system that tracks not only the types of methods' arguments and return values but also which checked exceptions can be thrown by the body of the method (throwing an exception is an effect ), which is used to ensure that all possible exceptions are either handled or explicitly propagated.\" - page 339 Afterword \" Every computer program is a mathematical object. Syntactically a program is just a large number; semantically it can represent a mathematical function, or a hierarchical structure which can be manipulated by formal reduction rules. This means that many techniques and results from mathematics, like Kleene's recursion theorem or Gödel's incompleteness theorem, can equally be applied to programs.\" - page 341 \" Computation, which we initially described as just \"what a computer does\", has turned out to be something of a force of nature. It's tempting to think of computation as a sophisticated human invention that can only be performed by specially-designed systems with many complicated parts, but it also shows up in systems that don't seem complex enough to support it. So computation isn't a sterile, artificial process that only happens inside a microprocessor, but rather a pervasive phenomenon that crops up in many different places and in many different ways.\" - page 341 \" Computation is not all-or-nothing. Different machines have different amounts of computational power, giving us a continuum of usefulness: DFAs and NFAs have limited capabilities, DPDAs are more powerful, NPDAs more powerful still, and Turing machines are the most powerful we know of.\" - page 341","tags":"Programming","url":"understanding_computation.html","loc":"understanding_computation.html"},{"title":"Logik","text":"Notizen zur Vorlesung: Einführung in die Logik Übersicht Aussagelogik Prädikatenlogik Zeichensystem der Aussagelogik Wahrheitsfunkt. Verknüpfungen Aussagelogische Form Interpretation von Aussagelogischen Formeln Zeichensystem der Prädiketenlogik Klassifikation von Aussagen Prädikatenlogische Form Interpretation von Prädikatenlogischen Formeln Metalogik der Aussagelogik Objektebene vs. Metaebene Logische Wahrheit Falschheit Folgerung Äquivalenz Widerspruch Konsistenz Abhängigkeit Metalogik der Prädikatenlogik Parallele zur Metalogik der Aussagelogik Logische Wahrheit Falschheit Folgerung Äquivalenz Weitere metalogische Begriffe Mathematischer Zugang zur Aussagelogik Formale Logik Logische Folgerung Beispiel: Prämissen (Voraussetzung): Alle Logiker sind Menschen. Alle Menschen sind schlafbedürftig. Konklusion (Schlusssatz): Alle Logiker sind schlafbedürftig. Das Gesamte ist ein *Schluss* . 2 Fragen Korrektheit des Schlusses Ist die Konklusion wahr? Merkmale Wenn die Prämissen wahr sind, dann ist auch die Konklusion wahr (‘Wahrheitstransfer') Ob die Prämissen wahr sind, spielt für die Beurteilung der Korrektheit des Schlusses keine Rolle. Aus einem korrektem Schluss lassen sich viele weitere korrekte Schlüsse (mechanisch) erzeugen. Für die Korrektheit eines Schlusses sind die Bedeutungen der in ihm vorkommenden Begriffe unwesentlich. Die Gültigkeit von korrekten Schlüssen hängt von Wörtern wie alle und einige ab Prämissen können auch (manifest) falsch sein, der Schluss bleibt trotzdem korrekt! Beispiel: Prämissen (Voraussetzung): Alle Logiker sind Menschen. Alle Menschen haben Eigenschaft S . Konklusion (Schlusssatz): Alle Logiker haben Eigenschaft S . Setze Eigenschaft S = Reptil sein Allgemeine Formulierung Prämissen (Voraussetzung): Alle A sind B . Alle B sind C . Konklusion (Schlusssatz): Alle A sind C . Der Schluss ist korrekt auch wenn man nicht weiss, was A , B und C bedeuten. Logische Form Form: Elemente isolieren, von denen die Korrektheit eines Schlusses abhängt Inhalt: Elemente die übrig bleiben Verschiedene Logiken (z.B. Aussagelogik, Prädikatenlogik) können anhand ihrer unterschidlichen logischen Form differenziert werden.","tags":"Mathematics","url":"logik.html","loc":"logik.html"},{"title":"Scheme (Lisp)","text":"This page collects notes about Scheme (an Lisp in general). Sources Most information is taken from: Structure and Interpretation of Computer Programs See also my notes on Structure and Interpretation of Computer Programs My Github repository with examples Evaluation Rules If self-evaluating : return value If name : return value of associated name in environment If special form : do something special If combination : evaluate all subexpressions (in any order) apply operator on arguments and return result Application Rules If primitive procedure , just do it If compound procedure , then evaluate body of procedure with each formal parameter replaced with corresponding actual argument value. Linear Recursion and Iteration See SICP section 1.2.1 and stack overflow It's confusing that both the recursive and the iterative implementation call themselves. There is a distinction between Recursive process and Recursive function A recursive function calls itself. But it can be implemented as recursive or iterative process. Recursive Process Iterative Process Function calls itself Function calls itself Itermediate result is kept on caller side Intermedia result is passed to the called function additinal argument needed initial value needed Each recursive call needs a new stack frame Stack frame can be reused (tail call) Recursive function call part of bigger expression Recursive function call not part of bigger expression Easier to understand More difficult to understand and to implement Needs stack Can be implemented in register machine (without stack) Recursive Process ( define ( factorial n ) ( if ( = n 1 ) 1 ( * n ( factorial ( - n 1 ))))) ;; 'factorial' is part of bigger expression Iterative Process ( define ( factorial n ) ( fact-iter 1 1 n )) ;; inital values need to be provided ( define ( fact-iter product counter max-count ) ;; max-cout: intermediate result ( if ( > counter max-count ) product ( fact-iter ( * counter product ) ( + counter 1 ) max-count ))) ;; max-cout: supply intermediate result to next call Iterative algorithms have constant space Develop iterative algorithm: figure out a way to accumulate partial answers write out table to analyze precisely initialization of first row update rules for other rows how to know when to stop Iterative algorithms have no pending operations when the procedure calls itself Expressions In Scheme everything is an expression Expressions can be nested arbritarly Sequences as Conventional Interfaces \" The key to organizing programs so as to more clearly reflect the signal-flow structure is to concentrate on the \"signals\" that flow from one stage in the process to the next. If we represent these signals as lists, then we can use list operations to implement the processing at each of the stages. \" \" The value of expressing programs as sequence operations is that this helps us make program designs that are modular, that is, designs that are constructed by combining relatively independent pieces. We can encourage modular design by providing a library of standard components together with a conventional interface for connecting the components in flexible ways.\" SICP section 2.2.3 Sequences as Conventional Interfaces Lazy Evaluation Normal-order (lazy) evaluation doesn't work well in some cases: Tail recursion (Iterative Process): the stack frame can't be reused because computation of a promise is not executed until it's needed. The delayed promises let the stack grow until their computation is forced. Side effects: Setting variables to values calculated by promises is difficult because it's not clear when the promise is forced to calculate the value. The time decoupling mechanism of promises (streams) doesn't work well with statefull models where time is of essence. Quotation \" Allowing quotation in a language wreaks havoc with the ability to reason about the language in simple terms, because it destroys the notion that equals can be substituted for equals. For example, three is one plus two, but the word \"three\" is not the phrase \"one plus two\". Quotation is powerful because it gives us a way to build expressions that manipulate other expressions\" SICP section 2.3.1 Quotation Backquote Preceding a list with a backquote symbol (`) is much like quoting it, except that anything in the list that is flagged with a comma is evaluated. SICP section 5.5.2 Compiling Expressions","tags":"Programming","url":"scheme_(lisp).html","loc":"scheme_(lisp).html"},{"title":"Evaluation strategy","text":"See also: Wikipedia: Evaluation strategy Applicative order evaluation (eager evaluation) All arguments to procedures are evaluated when provided to procedure (when procedure is applied) for example Scheme uses this evaluation strategy Normal order evaluation (lazy evaluation) Delay the evaluation of procedure arguments until they are needed","tags":"Programming","url":"evaluation_strategy.html","loc":"evaluation_strategy.html"},{"title":"Statements and Expressions","text":"\" The purpose of an expression is to be evaluated to produce another expression; a statement, on the other hand, is evaluated to make some change to the state of the abstract machine.\" [Tom Stuart, Understanding Computation] Statements often contain expressions but not the other way around. $$\\underbrace{x = \\overbrace{1 + 2}&#94;{\\text{expression}}}_{\\text{statement}}$$ Expressions (Ausdruck) is evaluated according to semantics in a context ‘ returns' a value can have side-effects (in most languages) some languages have expressions as only construct (functional, declarative) some languages have declarations for defining context of expression (among other things) can be nested with each other Statement (Anweisung) mostly just in imperative languages is executed , action to be carried out in most languages no result/value (no eval) only side-effects (change of state) in assembly languages statements are often called instruction/command most languages have fixed set of statements (can not be changed/extended) statements are often language keywords can't be redefined some statements can be expressions assignment increment function calls … Examples: Assignments Control statements jumps loops conditionals (if, else, switch, …) … procedure calls assertion declarations class/type definitions statements often begin with a identifier/keyword the syntax of statements can be described with BNF <https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_Form> _ Syntax diagram <https://en.wikipedia.org/wiki/Syntax_diagram> _ statements differ from subroutine calls by handling of parameters subroutine parameters are evaluated once statement parameters can be evaluated multiple times (e.g. condition in while loop) call-by-name (lazy evaluation) can't be nested with each others or with expressions Declaration set properties identifier (word/name) dimension scope … of variables constants functions classes enums type-defs …","tags":"Programming","url":"statements_and_expressions.html","loc":"statements_and_expressions.html"},{"title":"Requirements Engineering","text":"This article is work in progress. Conceptual Design => Technical Design => Implementation Actors, Personas Mock-ups (for non-engineer) Paper Mock-ups Tools for Mock-ups (functional) Rapid Prototype Architecture Geographic Architecture: Local, Client-Server, Cloud… Technological Architecture: SQL , JSON , XML , Programming Language Logical (Business) Architecture: Semantics, Implementation that satisfies the Requirements Functional and Non-Functional Requirements Functional Requirements: What the Software needs to do Non-Functional Requirements: Quality criteria: Security, extensibility, scalibility… Logical Architecture Read the Specification: Nouns => Actors/Entities, Verbs => Actions/Functionality/Behavior Entities + Behavior => Data Model Methods: input -> output (table form) Entities: data -> creator Components (Bottom-Top): Create Data Model Build Application API driven development Conceptual data model: first high-level, go in detail later describe reality => Model OOP SQL … Impedance mismatch Relationships (pointers/references in most languages) mostly more complex and with semantics Know the Problem! Don't start before you really know the problem! be convinced about solution Describe application domain (reality) Modeling Language (programming languages…) needed to capture details of reality e.g Java (and other OOP languages) can not properly model relationships Impedance mismatch if two connected models are not the same OOP <-> reality OOP <-> relational-dbs ( SQL ) Relational DB : Table: Relation (similar to class) Columns: Relation attributes (similar to class attributes) Keys: Pointers (relationships) Start with simple things => improve agile List available operations with input and output (e.g use Fitnesse…) Example: Operation in out create message, location - get location set of messages Sentences (e.g. use cases): Nouns: Objects (classes) Verbs: Methods Software objects requirements specifications, designs, documentations, program code, test plans, test cases, user manuals, project plans, … object identification object granularity (module:declarations and statements, documentation:chapters and sections) object representation (files, graph, …) Relationships composition relationships: tree with root representing product dependency relationships","tags":"Programming","url":"requirements_engineering.html","loc":"requirements_engineering.html"},{"title":"Files and File descriptors on Linux","text":"Some information on this page is taken from here (German). Process Control Block ( PCB ) Contains the information about a process. C struct task_struct Pointer to code section (text) Pointer to data section State: running, ready, blocked, terminated Instruction pointer ( IP ): next instruction to execute Saved registers Memory information: pages Scheduler information Priority Pointer to scheduler queue Table with open file descriptors hold by this process File Table 1 entry for each open file different processes can point to the same entry Entry: number of i-nodes position pointer mode (read, write, append…) pointer to v-node table v-node Table i-node information copied to v-node table when file is opened owner rights user/group ids number of links file information file size functions (file system dependent) File Descriptors File descriptors are unique ID 's (ints) that identify an open file. 3 descriptors are reserved by the system in <unistd.h> : Constant Number Meaning STDIN_FILENO 0 Standard input STDOUT_FILENO 1 Standard output STDERR_FILENO 2 Standard error The number can differ but the given ones are common. Functions that return a file descriptor (e.g. open(...) or creat(...) ) return -1 if en error occurred. File Types There are 6 different file types in Linux: Regular files Directories Device files Sockets Pipes (named Pipes and FIFOs) Links (Soft- and Hardlinks) Special Device Files See here for a description for device files.","tags":"Programming","url":"files_and_file_descriptors_on_linux.html","loc":"files_and_file_descriptors_on_linux.html"},{"title":"Object Databases","text":"Motivation Information Systems Design Conceptual Design -> Technical Design -> Implementation Orthogonal Persistence Data has to outlive the execution of the program Independence of longelivety of data How much code needs to be written for persistence (effort for programmer) None at all would be nice but not realistic ( GUI objects usually don't need storing…) Data is stored automatically Not always desirable: don't need to store UI , messages … 3 possible solutions implicit: save all objects automatically explicit: objects need to be saved manually on class base (different granularity) Data Type Orthogonality all data objects should be allowed to be persistent (long-lived, transient) The data types do not need to be modified (implicitly or explicitly) to allow persistence e.g Serializable interface in Java does not satisfy this criteria (classes that don't implement the interface can't be stored) Identification : mechanism for identifying persistent object not related to the type system Transferring data from persistence storage to memory e.g specify which classes/objects should be stored persistence separation of concerns No commercial DB system fully satisfies the above criteria Impedance Missmatch Difference between models OOP data model is based on object trees The Relational Model is based on tables OOP Languages Difference between attributes and relationships not clear Both are (usually) reference types in a class Different models because there are different requirements Mapping code between the two models needed A lot of solutions available Affects run-time (and often development) Inheritance ( OOP ) is difficult to model in relational DBs Objects spread over several tables (other approaches possible) An overview of the mismatch is shown on Tutorialspoint Mismatch Granularity An object model can have more classes than the number of corresponding tables in the DB . Inheritance In OOP inheritance is a base concept. The Relational Model doesn't support this directly. Identity The Relational Model defines identity. In OOP there is often a difference between equality and identity. Association OOP uses references. The Relational Model uses foreign keys. Navigation OOP uses following references in object tree. The Relational Model uses queries. Relational Databases Tables, Rows, Columns… Table: Relation (similar to class) Columns: Relation attributes (correspond to class attributes) Keys: Pointers (relationships) It's difficult to match the relational database model to the object oriented model Object-relational mapping ( ORM ) Object-relational mapping can be used against object-relational miss-match But it's not a very good approach There are a lot of solutions: Hibernate, QxOrm , … Develop own Database Support application development (programming language) Clients of the DB system are developers (not end user) Support persistent data management One data model ( OOP ) One language (Java?) SQL Data Definition Language ( DDL ) Definition of data models Concepts Relations Relation attributes and domains Primary and foreign key Operations Create tables ( class in Java) Declare relation attributes Declare primary and foreign keys Alter tables Data Manipulation Language ( DML ) Creation and management of data Concepts Relation touple Predicate Operations Insert Update Delete in Java: new and ‘setters' Query Language ( QL ) Retrieval of data Concepts Query Predicate Query result Operations (Relational Algebra) Project Join Select in Java: ‘getters' DDL , DML and QL in SQL and Java DD , DM supported in Java. Querying is very limited. DD SQL CREATE TABLE name ... Java class Person { String name ; String email ; } DM SQL UPDATE name ... Java new Person () p . name = \"Bill Stinnet\" Querying SQL SELECT col_name ... Java ... = p . name LINQ is a powerful and compile time safe support for querying. Learning Goals Motivation for Database technologies (not relational) Design- and run-time Complex application domains Schema and data evolution Understanding of object-oriented data model what is OOP ? ODMG New and alternative concepts and applications Develop an object database Requirements Challenges integration with application duplicate objects OO -specific indexing … Object database technologies Products db4o Object Store Objectivity Versant General concepts Solutions to the challenges Comparison to other database technologies Android SDK Hibernate Django … General Topics for DB 's [Prepared Statement] CRUD and ACID The acronym CRUD stands for the basic functions of a database system Create Read Update Delete The acronym ACID stands for the properties that allow for concurrent database transactions Atomicity Consistency Isolation Durability ODMG Standard What is an object? Something that can be pointed to? Can be passed on as a reference? Distinct from single values properties and behaviour representing something (moddeling the real world) Object: properties behaviour presenting something (modelling the real world) Examples in Java: Simple Complex Reference Integer Object Value int N/A In some languages ( PHP , JavaScript, …) Objects and Associative Arrays are the same (or similar). Some constructs can be extended over time possible with map (add key/value) not possible with class in Java Object Orientation needs a broader definition! not coupled to a specific programming language Most database systems often independent of programming languages e.g SQL very different from the language where the DB is used (independent of language) Object database systems are more coupled to one (or multiple) programming language Object Data Management Group ( ODMG ) Object Model Based on OMG object model Basic modeling primitives object : unique identifier (identity) literal : no identifier Object state defined by the values (properties) attributes relationships Object behavior set of operations (can be executed) Objects and literals are categorized by their type A type defines common properties and common behavior Types Specification properties attributes relationships operations exceptions Implementation language binding a specification can have more than one implementation Similar to C++ Header-File: Specification Source-File: Implementation Type Specifications Interface defines only abstract behavior interface Employee {...}; Class defines both abstract behavior and abstract state class Person {...}; Literal defines abstract state struct Complex { float real; float imaginary; }; Type Implementation Representation data structure derived from type's abstract state by the language binding for each property contained in the abstract state there is an instance variable of an appropriate type defined Methods procedure bodies derived from type's abstract behavior by the language binding also private methods with no counterpart in specification Subtyping and Inheritance Two types of inheritance relationships IS -A relationship inheritance of behavior multiple inheritance, name overloading disallowed interface Professor : Employee {...}; EXTENDS relationship inheritance of state and behavior single inheritance class EmplPers extends Person : Employee {...}; Co- and Contravariant rules must hold Input Arguments: Contravariant Return Values (and Exceptions): Covariant Object Query Language ( OQL ) Based on ODMG Object Model and SQL -92 select list_of_values from list_of_collections_and_typical_members where condition SELECT : Get attributes FROM : The base population of potential results to search in WHERE : Predicates (conditions) that have to be true so that the results are selected as result Mapping to OOP not obvious. Table in relational DBs has tow roles: Defining Type Container of all values (of that type) In OOP 2. is missing. A class just defines a type but is not a container of all instances of that class. ODMG defines Extents. Difference to SQL This is illegal as the \"dot\" operator cannot be applied to a collection of objects: select a.authors.title from Authors a where a.name = \"Tilmann Zaeschke\" Correct solution based on correlated variables: select p.title from Authors a, a.authors p where a.name = \"Tilmann Zaeschke\" Return Types Queries return sets, bags or lists As a default, queries return a bag select first: p.authored_by[1], p.title, p.year from Publications p In Java: Bag < Struct { Author first , string title , integer year } > Queries with DISTINCT return a set select distinct a.name from Authors a In Java: Set < Struct { string name } > Queries with ORDER BY return a list select p.title from Publications s order by p.year desc In Java: List < Struct { string name } > Extents Extent of a type is the set (collection) of all active instances class Person : extent of class Person would be the set of all person objects in the data management system Extents can be maintained automatically Collections OMDG supports collections: set: un-ordered, no duplicates bag: un-ordered, duplicates list: ordered, elements can be inserted array: ordered, elements can be replaced dictionary: maps keys to values Single elements Duplicates Ordered ? (heap) list Un-ordered set bag Support for objects and literals: Collection objects: Set<t> , Bag<t> , List<t> , Array<t> , Dictionary<t,v> Collection literals: set<t> , bag<t> , list<t> , array<t> , dictionary<t,v> Sub-collections OMDG supports sub-collections Relationships All relationships are binary many-to-many many-to-one one-to-one No support for ternary (or higher) relationships can be simulated by creating classes to represent relationship touple System maintains referential integrity! Keywords for attributes and relationships attribute : for normal class attributes relationship : for relationship references (integrity checks can be performed) Persistence Persistence by reachability not necessary the best option Database gives access to global names explicitly named root objects types defined in schema named extents of types store object graphs Other Concepts Supported Database operations Locking and concurrency control Transactions Access to meta-data Built-in structured literals and objects dates times time-stamps intervals … Object Definition Language ( ODL ) programming language independent, extensible and practical compatible to OMG Interface Definition Language ( IDL ) ODL Syntax: class name [ ( extent name , key name ) ] { { exception name { { type name } } } { attribute type name } { relationship type name inverse relationship } { type name ({ ( in | out | inout ) type name }) [ raises ({ exception }) ] } } extent and key of a class can be specified optionally relationships specify inverse to maintain referential integrity method signatures are implemented by language binding Collection Expressions Aggregate operators AVG , SUM , MIN , MAX , and COUNT apply to collections that have a compatible member type Operations for sets and bags UNION , INTERSECTION and EXCEPT inclusion tests (subset, super-set) Special operations for lists Simple coercion a collection of one element can be coerced to that element using the ELEMENT operator Flattening a collection of collections Language Bindings C++ Java Smalltalk … Hibernate Example of a Object-relational mapping tool Challenges Data has to outlive the execution of the program The program is written in an OOP language Problems of Object Identity Two object graphs can point to same objects Duplicates can be created When deserialized Programmer must take care to avoid multiplicity! Object Identity Multiple models Impedance mismatch Transformation must be implemented (Design Time) Transformation must be carried out (Run Time) Must be implemented for each application Possible use of SQL Integrating a relational database at the level of SQL (e.g. JDBC , Google Android, PHP mysqli) OK for simple application domains (e.g. few classes with base type attributes) Not OK for complex application domains (e.g. many classes with reference type attributes, multi-valued attributes, associations, inheritance) Object-Relational Mappings Map object-oriented domain model to relational database Hibernate Free implementation for Java maps Java types to SQL types transparent persistence for classes meeting certain requirements generates SQL for more than 25 dialects behind the scenes provides data query and retrieval using either HQL or SQL can be used stand-alone with Java SE or in Java EE applications Java Persistence API ( JPA ) Defines interface (and annotations) to use y other systems Enterprise Java Beans Standard 3.0 introduced annotations to define mapping javax.persistence package Main Concepts Configuration Connection to database (hibernate.properties or hibernate.cfg.xml) Which DBMS ? (MySQL, HSQL , …) Which DB Instance? (Database Name) Mappings between classes and database tables The Configuration object is created at first usually created only once during application initialization SessionFactory Created by using the configuration object Session objects can be created and supplied with configuration Heavyweight object: create during start up and keep for later use multiple databases require multiple SessionFactory objects Session object Used to get physical connection to the database lightweight designed to be instantiated each time an interaction with the DB is needed Persistent objects are stored and retrieved through Session object should not be kept open for a long time usually not thread safe create before use destroy after use Transaction object represent a unit of work with the DB handled by underlying transaction manager and transaction ( JDBC or JTA ) optional object transactions can be managed in own application code Query object SQL or Hibernate Query Language ( HQL ) bind query parameters limit number of results execute query Criteria object used to create and execute object oriented criteria queries to retrieve objects Configuration Hibernate needs to know where to find some information about class mappings and database XML file: hibernate.cfg.xml (or Java properties file hibernate.properties ) Properties Property Description hibernate.dialect The SQL dialect used to generate persistence code hibernate.connection.driver_class The JDBC driver class hibernate.connection.url The URL to the database instance hibernate.connection.username The username for the database hibernate.connection.password The password for the database hibernate.connection.pool_size The number of connections in the Hibernate database connection pool hibernate.connection.autocommit Autocommit mode for the JDBC connection There are additional settings for using a database along with an application server and JNDI . Persistent Objects Only Plain Old Java Object ( POJO ) can be saved in the DB . Requirements: Default constructor Class should contain an ID Maps to the primary key column in DB table Attributes should be private Public accessor functions (getter, setter) need to be provided JavaBean style accessors: getXYZ, setXYZ Persistence class either non-final or implementation of interface that declares all public methods Some additional minor restrictions from EJB framework Example: SessionFactory factory = new Configuration (). configure (). buildSessionFactory (); Session Connection to a database instance Created when we want to work with a database Using a Session object, we can store and retrieve application data Methods: save createQuery update delete Example: Session session = factory . openSession (); ... session . save ( person ); ... session . close (); Transaction Opened with a Session object beginTransaction() and closed using commit() Example: Transaction tx = session . beginTransaction (); ... tx . commit (); Query Represents a query (e.g. SELECT * FROM PERSONS , HQL ). Result is a list Example: List persons = session . createQuery ( \" FROM Persons \" ). list (); for ( Object p : persons ) { ... } Criteria An object-oriented representation of query criteria Mapping Associations Uni- and bidirectional associations Unordered and ordered associations Association cardanality types one-to-one many-to-one one-to-many many-to-many Join Tables to map complex associations Example: CREATE TABLE AUTHOR(AUTHOR_ID BIGINT NOT NULL PRIMARY KEY, ...) CREATE TABLE AUTHORSPUBLICATIONS( AUTHOR_ID BIGINT NOT NULL, PUBLICATION_ID BIGINT NOT NULL, PRIMARY KEY(AUTHOR_ID, PUBLICATION_ID)) CREATE TABLE PUBLICATION(PUBLICATION_ID BIGINT NOT NULL PRIMARY KEY, ... ) Mapping Inheritance Multiple strategies One table per class hierarchy One table per subclass One table per concrete class Mapping strategies can be mixed Different parts of inheritance hierarchy can have different strategy Implicit polymorphism One table per concrete class Common interface not mentioned in the mapping Common properties are mapped in every table Annotations Since Java 5 Java Persistence API ( JPA , Enterprise Java Beans 3.0) Annotations instead of XML for mapping Standardizes ORM Hibernate implements JPA Google Android Linux based OS Default applications: Search, Maps, Mail, Calendar, Contacts… Java SDK Support for persistent data management Smart Phone / Tablet Application component model Activity ( UI ) Service ( API , Computation) Broadcast Receiver (Events) Content Provider (Data Management) Manifest lists the application components sets activity to be shown at startup components can be made available to other applications Intent: request the use of application components (integration with other apps) showing activities using content provides listening to events starting services … Application Life-cycle States: New Activity, Running, Paused, Stopped, Destroyed Extend Activity Application can react to state changes Data Management Low-level: SQL High-level: Content Provider Database Instance SQLite CRUD C reate (Insert) R ead (Retrieve, Query) U pdate D elete Cursor Iterator over result Methods isLast() and moveToNext() Type specific getter methods (e.g getInt(3) ) Access with index Index can be retrieved getColumnIndex(String colName) Content Provider Abstract class implemented by application Uses Cursor URI ContentValue Object-Oriented Databases: Object Database Manifesto Avoid impedance mismatch Provide uniform data model Combine features of OOP Database Management Systems The object-oriented database manifesto 13 mandatory features 5 optional characteristics 4 open choices several important topics not addressed Object-oriented systems (mandatory) 1. Complex objects 2. Object identity 3. Encapsulation 4. Types and classes 5. Type and class hierarchies 6. Overriding, overloading and late binding 7. Computational completeness 8. Extensibility Database management systems (mandatory) 9. Persistence 10. Efficiency 11. Concurrency 12. Reliability 13. Declarative query language Objects (mandatory) Complex objects build from simpler objects (constructor) collections Object identity and equality Object ID ( OID ) unique and immutable sharing of objects (references) identical objects have same OID equal objects have same state shallow and deep equality Touple: entities and their attributes Sets: collections of entities Lists: order Constructor orthogonality supports arbitrary nesting such as sets of sets set of records records containing records … In contrast, relational databases only support sets of records (relation with touple) and touple containing atomic values Complex objects also require transitive retrieval and deletion of objects deep and shallow copying … Encapsulation interface signatures of public methods implementation includes data and methods object state modification only through public methods object data structure may be exposed for declarative queries ! Types and Classes Data types definition of object properties static part: object structure dynamic part: object behavior separation of interface and implementation Check of correctness at compile time Object classes container for objects of the same type objects can be added and removed used to add, remove and iterate over all existing objects to present, manipulate … them Generalization Hierarchies Powerful modeling tool Reuse (specification and implementation) Inheritance Substitution principle Inherited attributes and methods (from super-class) New attributes and methods can be added (in subclass) Migration between classes move objects between hierarchy levels object specialization and generalization Durability and Efficiency Persistence data has to survive the program execution orthogonal persistence implicit persistence Secondary storage management index management data clustering data buffering access path selection query optimization Orthogonality Persistence applicable to all objects of any type No additional code necessary Moving objects between persistent and memory representation is not needed Similar requirements for secondary storage management As a rule of thumb: a database should work without it, but with it, it should perform better If this requirement is met, a complete separation of the logical and physical level is achieved Concurrency Control and Recovery Concurrency management of multiple users concurrently interacting atomicity, consistency, isolation and durability serialisability of operations Reliability resiliency to user, software and hardware failures transactions can be committed or aborted restore previous coherent state of data redoing and undoing of transactions logging of operations Declarative Query Language High-level language express non-trivial queries concisely text-based or graphical interface declarative Efficient execution possibility for query optimization Application independent work on any possible database no need for additional methods on user-defined types db4o Open Source project Java and . NET Features No conversion of mapping needed Classes don't need to be changed to make them persistent (datatype orthogonality) One line of code to store (complex) objects local or client/server mode ACID transaction model Automatic management and versioning of database schema Small memory foot-print (single 2Mb library) Object Container Represents db4o databases local file mode or client connections to db4o server Owns one transaction operations are executed transactions transaction is started when object container is opened after commit/rollback next transaction is started automatically Manages link between stored and instantiated objects object identities loading, updating, unloading Storing Objects Store objects with method ObjectContainer.store(...) Arbitrary complex objects can be stored Persistence by reachability Retrieving Objects 3 Query languages Query by Example simple based on prototype objects selects exact matches only Native Queries expressed in application programming language type safe transformed to SODA (optimized) Simple Object Data Access ( SODA ) query API based on query graph methods for descending graph and applying constraints SODA Queries Expressed using Query objects descend adds or traverses a node in the query tree constrain adds a constraint to a node in the query tree sortBy sorts the result set orderAscending and orderDescending execute executes the query Interface Constraint greater and smaller comparison modes identity , equal and like evaluation modes and , or and not operators startsWith and endsWith string comparisons contains to test collection membership Updating Objects Update procedure for persistent object retrieve desired object from the database perform the required changes/modifications store object back to the database (calling store method) db4o uses IDs to connect in-memory objects with stored objects IDs are cached Deleting Objects similar to updating objects Method ObjectContainer.delete(...) removes objects Simple vs. Complex Objects Simple Structured Objects (objects in classical sense): properties methods Complex Object Structures: multivalued attrivutes arrays collections … Simple Structured Objects New objects are stored using the store method Persistence by reachability Object graph is traversed, each referenced object is stored Updating objects with store method Update depth 1 by default Only primitives and strings are updated No traversal of object graph (performance) Deleting objects with delete method Not cascaded by default Referenced objects have to be deleted manually Cascading delete can be configured for individual classes Updating Simple Structured Objects Cascading updates are configured per class with ObjectClass.cascadeOnUpdate(..) Update depth can be configured ExtObjectContainer.store(object, depth) updates referenced objects to given depth ObjectClass.updateDepth(depth) defines update depth for a class (and all its objects) Configuration.updateDepth(depth) sets global update depth for all objects Deleting Simple Structured Objects Cascading deletes similar to cascading updates configured per object class CommonConfiguration.objectClass (...) ObjectClass.cascadeOnDelete(..) What happens if deleted objects referenced elsewhere??? Cache and disk can become inconsistent when deleting objects ExtObjectContainer.refresh(..) syncs objects Object Hierarchies Complex object structures are handled automatically hierarchies, composite hierarchies inverse associations inheritance and interfaces multi-valued attributes, arrays and collections db4o database-aware collections ArrayList4 and ArrayMap4 implement Collections API as part of transparent persistence/activation framework ActivatableArrayList , ActivatableHashMap , … complex object implementation becomes db4o dependent Transparent Persistence Persistence should be transparent to application logic Store objects in database once ( store method) avoid multiple calls of store Logic of transparent persistence framework Activatable interface (no datatype orthogonality) objects are made persistent by store method objects bound to transparent persistence framework with bind method commit: transparent persistent framework scans for modified objects and invokes store Enabling transparent persistence config . add ( new TransparentPersistenceSupport ()); Activation Activation controls depth of loaded fields field values (objects) are loaded in memory to a certain depth when query retrieves objects activation depth: length of reference chain fields beyond activation depth: default value (e.g null ) Activation cases ObjectSet.next(...) called on an query result explicit ObjectContainer.activate(...) db4o collection element accessed members of Java collections are activated automatically when collection is activated Controlling activation default depth: 5 ObjectContainer.activate(..) and ObjectContainer.deactivate() per class configuration Methods for per class configuration of activation depth ObjectClass # minimumActivationDepth ( minDepth ) ObjectClass # maximumActivationDepth ( maxDepth ) ObjectClass # cascadeOnActivate ( bool ) ObjectClass # objectField (...). cascadeOnActivate ( bool ) Transparent Activation Make activation transparent to application logic activate fields automatically when accessed Logic of transparent activation framework Activable interface (no datatype orthogonality) at object instantiation db registers itself in object with bind(..) Enabling the transparent activation framework config . add ( new TransparentActivationSupport ()); db4o Transactions ACID Data transaction journaling no data loss in case of system failure automatic data recovery after system failure thread-safe for simultaneous interactions all work in ObjectContainer is transactional transaction started when container opened current transaction committed when container closed explicit commit and rollback possible ObjectContainer.commit(...) ObjectContainer.rollback(...) Configuration Embedded Db4oEmbedded.newConfiguration() Client/Server Db4oClientServer.newClientConfiguration() Db4oClientServer.newServerConfiguration() External tools performance tuning database diagnostics Indexes optimize query evaluation Defragment removes unused fields, classes, management information compacts db file, faster access command line interface or from application Statistics queries objects: stored, retrieved, activated, … I/O, Network, … Logger logs objects in db logs objects of a given class run from command line Indexes Trade-off between increased query performance decreased storage, update and delete performance Set by configuration or annotation ( @Indexed ) B-Tree based indexes on single object fields Tuning for Speed Heuristic to improve performance weak references BTree node size free-space manager locking flushing callbacks caches … Object loading use appropriate activation depth use multiple object or session containers disable weak references if not required Database tests disable detection of schema changes disable instantiation tests of persistence classes at start-up Query evaluation set field indexes on most used objects to improve searches optimize native queries Distribution and Replication Embedded mode DB accessed by clients in same JVM direct file access: 1 user and 1 thread at a time client session: 1 user and multiple threads Database file opened, locked and accessed directly Db4oEmbedded.openFile(configuration, name) Client/Server mode Clients in multiple JVMs DB access on server Client opens TCP / IP connection to server Db4oClientServer.openServer(filename, port) Db4oClientServer.openClient(host, port, user, pass) Client sends query, insert, update and delete instructions to server Client receives data from the server Replication redundant copies of database on multiple servers changes replicated form master to client servers several forms of replication supported snapshot replication transactional replication merge replication needs to be coded into application cannot be configured on admin level replication only on demand client/server semantics introduced by developer one interface for all forms of replication Replication Modes Snapshot Replication Snapshots of master replicated to client state-based periodical schedule special SODA query to detect all new and updated objects Transactional Replication Changes synchronized after transaction operation based changes replicated immediately Single object replication with ReplicationSession Merge Replication Changes from clients merged to central server Other clients updated to reflect changes Transactionally of on a periodic basis Typically if subscribers are occasionally offline Replication System Separated from db4o core uni- or bidirectional replication Replication of relational databases based on Hibernate Supported replication providers db4o → db4o db4o → Hibernate Hibernate → db4o Hibernate → Hibernate 3 steps required Generating unique IDs and version numbers Creating a ReplicationSession Replicating objects Mode dependent on implementation Bidirectional by default Unidirectional can be configured ReplicationSession.setDirection(from, to) ReplicationSession.replicate(object) newer object transferred to DB Object granularity Conflict handling has to be done by developer Callbacks Called in response to events activate deactivate new update delete Methods called before and after event can... called before event on... called after event Interface ObjectCallbacks Use Cases Recording or preventing updates canUpdate() and onUpdate() Setting default values after refactoring canNew() Checking object integrity before storing objects canNew() and canUpdate() Restoring state when objects activated Update UI , restore network connections, … Object Instantiation Three different techniques for instantiation Constructor Bypassing Constructor Translator Bypassing Constructor is default (if available) Can be configured globally or per class Constructors db4o tries first default constructor if not public default constructor available all constructors are tested to create instance default values (e. g null ) passed as arguments first successfully tested constructor is used if no instance of a class can be created, object can't be stored Configuration interface: // global setting (default: depends on environment) CommonConfiguration # callConstructors ( true ) // per class setting (default: depends on environment) CommonConfiguration # objectClass (...). callConstructors ( true ) // exceptions for debugging (default: true) CommonConfiguration # exceptionsOnNotStorable ( true ) Bypassing Constructors Constructors that cannot handle default values or null must be bypassed platform specific mechanism Not all environments support this feature Default setting (if supported by environment) Breaks classes that rely on constructors being executed Translators needed if neither Constructors nor Bypassing Constructors can be used if constructor needed to populate transient members if constructor fails when called with default values Interfaces ObjectTranslator and ObjectConstructor Type Handles instead of Translators at lower level type handler registered for class that it handles write to and read from byte-arrays Versant Highly scalable Distributed Many platforms and programming languages C, C++, Java db4o acquired by Versant Lot of features easy to use Full Persistence Orthogonality but code is injected (into byte-code) Indexing Like SQL Point/Range queries Index structures for Relational DBs not ideal for Subtyping Polymorphic indexes in Versant Architecture Always network based no embedded option (as db4o) Client Versant Manager Object cache Application logic Versant Server possible to use multiple servers Access to data Volumes (page cache) Logical log file Physical log file Dual cache Object cache (client) Page cache (server) Database Volumes System Volume (created automatically) Class descriptions Object instances Data Volumes (optional) Increase database capacity Partition data Logical Log Volume (created automatically) Transactions and redo information For recovery and rollback Physical Log Volume (created automatically) Physical data information For recovery and rollback Versant Manager Presents objects to app Caches objects in virtual memory Manages transactions Distributes queries and updates to servers Conversion between Versant DB format and client machine format Cache Cached Object Descriptor Table Map from logical object identifier ( LOID ) to object in memory attributes pointing to other object: access object in memory or DB pointer LOID Meaning NULL NULL object deleted? not NULL NULL not saved persistently NULL not NULL not loaded in cache not NULL not NULL in DB and in cache Other entries: locking information state information … Versant Server Object retrieval (disk and page cache) Transactions and locks Logging and recovery Indexing Versant doesn't have the notion of activation. Data is retrieved automatically when dereferencing pointers. Processes, Sessions and Transactions Session (same as ObjectContainer in db4o) On client side Represents database instance Container A session object is always bound to one single transaction It's possible to have multiple threads attached to one session This is dangerous! Not transaction support Multiple sessions within separate threads or multiple session in one thread Each session object on the client is bound to one transaction thread in the server Transaction ( ACID ) support The server thread accesses the page cache Java Versant Interface Easy-to-use storage of persistent Java objects pure Java (syntax and semantics) instances of nearly all classes can be stored (and accessed) works with Java GC Client-server architecture access to the Versant object DB client libs cache objects for faster access DB queries are executed on server Configuration and code-generation integrated in build process JVI Layers Versant provides different API layers. The fundamental layer is usually not directly needed by the application developer. Fundamental Layer DB centric Create ‘Meta-classes' Objects manipulated indirectly through handles package com.versant.fund Query classes FundQuery and FundQueryResult Create wrapper code for real classes (reflection) Transparent Layer language centric usually used by developer code is injected to application code build on top of fundamental layer package com.versant.trans Java classes are mapped to objects in fundamental layer Persistent objects are cached in memory For persistent objects retrieved from a DB Java objects are constructed (in memory) First and Second Class Objects Versant distinguishes first class and second class objects Configured externally in a file Configured on class basis First Class Objects ( FCO ) main objects that can be saved and retrieved independently have a Logical Object Identifier ( LOID ) changes are saved automatically (independence) strong entities Second Class Objects ( SCO ) can be saved only as part of an FCO can't be results of queries can be part of first class objects stored as binary array weak entity used for optimization Persistence Categories can be configured on class basis First class objects Persistent always ( p ) becomes persistent at instantiation (independence) marked as dirty when modified Persistence capable ( c ) new instances are transient but can become persistent marked as dirty when modified Super-class of p or c must also be p or c respectively Second class objects Transparent dirty owner ( d ) changes to object automatically mark the owner object as dirty works only when the owner is stored ( FCO ) Persistence aware ( a ) for code that is aware of FCO code needs to mark manually the FCO as dirty Not persistent ( n ) no managed by Versant system Persistence and Navigation Persistence by reachability is provided Database root persist the root of an object graph name it (for retrieving it later) Methods makeRoot() , deleteRoot() and findRoot() Transparent navigation ‘ activation' handled automatically and lazy objects are transparently locked and retrieved works across DB boundaries OMDG Layer language centric ODMG database, transactions, collections build on transparent layer packages com.versant.odmg and com.versant.odmg Application Development Develop Java classes make code ‘persistence aware' sessions, transactions, concurrency Create configuration file specify persistence category for each Java class Compile Run enhancer to make byte-code changes persistence behavior inherited from com.versant.trans.Persistent explicit at compile time or implicit at run time (class loader) create DB Run application Byte-code Enhancement Code which creates schema object in DB Code for writing and reading to and from DB Code for accessing attribute values Object Lifecycle Creation of persistent objects Java objects in memory Each object has internal DB information in object cache Commit Object data written to DB Proxy Java objects retained in memory Rollback New database objects will be dropped Querying Queries are passed to DB server Proxy object for every object in result Accessing objects Objects are fetched or de-serialized transparently Versant Collections Standard collection classes can't be byte-code enhanced Special collection classes for FCO s and SCO s Scalable large collections ( FCO ) OMDG collections (are FCO s) additional query facilities Versant Query Language ( VQL ) Subset of OQL ( ODMG ) Similar to SQL Executed on server Can be parameterised prepared statements Event Notification and Persistent Object Hooks Like callbacks in db4o Event Notification Propagation of events from DB to client Event types class: create, modify or delete an instance object: modify or delete object or group of objects transactions: begin or end of transaction user-defined events package com.versant.event Persistent Object Hooks Allow intervention of state transitions activate() , deactivate() preRead(boolean act) and postRead(boolean act) preWrite(boolean deAct) and postWrite(boolean deAct) vDelete() : can be used to maintain referential integrity (cascading delete) ObjectStore General Topics of Persistence Orthogonal persistence is not always desired ( UI , messages, …) Persistence identification: do we need to explicit store/retrieve objects Possibility to select what is in DB and what is in memory Declarative query language what, not how more power-full easy to use more expressive Difference between persistent always: stored automatically persistent capable: needs to be stored explicitly Architecture C++ and Java Client/Server application Lightweight and professional editions available Based on virtual memory mapping pages cache forward architecture (very performant) Virtual memory mapping architecture (extends operating system) logical vs. physical address page fault address translation Characteristics (ObjectStore) virtual shared distributed heterogeneous persistent transactional Logical vs. physical address data is referenced uniquely using a 4-part key database segment cluster offset (in cluster) theoretical address space to \\(2&#94;{128}\\) Reserved virtual memory region for persistence: Persistent Storage Region ( PSR ) Physical memory and secondary storage all data accessed by client must be in PSR cache hold recently accessed data even across transactions Data is cached on different levels Granularity: pages close to persistence capable (in Versant) overloaded new operator instance based persistence Virtual Memory Mapping Architecture Page faults ObjectStore maps data into app when page fault occurs data is paged into memory from cache if not in PSR or fetched from server if not in cache Address translation done when data is fetched into cache re-translation can occur (when PSR gets full) trade-off nice to have the ability to use direct SW pointers translating pointers has scalability implications Server Side Components Server Enforces ACID using ‘page permits' co-operation between servers with two-phase commits automatic recovery mechanism Database managed by one server one server can manage multiple DBs (distribution) binary files storing pages of memory containing C++ objects Transaction log each server owns transaction log pages only propagated to DB when transaction commits used for automatic recovery faster commits Multi-version Concurrency Control ( MVCC ) … Client Side Components Client C++ program linked with ObjectStore libraries even Java programs using ObjectStore are embedded (session) in a C++ part interacts with DB pages automatically fetched from DB Cache one cache memory mapped file per client process pages fetched from DB are held in cache pages can be retained in cache between transactions Cache manager retrieves data from the server meta-information (Commseg) Commseg memory mapped file contains meta-information about pages in cache stores permit and lock for pages in cache permit s can be retained between transactions Cache manager one manager per client machine shared by all clients on that machine handles permit revokes read/write cache and Commseg not directly involved in page fetch Persistent Storage Region ( PSR ) reserved area of virtual address space (in C++ part) addresses of persistent objects used by client are mapped to PSR pointers of application will be in the range of the PSR at end of transaction PSR is cleared can be reused for next transaction Fetching and Mapping Pages Client automatically fetches and maps pages ‘ lazy' fetches held in client cache Server permits and client locks acquired automatically ensuring transaction consistency Existing page can be swapped out if not enough room in cache for new page updated pages are sent to the server read-only pages are dropped from cache (copy in DB ) Fetching and Mapping Sequence ObjectStore installs SIGSEGV (segmentation fault) handler Program obtains pointer p to object on page x Dereferencing p causes SIGSEGV handler to be called Virtual mapping table is consulted Page is fetched from server and stored in cache Page x is mapped to address space Execution continues Cache-Forward Architecture To provide high performance in ObjectStore data cached across transaction boundaries number of used locks is reduces cached data kept in globally consistent state Two types of locks on pages transaction locks : represents state of page during transaction ownership permits : represents state of page in cache Permits are tracked by server server serves permits on pages that are sent to client Locks are taken by client client can lock pages according to given permit Shared Virtual Memory Lazy call-back mechanism for permits Server maintains table of permits for each client When client requests page from server Server checks for other clients with permit for page (and permit type) Server issues call-back if one or more clients have conflicting permits Page Permits and Locks Lock for client (locally) Permission for server (globally) Read permit client can lock page for reading without consulting server many clients can hold a read permit for the same page Write permit client can lock page for reading or writing without asking server only one client can hold a write permit for a page at any given time Cache manager inspects permit and lock status for call-back ✓: Positive ✗: Negative, but permit is flagged to be revoked at transaction end Permit Lock Response read read ✗ server only calls back permit of other client needs to write read no lock ✓ write read ✓ permit for page downgraded to read write write ✗ write no lock ✓ Pages on server are fetched with permission (read or write) Local locks don't tell server about lock/un-lock about after transaction Distribution and Heterogeneity Clients can access objects in different remote DBs in one transaction Clients and servers can run on different platforms Win Linux Unix … Physical object layout is transformed automatically by client at run-time when page is mapped into cache Persistence persistence by instantiation in C++ Overloaded persistent new operator Several options for object allocation transiently on the heap DB segment cluster … Persistence is orthogonal to the type of an object (data-type orthogonality) Transactions Basic ACID properties Atomicity after commit: guaranteed that data was written and is recoverable after abort: changes are undone Consistency it's impossible to apply or lose update while data is written Isolation serialisability ( CPSR ) is guaranteed by 2-phase locking MVCC provides serialisability for read-only transactions (using snapshots) Durability changes are written to transaction log first background process propagates changes to DB Transaction Types Read or Write Local or Global local: only initiating thread is allowed global: allows all threads in a session to share transaction Lexical or Dynamic transactions Lexical transactions automatically retry on lock must start and end in same code block always thread-local Dynamic transactions lower level of os_transaction class better suited for multi-threaded applications Database Layout Memory pages held in hierarchy of clusters in segments Segments logical partitioning of objects Segment 0: schema segment DB schema DB roots Segment 2: default segment Segment 4: first user-created segment max \\(2&#94;{32}\\) segments per DB Clusters group closely related objects each segment has default cluster 0 max \\(2&#94;{31}\\) clusters per segment Developing Applications ( API ) ObjectStore libraries objectstore : run-time os_database : DB management os_transactions : transaction handles and functionality os_typespec : determine type specification os_database_root : manage roots os_segment : segment access and management os_cluster : cluster access and management Development process writing persistent classes, schema file, app logic compile schema file with pssq compiler compile classes with C++ compiler linking Managing Databases provided by os_database create() : creates new DB open() : opens DB save() : saves DB , changes permanent close() : closes DB , but doesn't save stat destroy() : deletes DB Transactions provided by os_transaction all interactions with DB must be in a transactions can be nested defining and working with transactions directly using os_transaction class (dynamic) using macros (lexical) Creating Persistent Objects Use overloaded new operator: :::cpp os_database db = os_database::open(\"publications.db\", 0, 1); Author scheel = new(db, os_ts ::get()) Author(\"Matthias Geel\"); db->close(); It's also possible to instantiate persistent array of objects. Updating and Deleting Persistent Objects Changes to persistent objects are propagated to DB automatically when pages are sent back to server client app updates memory-mapped persistent objects (using standard C++) persistent objects are deleted when memory-mapped objects are deleted (using standard C++) Fully transparent to app developer Collections and Relationships Relationships between classes modeled as collections library of non-templated and templated collection types available traversal, manipulation, retrieval classes os_collection , os_Collection<E> and sub-classes Relationships: // Relationship to the customers orders: os_relationship_m_1 ( Customer , orders , Order , customer , os_List < Order *> ) orders ; Cursors over Collections used to navigate and manipulate collections class os_Cursor cursors can be reused by rebinding to other collection Queries over Collections specify element type query string schema DB query string indicates selection criterion in C++ or pattern matching expression function calls in query strings restricted to basic types nested queries supported Database Roots persistent objects with a well-known name class os_database_root root name as char * pointer to object as void * Objectivity/ DB Key Features Logical and physical pages non-destructive manipulation simple rollback/commit Relationships bidirectional relationship outside of class LINQ Parallel Query Engine Graph Navigator Core implemented in C++ Language support C++ C# Java Smaltalk Python SQL ++ XML Platform support Windows Linux, Altix UNIX : Solaris, HP - UX , IBM RS /6000 OS X 32-bit and 64-bit Architecture Client/Server Lock Servers Data Servers Query Servers (Parallel Query Engine) task splitter multiple query server scanning portions of data in parallel Client-side Objectivity Kernel Federation Federation > Database > Container > Objects A Federation contains several DBs . A DB contains several Containers . A Container contains several Objects . Logical Physical Federation File Database Files Container Pages Objects Slots Federated DB contains one or more DBs A DB file can be copied to other machine Databases Databases are files contain objects in container up to 65530 databases per federation a database holds up to 65530 containers can be moved or copied to any disk/machine can be marked read-only or taken off-line Containers Collections of objects in DB contained objects can be of any size or type can grow up to 65530 pages Useful for logical partitions by owner by attribute by time … Pages Logical and Physical logical page is permament part of object ID ( OID ) physical page is where the page is put in the file Different age sizes possible Unit of transfer from disk or server Unit of locking one writer multiple readers Logical and Physical Pages When page is modified new page is written to container old page is kept journal file is written containing current page map When a transaction commits page map is updated and written to disk old page is freed (for reuse) journal file is truncated lock removed on lock server If a transaction aborts new page is freed (for reuse) lock is removed from server Persistent Object Model Language independent Persistence by inheritance Base types numeric: sbyte, short, int, long, byte, ushort, uint, ulong, float, double string: ASCII (8-bit), UTF8 , UTF16 boolean date/time Complex types embedded: stored as part of parent object referece: parent object stores object identifier enumeration arrays (fixed and variable-length) Relationships Collections Relationships between objects declared in classes unary and binary to-one and to-many Storage and management of relationships inline non-inline binary associations represented as separate construct internally Consistency of relationships referential integrity inverse relationship of binary relationship is updated automatically objects are removed from all relationships when deleted Non-Inline Relationships (default) Each object with relationships has a default relationship array all non-inline relationships are stored in that array Each relationship in the array is identified by relationship name (id, not string) object id ( ODI ) of related object Objectivity traverses relationships until it locates the desired relationship array is like part of an adjacent matrix Inline Relationships To-one inline relationships: embedded as fields of an object To-many inline relationships: placed in their own array Deletion and Lock Propagation Relationshis can have semantics Deletion propagation if object is deleted all associated objects are deleted Lock propagation if object is locked all associated objects are locked Developer specifies operation and direction of propagation Copy and Versioning Behaviour Policies define what happens to object relationships when a copy or a new version of an object is made copy: old an new object associated with same objects drop: only old object is associated with other objects move: only new object is associated with other objects Domain Classes . NET partial classes to separate app and persistence code Persistence by inheritance both partial classes inherit form ReferenceableObject Persistence code class defines schema class and attributes functionality to create and dispose objects properties for attributes defined by schema proxy cache for each relationship helper and utility functionality App code class contains user-defined code Connection, Sessions and Threads Static funtions for startup, shutdown and connection One connection to federation per application process Seesions manage resources (cache, transaction state …) one or many per thread or one shared by many threads Cache remains intact through commits and checkpoints flushed if transaction aborted Persistent Collections Built-in persistent collections: sets, lists and maps automatically persistent (when created) conform to System.Collections.Generic interface Types of persistent collections ordered vs. unordered scalable vs.non-scalabse Iterators Iterators are transient provide access to persistent objects Predicates can be supplied to iterators no (or less) if s in loop Not an efficient method for looking up objects predicates evaluated on client (unless index is available) can be improved with indexes and scoping Result is not built entirely before it is returned clients can start process result (instead of getting blockes) sorting of result is not possible Scope Names Generalization of DB roots Unique name Possible on each storage level container database federation Retrieving Objects -Scope names - Creating links (and following them) - Individual and group lookup - keys - iteratos - Parallel query - Parallel Query Engine - divides query among several servers - Content-based filtering - predicate-query language - used to follow to-many relationships - used in parallel queries LINQ Overview Part of . NEW ( System.Linq ) add native query capabilities to . NET languages Standard query operators defined by class Enumerable Language extensions are translated into method calls Static safety Graph Databases Domain specific solution for a group of problems a kind of solution for a kind of problems Technology models (describe the world): Key-Value Map Relational Model OO Model Mapping between models There is no generic model for all domains Social Network Analysis People are connected to each other Networks may be represented as graphs Nodes: peoples Edges: relationships Social Network Analysis to gather information Graphs Nodes and Edges may have attributes Often key-value pairs Graph algorithms can compute different values Connections vary: Uni- / Bidirectional connections Nodes with single / multiple connections Multiple connections with different attribute Explicit / implicit connections Transient / long lasting connections One-time / repeated connections regular / irregular connections Graph Metrics Path Lengths Dunbar's number Erdős number Bacon's number Small world phenomenon Node Properties Degree Centrality The number of direct connections a node has Betweenness Centrality (Bridge) A node is between two important nodes has great influence what flows in the network Closeness Centrality A node has the shortest path to all others good position to monitor information flow in network Network Structure Properties Network Centralization Dominated by one or few nodes (single) point of failure Density/Cohesion Proposition of direct connections relative to all possible connections Distance (Small World) Minimum numbers of edges needed to connect two particular nodes Clustering Coefficients Likelihood that two associates of a node are also associates to each other ‘ cliquishness' General Model Common representation Uniform graph representation needed to apply algorithms Always 2 core identities connected by action e.g. connect 2 authors that worked on same publication or connect publications that have the same author Graph Database Implementations API Support for CRUD High-level operations for graph traversals Sometimes graph algorithms part of DB ACID Scalable Examples: Objectivity InfiniteGraph Neo4j OrientDB InfiniteGraph (Objectivity) Graph library on top of Objectivity/ DB Distributed graph DB Classes for Vertex and Edge are predefined and persistent capable BaseVertex BaseEdge DB API extended with graph methods Traversing/Querying the graph: Navigator Engine Put the current path from here Follow path from here if yes: which path to follow Interfaces for Navigator Engine Path Guide (path to go next) Path Qualifier (terminate current path?) Result Qualifier (put path in result set?) Result handler is called after navigation on result set Properties of the Navigator Engine not declarative interfaces to implement some predefined queries available Neo4j Open Source DB for Java Distinction between marking transaction and finishing (committing) Only generic node class available Generic Node ( GraphDatabaseService.createNode() ) Generic Edge ( Node.createRelationshipTo(...) ) set properties as key-values ( setProperty(...) ) Generic nodes can be integrated into Java classes (different use of nodes) Graph Traversal using fluent interface (chaining method calls) Provides special methods for breadthFirst() or depthFirst() evaluator(Evaluator) Continue from here? Put node/path in result? Relationships to follow ( relationships(RelationShipType, Direction) ) all these methods return a TraversalDescription object, supports chaining evaluator looks at a Path object decides if it should be in the result decides if the traverser should continue actual path Method to be implemented: Evaluation evaluate(Path path) return value: EXCLUDE_AND_CONTINUE EXCLUDE_AND_PRUNE INCLUDE_AND_CONTINUE INCLUDE_AND_PRUNE Iterator over result set Not declarative Graph algorithms find all paths between two nodes Dijktra-based cheapest path Paths with given length Shortest paths REST API Management ( CRUD ) of nodes and edges GET / POST JSON in-/output Graph traversals Graph algorithms Query language: Cypher Declarative pattern matching supports update operations ASCII art try it online Comparison between InfiniteGraph and Neo4j Infinite Graph (Objectivity) Neo4j no database orthogonality partly data-type orthogonality ( RelationshipTypes need to be implemented) independence (persistence orthogonality) (no) independence (persistence orthogonality) Extend BaseVertex and BaseEdge Only generic nodes and edges available Navigation with implementation of interfaces (Qualifiers) Navigation with fluent interface (chaining) Navigation not declarative Navigation not declarative Part of Objectivity (object database) Some graph algorithms included REST API : CRUD , JSON , algorithms Cypher: ASCII Art, declarative query language Version Models Various models proposed temporal DBs CAD / CAM SW configuration and engineering environments Some object-oriented DBs provide versioning Comparison: Version Control Systems: unstructured data Versioning DBs: structured data Versioned Object Concept with number of states Different levels of granularity entire files tuples of relation attributes of a class objects in OOP system Each version is a possible representation of the object corresponding directly to one of its states Version Organsiations Relations between version not related (set) linerar versioning (time, list) trees (branches) DAG (merging) References Specific reference references single version of object directly Generic reference references entire object has to be dereferenced to a version when traversed default version it's not always clear which version is the default one (e.g. DAG ) Storage Strategies Representing versions at physical level store complete versions of objects store changes (deltas) between versions Delta-based approaches forward or backward deltas state-based or operation-based delta Different strategies have different perfomance storage vs. retrieval space vs. time … Operation and Interaction Models Operations Control evolution of versions (of single objects) create new version branch a parallel version merge two parallel versions delete a version Interaction or transactions models working with complex objects and object graphs automatic versioning transparent to user library model uses check-out and check-in operations long running and nested transactions Queries and Configurations additional constraint to seletc correct representations (versions) various implementations configurator evaluates rules against versioned objects declarative queries express constraints in extended language logical (based on feature logic) Dereferencing of generic references query evaluator needs to select specific version access for parallel versions access for sequencial versions Temporal Databases one of the first application domains for version models manage different time-dependent data field of research numerous approaches mostly based on relational DBs Time in Databases does time stored in a DB represent time in the domain (e.g. event schedule)? time of DB operation (e.g. insertion, update)? can the time be arbitrary modified ? is the time value provided by app or DB ? … different types of time to characterise temporal data Transaction, Registration or Physical time transaction or registration time captures when values were stored in DB AS-OF operation Valid or logical time real-world time used to express when values existed in real world WHEN operation Bitemporal: combination of physical and logical time User-defined time all other aspects of time Classification of Temporal Databases Static or snapshot DB conventional DB does not manage temporal data supports only one single version if value is updated, previous value is lost Static roll-back DB like version control system (more or less) changes can be made only on actual version errors in older versions can't be fixed keeps track of transaction time supports AS-OF operation no way to correct an error space or computation overhead (destructive vs. non-destructive) Historical DB keeps track of valid time supports WHEN changes can be made to previous values Temporal DB bi-temporal keept track of transaction and valid time supports AS-OF and WHEN operations Spatio-Temporal DB DB for moving objects continuous update of location uncertainty of location value Querying Moving Objects with Uncertainity Point Queries operators over singel trajectory Where_At(trajectory T, time t) : expected location on route T at time t When_At(trajectory T, location l) : times object expected to be at location l on T Spatio-Temporal Range Queries set of 6 boolean predicates give qualitative description of relative position location changes continuously: condition satisfied sometime or always within $[t_b,t_e] due to uncerainty: condition satisfied possibly or definitely at point $p \\in [t_b,t_e] possibly : trajectory with uncretainity sometime : don't care about time Leads to 8 possible operators Possibly_Sometime_Inside( T, R, tb te) Sometime_Possibly_Inside ( T, R, tb, te) Possibly_Always_Inside( T, R, tb, te) Always_Possibly_Inside( T, R, tb, te) Definitely_Always_Inside( T, R, tb, te) Always_Definitely_Inside( T, R, tb, te) Definitely_Sometime_Inside( T, R, tb, te) Sometime_Definitely_Inside( T, R, tb, te) Some of them are semantically equivalent Representing Temporal Data Tuple Versioning tuple is extended with attribute for temporal dimension can be realised without violating relational first normal form Example: Employee Office Salary TS TE Anne A12 5500 2000 now Bob B 34 4000 2002 2003 Bob B 34 5500 2003 now Charles C 56 6700 1995 2000 Charles C 56 7500 2000 2006 Charles C 56 7000 2006 now Denise B 34 3000 1990 1995 Denise B 34 5300 1995 2002 Attribute Versioning each attribute is extended with temporal information requires non-first normal form \\(NF&#94;3\\) relational systems Example: Employee Office Salary Anne A12 (5500, 2000, now) Bob B 34 (4000, 2002, 2003) (5500, 2003, now) Charles C 56 (6700, 1995, 2000) (7500, 2000, 2006) (7000, 2006, now) Denise B 34 (3000, 1990, 1995) (5300, 1995, 2002) Conceptual and Data Models Early models: extend relational of E/R model Bitemporal Conceptual Data Model ( BCDM ) tuple versioning using 4 additional columns per tuple transation time and valid time special ‘until changed' and ‘now' values indicate if a tuple is current query language TSQL2 extension of SQL introduces VALIDTIME and WHEN clause integrated into SQL3 as SQL /Temporal Homogeneous and Heterogenous Models homogenous if temporal domain does not vary from one attribute to another all models that use tuple versioning are homogenous heterogenous if attributes of a tuple associated to different times Storage Models Temporal relation as 3-dimensional data structure sequence of relations data cube implemented using a two level store structure primary store contains current versions satisfy all non-temporal queries history store hold in remaining history versions traditional access methods cannot be used in such a storage model Two-level Storage Structures Reverse Chaining Accession Lists Clustering Stacked Versions Engineering Databases Engineering application domains Computer-Aided Design ( CAD ) Computer-Aided Manufacturing ( CAM ) support development and maintenance of products Requirements data structures concurrency control concepts define and manage complex (and hierarchical) design objects versionen support for complex objects iterative development alternatives and trial-and-error experiments 2-dimensional version models linear revision dimension non-sequential variation dimension Design Space Version Model Modeling primitives component hierarchies ( is-a-part-of ) version histories ( is-a-kind-of , is-derved-from ) configurations combine hierarchies and version histories Software Configuration Systems Developed for SW development SW Configuration Management ( SCM ) SW Engineering Environments ( SEE ) manage product directly fully automting process of building final product built around source files and program modules reference and dependencies management complex hidden inside source code files Product Space Representation Data model with type relationships composition tree with files as leaves dependencies are sepresented within tree build information can be computed from composition and dependency relationships Without spanning tree all files of one module are summarised as one object only source dependencies are represented directly corresponds to logical structure Version Space Version model defines how objects are versioned versioned object is container for set of object version common properties shared by all versions (invariants) differences (deltas) between versions symmetric deltas directed deltas (changes) Definition of version set extentional versioning enumerates all member of version set intentional versioning uses predicate defining version set members Evolution revisions: track of history variants: alternatives can be used for cooperation and collaboration Version Space Representation One-level representation Successor relationship (Sequence) Branch (Tree) and Merge ( DAG ) Two-level representation Revision Variants Indexing Make OODBs faster and more performant Compare OOP model to Relational model Main differences between OOP and Relational model: inheritance (is-a) multitype attributes (sets, collections) First normal form in Relational Databases disallows mutlivalued attributes (collections) in a field dereferencing a path of pointers/references (traversing the object graph) OODB systems can be faster when using additional information that a RDBMSs doesn't have Different approaches to: Mapping ( ORM ) Physically represent data differently (eg. Objectivity) Use additional (meta-) data but keep the representation of the data secondary datastructures index structures Group record: pages, clusters … Type Hierarchy Indexing Key- vs Type-Grouping Key-Grouping build index along the actual attribute value as \\(B&#94;+\\) -trees additional information about typing and sub-typing in the leafs Type-Grouping index built along typing information values are secondary datastructure Extent and Full Extent Extent: all objects of a given class (without sub-classes) Full Extent: all objects of a given class and it's sub-classes Single Class Index ( SC -Index) Index construction for attribute of a type t incorporate only direct instances of a particular type in index construct search structure for all types in sub-hierarchy of t search data structures (called SC -Index components ) Evaluator needs to traverse all components referenced by query Usually implemented using \\(B&#94;+\\) -trees Type-Grouping Class Hierarchy Index ( CH -Index) One search structure for all objects of all types of indexed hierarchy One index on the common attribute for all classes of a inheritance graph Leaf node consists of key-value key directory contains an entry for each class that has instances with the key-value in the indexed attribute entry for a class consist of class identifier and offset for list of OIDs in index record number of elements in list of OIDs (for each class in the inheritance graph) list of OIDs Evaluator scans through \\(B&#94;+\\) -tree once selects OIDs f types referenced by query discards other OIDs Point queries perform good Range queries depend on number of referenced types good when queries aim at indexed type and all sub-types bad if only few types of indexed hierarchy hit by query Key-grouping structure H-Tree Set of nested \\(B&#94;+\\) -trees Combining class hierarchy with SC -Index Nesting reflects indexed type hierarchy each H-tree component is nested with H-trees of immediate sub-types H-tree index for attribute of inheritance sub-graph is H-tree hierarchy nested according to super-type-sub-type relation Avoids full scans of each B-tree component when several types queried Single type look-up: don't search nested trees Type hierarchy look-up: traverse nested trees Type-grouping structure Class Division Index ( CD -Index) Compromise between indexing for each type indexing extent for each type Specific family of type sets for indexed hierarchy Look at all possible combinations of full extends for all types Combine parts of sub-type hierarchies to use one index structure (like SC -index) Each typeset managed with search data structure Parameters q and r give bounds q : number of index structures needed to build a type extent r : number of times a type set is managed redundantly or replicated big q : slow queries big r : slow updates Multiple index structures need to be accessed for a query trade-off between: number of index structures to access for a query number of index structures to maintain Type-grouping structure Multi-Key Type Index ( MT -Index) Multi-Dimensional indexing (locality) Type information as additional attribute available Compromise between type grouping key grouping Type membership as additional object attribute symmetrical indexing of object types and attributes indexing of more than one attribute with single search structure Linearization Key-grouping structure (type is handled as an attribute) Aggregation Path Indexing Backward queries without full object tree traverses Forward queries without retrieving intermediate objects less hops to get to objects referenced over seversl levels shortcut forward Nested Index ( NX ) Direct association between an ending object and corresponding starting objects along a path Bypassing intermediate objects Only allows backwards traversals of full path Equivalent to backward traversal in ASR canonical A Nested Index NX (P) for path P with length 1 is equivalent to a Multi-Index MX (P) for the same path Updating index structure is expensive Path Index ( PX ) Records all sub-paths leading to an ending object Predicates can be evaluated on all classes along the path Equivalent to backwards traversal of right-complete ASR A Path-Index PX (P) for path P with length 1 is equivalent to Multi-Index MX (P) an a Nested Index NX (P) for the same path Updating index structure better than with NX Join-Index ( JX ) Originally for optimization of joins in Relational DBs Consists of a set of binary join indexes (Slides p. 33) Forward references are also indexed Cost compared to MX doubled Multi-Index ( MX ) Like Join-Indexes in Relational DBs Divide path (of arbitrary length) into sub-paths sub-paths have length 1 index maintained over sub-paths Query evaluation concatenating n index edges requires n index scans supports backward traversals and queries no forward traversals and queries Each index enty is represented as a pair A key-value set of OIDs of objects holding this key-value (for indexed attribute) Updates cheap Complex queries expensive (hop over nodes) each reference in object has index structure for backward references Access Support Relations ( ASR ) Given a path create 4 index structures that support all desired queries (Nested Index, Path Index, Multi-Index) Canonical representation Full representation Left representation Right representation - Related to Relational DB Joins: canonical is like regular join in SQL ASR Compositions Index Graph Queries that do not traverse the path at either endpoint can't be answered efficiently Aggregation path can be split in sub-paths (partitions) Set of partions: decompositions of an ASR Overview: Aggregation Path Indexing Nested Index and Path Index implemented using trees or hash tables Collection Operations For: sets, bags, lists, arrays new modelling features, enhanced expressiveness increased complexity of indexing and query optimisation OQL provides constructors and operators for collections Signature Files Index construction for a multi-valued property of a type Query over multi-valued properties Compute signatures for attribute values, elements … Use mathematical operation Super-set, sub-set … ‘ actual drop': true positive ‘ false drop': false positive The OM Data Model Extended Entity-Relationship model for OOP data management special features difference between typing and classification types (model): names attributes behaviour relationships objects: attributes and methods multiple inheritance, multiple instantiation (gain/lose roles), multiple classification collections as first-class concepts binary associations as first-class concepts constraints for integrity, classification and evolution data definition, manipulation and query language ( OML ) Typing and Classification Typing Classification static dynamic representation of entities roles of entities defines format of data values defines semantic groupings as collections of values defines operations defines constraits among collection defines inheritance properties Collections: member type ‘Java' classes multiple inheritance/instantiation better understanding of issues important to recognise the two concepts Reduces complexity of type graphs no need to introduce subtypes to represent each classification Rich classification structures Support for relationships between objects accociation over collections instead embedded in objects Integration of: Database Programming Languages Collections semantic groupings of objects Member types of collections constrain membership in a collection can define a view of object accessed in context of collection Object evolution objects can gain/loose roles by being added or deleted form collections type change not always required (???) OM Data Model Layers OM distinguishes typing from classification Type-layer object representation objects: type units can gain and lose types dynamically: dressing and stripping multiple inheritance multiple instantiation objects can have types from parallel inheritance hierarchies objects can have types that are completely unrelated Classification-layer based on types from type layer semantic groupings: collections membership constrained by type associations to link objects together kinds and roles constraints subcollections and supercollections cardinality to describe associations evolution constraints … multiple classification collections and associations Associations can be nested model n-ary relationships clearer semantics allow uniform query language Things that exist types values for attributes Collection is not just contains objects relationship to objects name constraint (subset relation) joint disjoint associations collection to pairs e.g: A person becomming a friend or an enemy doesn't change (or add/remove) any properties of a person type Objects can gain state and behavior dynamically distinguish object: can refer to, identity, can be pointed to instance: container of values of a particular type Relationships intentional / extensional Query Decide if something in a collection or not Traversing result (iterator, cursor) SQL relationships mapped by primary-/foreign-key select can be used instead of joins to follow relationships A selection query is fully specified when: providing a set of candidates and a function that takes individual candidates and returns true or false OM Model queries Declaritive query language Collections are sets of candidates for queries Map-Reduce Collection Constraint Disjoint: XOR Cover: OR Partition: one and only one needs to be true Associations (Relationships) binary relation between 2 collections cardinality tuple as type of collection can be nested: n-ary relationships Object Model Language ( OML ) Declarative OOP language for OM data model OML Data Definition Language object and structured type definition method definition and emplementation collection, association and constraint definition OML Data Manipulation Language creating and updating objects ceate, update, delete, dress and strip operations OML Query Language expressions and functions for values of base types operations on objects access attributes invoke methods operations on collections (collection algebra) map-reduce very important operations after selection Collection Algebra Operations defined for collections of OM model union intersections difference selection map takes a set of objects and applies a function on each object the result set has the same carinality reduce take a set of values (objects) return one value (object) flatten Sematics of operations depend on collection behaviour set theory defines semantics generalisation for bag , sequence and ranking mixing and converting collection types Binary Collections All collections-operations can be applied to binary collection Binary collections suort additional operations domain and range not same as collection with tuple containing ‘range' and ‘domain' domain and range restriction (selection) subtraction inverse swapping domain and range nest grouping of binary collections compose find associatd objects in DB it may be necessary to compose ( join ) associations closure repeatedly compose binary collection with itself division divide a binary collection with a unary collection","tags":"Programming","url":"object_databases.html","loc":"object_databases.html"},{"title":"Synchronization Primitives (Read-Modify-Write)","text":"Read-Modify-Write Compare-And-Swap Todo Load-Link/Store-Conditional This works: load_link ( R ); // ... store_conditional ( R , x ); // ok! But this fails: load_link ( R ); // ... store_conditional ( R , x ); // possibly other thread (ok!) // ... store_conditional ( R , x ); // fails! Test and Set Pseudo code: // atomic test_and_set ( V : mem_address ) : // V in {0,1} tmp : = V V : = 1 return temp Reset: // normal write reset ( V : mem_address ) : V : = 1 Generic RWM Pseudo code: rwm ( V : mem_address , f : function ) return value tmp : = V V : = f ( V ) return tmp Mutex (Shared Memory) Mainly asynchronous shared memory systems read/write: atomic but only either read or write at a time Mutex code Requirements: At most one process in critical section No deadlocks if there is a process in the entry section then later there is a process (maybe an other one) in the critical section No lockout (starving) if there is a process in the entry section then later the same process is in the critical section Unobstructed exit no process is stuck in the exit section (no loops…) General: 1 bit suffices for mutex without deadlock O(log(n)) bits needed for fairness","tags":"Programming","url":"synchronization_primitives_(read-modify-write).html","loc":"synchronization_primitives_(read-modify-write).html"},{"title":"Models of Computation","text":"Boolean Circuits AND / OR / NOT gates Gate: function Inputs: arguments to function In-/Outputs: wires Circuits: combination of gates Truth table: all possible combination (look-up table) XOR Parity: odd/even 0: even 1: odd 0 is even number, 1 is odd number works with more than 2 inputs X Y Z XOR 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 Cost of circuit: number of AND gates and OR gates usually NOT gates are not counted (they are too simple) Truth table: for \\(n\\) inputs: \\(2&#94;n\\) combinations Number of functions: for \\(n\\) inputs: \\(2&#94;{2&#94;n}\\) functions (circuits) General of XOR : for \\(n\\) inputs \\(2&#94;{n-1}+1\\) gates needed Finite State Machines ( FSM ) Also called Finite State Automaton ( FSA ) See also Tutorialspoint Can be represented as a 5-touple \\((Q, \\Sigma, \\delta, q_0, F)\\) : \\(Q\\) : finite set of states \\(\\Sigma\\) : alphabet (finite set of symbols) \\(\\delta\\) : transition function \\(q_0\\) : initial state ( \\(q_0 \\in Q\\) ) \\(F\\) : set of final states ( \\(F \\subseteq Q\\) ) Deterministic / Non-Deterministic FSM Deterministic For each arriving event it's clear what the next state is Non-deterministic For some (or all) events the next state can be more than one (it's not clear to which state the machine will change) Any non-deterministic FSM can be turned into a deterministic FSM Acceptor (Recognizer) Calculates a Boolean function All states are either accepting or rejecting for a given input In graphical notation the accepting states have a double circle Non-deterministic Acceptor accepts if there is any way to accept methodically try all possibilities ‘ Parallel World' - try all possibilities simultaneously in separate copies of the machine (for complexity theory) Classifier Has more than two final states gives single output when it terminates Transducer Produces outputs based on current input and on previous state 2 different types Mealy Machine: output depends only on current state Moore Machine: output depends on current state and current input They can be transformed into each other Register Machines A register machine has multiple registers that store positive integers There are only two possible operations on these registers incrementing ( \\(+1\\) ) decrementing ( \\(-1\\) ) - Decrementing a register that holds \\(0\\) fails There are other (slightly different) definitions of register machines that allow different operations (e.g checking for \\(0\\) ) Register Machines can be graphically represented like FSMs or as a simple list of instructions (programming language) Example of list of instructions for a Register Machine: Action Next Instruction Alternative Instruction (if actual instruction fails) 1. x- 2 4 2. y+ 3 - 3. x- 1 8 4. y- 5 6 5. x+ 4 - 6. x- 7 5 7. x- 9 HALT 8. y- 9 3 9. x+ 10 - 10. x+ 11 - 11. x+ 8 - Register Machines and Turing Machines can simulate each other in polynomial time For each Register Machine with more than 2 registers ( \\(N\\) ) there is an equivalent Register Machine with only 2 registers The \\(N\\) registers need to be encoded Whenever a loop ends (in a 2 Register Machine) one register is \\(0\\) in the other register is some information available Fractran See also Wikipedia:Fractran Example PRIMEGAME : $${\\frac {17}{91}},{\\frac {78}{85}},{\\frac {19}{51}},{\\frac {23}{38}},{\\frac {29}{33}},{\\frac {77}{29}},{\\frac {95}{23}},{\\frac {77}{19}},{\\frac {1}{17}},{\\frac {11}{13}},{\\frac {13}{11}},{\\frac {15}{2}},{\\frac {1}{7}},{\\frac {55}{1}}$$ State/Memory: an Integer Computation: multiply by first fraction that yields an integer Start with \\(2\\) : calculate primes. Implementation in racket: Fractran is similar to stack machines. Tilings See also Wikipedia:Wang tile Graphical model of computation The edges of each tile have special form/color Tiles can be arranged on plain so that edges match Tiles as Turing Machine Tiled plain can be seen as Turing Machine over time one dimension (horizontal, x) is tape other dimension (vertical, y) is time so each row shows the tape at a given time 3 kind of tiles needed Kind of Tile Use TAPE One for each symbol in the alphabet ACTION One tile for each transition of the TM PREPARE Two for each (symbol, state) pair Seed row: initial configuration We can tile the floor with the tiles if TM halts Cellular Automata Inspired by natural cells World is grid of cells Cells have state (e.g. dead/alive) Next state of a cell is calculated by actual state of the cell actual state of the neighbor cells All cells work with same rules Time is discrete (round based) Often used in physics Good model for unreliability parallel, local comuptation can model a Turing Machine The Model \\(N\\) -dimensional array of cells (infinitely large) Finite number of states each cell is in some state At each time step, each cell updates its state based on it's own state the states of its neighbors Variations: number of dimensions neighborhood size (and geometry) asynchronouns error-prone (add randomness) finite size grid type Variants Conway's Game of Life My implementation (Qt, C++) Toom's rule asymetrc rule Lambda Calculus Meta-Mathematics Turing Machines simulate a person that calculate (State: Mind, Tape: Paper) Lambda Calculus: about functions Syntax Traditional Syntax: $$f(x) = \\frac{e&#94;x - sin(x)}{x+3}$$ Lambda Syntax: $$\\lambda x. \\frac{e&#94;x - sin(x)}{x+3}$$ Functions in lambda calculus don't have names Just put the function completely, where it is used Anonymous functions Functions: ‘Plugging in' arguments Functions return other functions (Currying) For example: $$((\\lambda xy. 2x + y) 2 ) 3 = (\\lambda y. 4 + y) 3 = 4 + 3 = 7$$ Can get rid of function names multi-argument functions Main Operations \\(\\beta\\) -reduction: apply a function to an argument using substitution Reduction is an optimistic term since the result of the \\(\\beta\\) -reduction can be bigger then the expression before \\(\\alpha\\) -conversion: changing a functions ‘argument symbol' ( \\(\\lambda x.x \\equiv \\lambda y.y\\) ) \\(\\eta\\) -conversion: \\(\\lambda v. f v \\equiv f\\) because \\((\\lambda v. f v) a \\equiv f a\\) Church Numerals Numbers can be encoded as functions Arbitrary encoding of numbers suggested by church Each number is a function that takes 2 arguments: \\(f\\) and \\(x\\) : $$\\begin{align*} 0 :&= \\lambda f.\\lambda x. x\\\\ 1 :&= \\lambda f.\\lambda x. f x\\\\ 2 :&= \\lambda f.\\lambda x. f (f x)\\\\ 3 :&= \\lambda f.\\lambda x. f (f (f x))\\\\ \\cdots \\\\ n :&= \\lambda f.\\lambda x. f&#94;n x \\end{align*}$$ The number \\(n\\) is a function that takes a function \\(f\\) as argument and applies it \\(n\\) -times to the second argument \\(x\\) Example: \\((5 inc) x\\) : apply function \\(inc\\) \\(5\\) -times to \\(x\\) Lambda Calculus Expressions Using just substitution step to calculate seems complex, but only one operation needed: substitution Variables \\(a\\) , \\(b\\) , \\(c\\) … representing functions Function Application \\((a b)\\) : \\(a\\) applied to \\(b\\) Function Creation \\(\\lambda a. aa\\) Evaluation Example: Examples Increment $$\\lambda k f x. f ( k f x)$$ Example: increment \\(2\\) $$ \\begin{align*} \\underbrace{(\\lambda k f x. f ( k f x))}_{inc}\\underbrace{(\\lambda f x. f ( f x))}_{2} &= \\\\ (\\lambda f x. f ( (\\lambda f x. f ( f x)) f x)) &= \\\\ (\\lambda f x. f ( f ( f x))) \\end{align*} $$ Addition Multiplication Boolean Logic If-Then-Else Pair Conclusion No distinction between code and data: everything is code! No structure is safe from substitution (safety issue) Data has to be run, can't be examined Recursion in \\(\\lambda\\) -calculus a function can't reference (call) itself since there are no names for functions Tail-call recursion: in some cases the same stack frame can be reused for recursive functions How can we build an infinite loop (recursion)? This structure rebuilds itself infinitely: $$\\Omega = (\\lambda w. w w) (\\lambda w. w w)$$ Y-Combinator See Wikipedia and The Y Combinator (no, not that one) . $$Y = \\lambda f. (\\lambda x. f (x\\; x))(\\lambda x. f (x \\;x))$$ Beta recursion gives: $$\\begin{align*} Y g &= \\lambda f. (\\lambda x. f (x\\; x))(\\lambda x. f (x\\; x)) g \\\\ &= (\\lambda x. g (x\\; x)) (\\lambda x. g (x\\; x)) \\\\ &= g ((\\lambda x. g (x\\; x)) (\\lambda x. g (x\\; x))) \\\\ &= g (Y\\; g) \\end{align*}$$ Only way to do a loop if you don't know in advance how many iterations the loop needs to be done Functions can't see their own structure Needs to be provided as argument so the function can reproduce itself: ‘twin'-functions Numbers Numbers are an abstract concept It's only possible to manipulate representations of numbers There Are a lot of different representations of numbers. Each representation has it's pros and cons. Decimal Roman Numerals Binary Two's complement Binary with digits up to 2 (Redundant binary representation) Addition can be faster Logical operation are slower Ternary Church Numerals Prime Factorization: \\(1, 2, 3, 2&#94;2, 5, 2\\cdot 3, 7, 2&#94;3, 3&#94;2, \\ldots\\) Multiplication and Factorization is easy Increment by one is hard Chinese remainder theorem Comparing two numbers is hard p-adic number related to two's complement Tag Systems Tape with a starting string Reading and deleting at the beginning (left) Appending (writing) at the end (right) 2-Tag System reads 1 symbol at the beginning of the tape appends a string at the end of the type appended string depends on read symbol removes 2 symbols at the beginning of the tape Example: Test for odd/even number Alphabet: \\(\\Sigma = \\{C_1, C_2,C_3\\}\\) Rules: \\(C_3 \\rightarrow \\epsilon\\) \\(C_1 \\rightarrow C_2 \\; O\\) \\(C_2 \\rightarrow E\\) \\(\\epsilon\\) : empty word \\(O\\) : Odd \\(E\\) : Even \\(C_1 \\; C_1 \\; (C_3)&#94;x\\) \\(a&#94;x\\) : symbol a is repeated \\(x\\) times the given rules show if \\(x\\) is odd or even Example even: $$\\begin{align*} C_1 C_1 C_3 C_3 C_3 C_3 \\rightarrow \\\\ C_3 C_3 C_3 C_3 C_2 O \\rightarrow \\\\ C_3 C_3 C_2 O \\rightarrow \\\\ C_2 O \\rightarrow \\\\ E \\end{align*}$$ Example odd: \\begin{align*} C_1 C_1 C_3 C_3 C_3 \\rightarrow \\\\ C_3 C_3 C_3 C_2 O \\rightarrow \\\\ C_3 C_2 O \\rightarrow \\\\ O \\end{align*} Example: Power of Two \\begin{align*} 2&#94;n &\\rightarrow n \\\\ C_1 &\\rightarrow C_1 C_2 C_4 C_5 \\\\ C_2 &\\rightarrow \\epsilon \\\\ C_3 &\\rightarrow C_3 \\\\ C_4 &\\rightarrow C_4 C_5 \\\\ C_5 &\\rightarrow C_6 \\end{align*} Starting with: $$(C_3)&#94;{2n}C_1C_1 \\rightarrow (C_6)&#94;n$$ For \\(n=2\\) : $$\\begin{align*} C_3 C_3 C_3 C_3 C_1 C_1 \\rightarrow \\\\ C_3 C_3 C_1 C_1 C_3 \\rightarrow \\\\ C_1 C_1 C_3 C_3 \\rightarrow \\\\ C_3 C_3 C_1 C_2 C_4 C_5\\rightarrow \\\\ C_1 C_2 C_4 C_5 C_3 \\rightarrow \\\\ C_4 C_5 C_3 C_1 C_2 C_4 C_5 \\rightarrow \\\\ C_2 C_4 C_5 C_4 C_5 C_3 \\rightarrow \\\\ C_5 C_4 C_5 C_3 \\rightarrow \\\\ C_5 C_3 C_6 \\rightarrow \\\\ C_6 C_6 \\end{align*}$$ Simulation of Turing Machines with Tag Systems Some of the notes here are taken from ‘Understanding Computation' by Tom Stuart. Tape uses only 2 characters: \\(0\\) and \\(1\\) \\(0\\) is the blank character Split the tape into two pieces left part character under the head all characters left of the head right part all characters right of the head Interpret left part as binary number Interpret right part as binary number written backwards Encode those 2 numbers as string (suitable to be used for Tag System) Simple operations: Simulate reading from tape writing to tape moving the head with simple operations doubling / halfing incrementing / decrementing odd / even (parity) check … e.g move head to right: doubling left number halving right number reading from tape check if left number is even/odd writing \\(1\\) : incrementing number \\(0\\) : decrementing number Current state of simulated Turing machine represented by different encoding e. g State 1: use a , b , c State 2: use d , e , f … Convert each Turing Machine rule into a Tag Systems that rewrites the current string in the expected way Combine all the Tag Systems to make a large one the simulates every rule of the Turing Machine Cyclic Tag Systems This are simplified Tag Systems The tape is allowed only to have 2 symbols \\(0\\) and \\(1\\) A string is only appended if a \\(1\\) is read The rules for appending strings is ordered The deletion number is \\(1\\) Algorithm: In each round: Read next rule in rule-set Read next symbol on tape if \\(1\\) : append the string from the rule set to the tape if \\(0\\) : ignore the rule Remove the last read symbol from the tape Start from the beginning with reading the next rule A Cyclic Tag System can simulate a normal Tag System. See ‘Understanding Computation' by Tom Stuart on how to do that. Diophantine Equations See also Wikipedia:Diophantine equation Polynomal equations Only integer solutions are sought Properties no state no dynamics no time no loops no ‘ IF '/' THEN ' just equations which all must be true How to compute? Consider a space-time history Solve it in the digits of a large number Use equations to force it to be valid Primitive Recursive Functions Recursion can do a lot! Functions are defined using itselves or other functions Each function has 2 definition: Definition for argument \\(0\\) General definition Function Name Definition ( \\(\\text{if } n=0\\) ) Definition ( \\(\\text{if } n=S(m)\\) ) \\(S(n) = n + 1\\) Successor intrinsic/primitive - \\(A(n,a)=n+a\\) Addition \\(a\\) \\(S(A(m,a))\\) \\(M(n,a)=n\\cdot a\\) Multiplication \\(0\\) \\(A(a,M(m,a))\\) \\(E(n,a)=a&#94;n\\) Exponentiation \\(1\\) \\(M(a,E(m,a))\\) \\(V(n)=sign(n)\\) Positivity \\(0\\) \\(1\\) \\(P(n) = n-1\\) Predecessor \\(0\\) \\(m\\) \\(D(a,n) = a-n\\) Subtraction \\(a\\) \\(P(D(n,m))\\) \\(R(n,a)=n \\bmod a\\) Remainder \\(0\\) \\(M(S(R(m,a)),V(D(P(a),R(m,a))))\\) \\(C(n,a)=\\prod_{i=2}&#94;{n+1} a \\bmod i\\) ‘mod Product' \\(0\\) \\(M(C(m,a),R(a,(S(S(m)))))\\) \\(Z(n)=1\\) if prime ; \\(0\\) else Primality \\(0\\) \\(V(M(m,(P(m),S(m))))\\) Infinite loops not possible Can simulate a Turing Machine only for a given number of steps (weaker than TM 's) Number of iterations of a loop need to be known from beginning Infinite loops are not possible Not considierd as universal system Some Simple Models Chemical Reaction Networks Inspired by chemical reaction equations $$\\begin{matrix} A & \\rightarrow & B \\\\ C & \\rightarrow & D \\\\ B + C & \\rightarrow & A \\\\ A + D & \\rightarrow & A + 2E\\\\ B + E & \\rightarrow & B + D \\end{matrix}$$ Nondeterministic: any reaction can happen at any time Primitive Recursive Functions can always compute this in bounded time, even if answer is \\(\\infty\\) . Chemical Reaction Networks are not stronger than Primitive Recursive Functions. Petri Nets A lot of different form of Petri Nets Components Transition (edges) Places (nodes) Tokens (arcs) travel from transitions to places (or vice versa) Nondeterministic Similar to (and as powerful as) Chemical Transition Networks Used to simulate concurrency in distributed systems (but not turing-complete) Vector Addition Systems Rules: Starting position and list of possible moves (like a knight in chess) no coordinate ca go negative N-dimensional is posible A B C D E F G -1 1 0 0 0 0 0 0 0 -1 1 0 0 0 1 -1 -1 0 0 0 0 -1 0 0 -1 0 1 0 1 0 0 -1 0 1 0 0 -1 0 0 -1 0 1 0 1 1 0 0 0 -1 7 moves for 7 dimensions Non-deterministic Same as Chemical Reduction Networks Put equations in vectors (linear combination) Unordered Fractran Similar to fractran Choose any fraction randomly that yields an integer when multiplied with given number non-deterministic Broken Register Machine decrementing a register might fail even if register could be decremented General Notes A lot of these systems get more powerful if they allow prioritizing determinism: turing complete Halting Problem Suppose a Turing Machine \"A\" can take \\(Enc_1(table)\\) , \\(Enc_2(tape)\\) and decide wether a TM with the given table will ever halt, if started on the given tape. This is impossible! Idea (proof): Make a machine that does the opposite of whatever machine you give it (halt or not halt) The goal is to feed this machine to itself Analyze what machines do when they are fed to themselves Universal Machine It's possible to create a \"universal\" machine, which just (slowly) does the same thing that the machine described to it would do. a programmable Turing Machine see Universal Turing machine","tags":"Programming","url":"models_of_computation.html","loc":"models_of_computation.html"},{"title":"SystemC","text":"SystemC is a system-level modeling language. It's implemented as a C++ library. The library is open source and platform independent. It contains an event-driven simulation kernel for executing models. Accellera is the main resource for SystemC. Simulation SystemC contains a kernel for discrete-event driven simulation. The main purpose of the kernel is to ensure to model concurrent activities (parallelism) correctly. The simulation clock represents the current value of the simulation time. Initialization of simulation model Set initial states of subsystem modules Fill event queue with initial events Timing routine Determine next event from event queue Advance simulation clock to the time when the event is to occur Event routine Update the system state when a particular type of event occurs A more detailed description can be found on the Doulos tutorial : Initialization: execute all processes (except SC_CTHREAD s) in any (undefined) order Evaluation: select a process (ready to run) and resume its execution This may cause immediate event notifications to occur which may result in additional processes being made ready to run in this same phase Repeat step 2. until there are no processes ready to run Update: execute all pending calls to update() resulting from calls to request_update() made in step 2. If there were any delta event notifications made during steps 2. or 4. determine which processes are ready to run due to all those events and go back to step 2. If there are no timed events: simulation is finished Advance the simulation time to the time of the earliest pending timed event notification Determine which processes are ready to run due to all the timed events at what is now the current time go back to step 2. (note: the list above is taken mostly from the Doulos tutorial) Delta Cycle Within the same simulation cycle cause and effect events may share the same time of occurrence. The simulator uses a zero duration virtual time interval : delta cycle The delta cycle is the smallest simulated time slot. It consist of two phases: Evaluation phase Update phase Separating the two phases makes it possible to guarantee deterministic behavior. A channel will not change it's value until the update phase occurs. It cannot change the value during the evaluation phase. It's possible to run a process without the update phase (without the delta cycle) with notify() . Constructs Modules Building blocks of SystemC models. Hierarchy (subsystems) Abstraction IP reuse Modules are called by the simulation engine if an relevant event is scheduled. Modules process events, manipulate the event queue and contain (and manipulate) the system state. Processes Processes must be contained in a module. SC_THREAD Typically called once, run in an endless loop Can be suspended with wait() Keep the state of execution implicitly SC_METHOD Execute repeatedly form beginning to end Simulate faster Do not keep state of execution implicitly Wait and Notify wait: halt process execution until an event is raised wait() wait with arguments: dynamic sensitivity wait(sc_event) wait(time) wait(time_out, sc_event) notify: raise an event (method of sc_event class) my_event.notify() : immediate notification. Processes sensitive to the event will run during current evaluation phase notify with arguments: delayed notification my_event.notify(SC_ZERO_TIME) : notify during evaluation phase of next delta cycle my_event.notify(time) : notify after time (in evaluation phase of that delta cycle) notify allows non-deterministic behavior Communication Event Flexible low-level synchronization primitive Channel Container for Communication and Synchronization Can have state (private data), transport data/events Implement one or more interfaces Interface Set of access methods to channel Interface methods need to be implemented (pure virtual) Other communication and synchronization models can be built based on these primitives. Channels Separating communication from behavior Interfaces define access to channel Ports are used to access the channel Non-Determinism SystemC allows for non-determinism. This might not be acceptable for hardware modeling. But it can be needed for software system modeling. An example of non-determinism can be found here: Doulos: Primitive Channels and the Kernel . It shows two threads ( SC_THREAD s) in a module that both access the same variable. It's not known in which order the variable is accessed. In software systems a mutex might be needed to control the access to the variable.","tags":"Programming","url":"systemc.html","loc":"systemc.html"},{"title":"Open Source Software","text":"Open Source, Free Software, Proprietary and Companies interests I'm a big fan of Open Source/Free Software. I'm very exited how many projects are around and widely used and supported. But I also observe the fear of companies with a more traditional background to touch anything that has to do with open source let alone to put some of their work into the public. Open Source or Free Software or Beer or Speech or Gratis or … All this different expressions for the same concepts are just confusing. And arguing about them doesn't help to make the idea behind this buzzwords to get a broader acceptance. Permissive Copy Left/Right There are a lot of confusing different license strategies. If a new project is started it's important choose (or define an new) license carefully. It not only gives the initiator of the project the legal security, but it is also a statement to the public. Companies might consider to use open source with a permissive license than an copy left one since they don't need to publish the source code of their product. They might even do some improvements that then get's back to the community. And it could even happen that they consider to give the comunity something back and release some library or tool to the public. Companies should release at least some libraries or frameworks as open source. They might get improvements from the community. And if the architecture is usable for other projects it means it's flexible, extensible and decoupled from other code.","tags":"Programming","url":"open_source_software.html","loc":"open_source_software.html"},{"title":"Cosine Distance","text":"useful in spaces that have dimensions Euclidian spaces Discrete euclidian spaces (i.e vectors of integers, vector of bools) … Points are thought as directions No distinction between a vector and a multiple of it The Cosine Distance between two points is the angle that the vectors to these points make The angle will be between \\(0&#94;{\\circ}\\) and \\(180&#94;{\\circ}\\) regardless of how many dimensions the space has $$d_{cos}(\\vec a, \\vec b) = arcos \\left ( \\frac{\\vec a \\cdot \\vec b}{\\| \\vec a \\|_2 \\cdot \\| \\vec b \\|_2} \\right ) = arcos \\left ( \\frac{\\sum_{i=1}&#94;na_i \\cdot b_i}{\\| \\vec a \\|_2 \\cdot \\| \\vec b \\|_2} \\right )$$","tags":"Mathematics","url":"cosine_distance.html","loc":"cosine_distance.html"},{"title":"L2-Norm","text":"L2-Norm A vector \\(\\vec a\\) has the \\(L_2\\) -Norm: $$\\left \\| \\vec a \\right \\|_{2} = \\sqrt{\\sum_{k=1}&#94;n|a_k|&#94;2}$$ \\(n\\) : Dimensions \\(a_k\\) : \\(k\\) -th element of \\(a\\) L2-Distance The \\(L_2\\) -Distance between two vectors \\(\\vec a\\) and \\(\\vec b\\) is: $$d \\left ( \\vec a, \\vec b \\right ) = \\sqrt{\\sum_{k=1}&#94;n|a_k - b_k|&#94;2}$$","tags":"Mathematics","url":"l2-norm.html","loc":"l2-norm.html"},{"title":"Git: Commit a older version to HEAD","text":"If a HEAD has to be reset to a older version this is a possible workflow. The solution is from stackoverflow First checkout the version that should be the new HEAD . git checkout <SHA-1 id of commit> Now we create a temp branch and check it out. git checkout -b temp This is equivalent to git branch temp followed by git checkout temp . Now we want master (or an other branch) to point to it. git checkout -B master temp This is equivalent to git branch -f master temp followed by git checkout master . Now we can delete the temp branch: git branch -d temp Now we can push to a remote ( -f might be needed): git push -f origin master","tags":"Programming","url":"git__commit_a_older_version_to_head.html","loc":"git__commit_a_older_version_to_head.html"},{"title":"Hardware-Software Codesign","text":"0. Overview Specification and Models of Computation State-Charts Kahn Process Networks Design Space Exploration Mapping Partitioning Multi-Criteria Optimization Performance Estimation Simulation-based Methods Worst-Case Execution Time Analysis Performance Analysis of Distributed Systems Thermal-aware Design 1. Introduction System Design Specifications and Models Design Estimation Embedded Systems Information processing system embedded in a larger product Interface to outside world external process Sensors, actors Human interface Technical environment does not forgive errors Stricter requirements (usually update not possible) Difference Embedded Systems ↔ General Purpose Computing Embedded Systems General Purpose Computing Few Applications, known at design-time Broad class of applications Not programmable by end user Programmable by end user Fixed run-time requirement Faster is better Levels of abstraction (from higher to lower) Transistors Gates Processor (Memory, Register, ALU , …) Architecture ( CPU , ASIC , FPGA , LAN , …) Distributed System Design Design aspects look similar in every level Design goes usually from higher abstraction level to lower level Abstraction: leave out (unimportant) details System level design: SW Synthesis (compile) HW Synthesis Interfaces Estimation Reason about solution Different solutions for a specific task Precision of estimation is different on each level Higher level estimation usually less precise than lower level Mapping Mapping parts of specification to different parts of the system Similar to scheduling but bigger design space (including HW ) Specification Specification: in embedded systems higher requirements to correctness Specific languages: Developer is restricted (lesser errors) Better code generator/synthesizer Doing things right is difficult! Observer Pattern is not save! Race conditions Dead-Locks Use non-locking algorithms! 2. Models of Computation Model of computation = Components + Communication Hierarchy Behavioral Structural Timing State oriented behavior (micro controller, FPGA ) Dataflow oriented behavior ( DSP ) State Charts Classical automata (Moore/Mealy): FSM State: Information needed to get (determine) the output form the input State Charts introduce hierarchy Combining States with Sub-States in Super-States Active States Basic States (no Sub-States) Super-States (ancestor states): OR -Super-States (hierarchy) Default State, History State (can be combined) AND -Super-States (concurrency) 2 distinct features form FSM Hierarchy Difference of Control-Path and Data-Path Communication between Control- and Data-Path Control-Path to Data-Path: action Data-Path to Control-Path: condition Events, Conditions, Actions $$\\underrightarrow{event [condtition] / action}$$ No storage of events (only available for next step) Events are global Evaluation of State Charts Evaluation is not trivial! Three phases Event is emitted Transition is selected Simultaneously make transition and apply the actions Execute the right hand side of action simultaneously and assign then after Specification and Description Language ( SDL ) Targeted at unambiguous specification and description of systems Asynchronous message passing Appropriate also for distributed systems Communication between FSMs (or processes) message passing with FIFO queues FIFOs can be indefinitely large Each process fetches entry from FIFO if input enables transition it takes place otherwise discard input All orders of events are legal: different correct behavior in simulators (not deterministic) Dataflow Languages Imperative language style ( program counter ) Movement of data is priority Scheduling: responsibility of the system (not the programmer) Basic characteristics: all processes run ‘simultaneously' Processes can be described with imperative code Processes can only communicate through buffers Sequence of read tokens is same as of read tokens Useful for applications that deal with streams of data Concurrent: maps to parallel hardware Perfect for block-diagram specifications (control systems, signal processing) Kahn Process Networks General-purpose scheme for parallel programming read : destructive and blocking (reading empty channel blocks until data is available) write : non-blocking FIFO : infinite size Unique attribute: determinate Determinacy Random: knowing about system and inputs is not sufficient to determine output Determinate: the histories of all channels depend only on the histories of the input channels Importance Functional behavior is independent of timing (scheduling, communication time, execution time of processes) Separation of functional properties and timing Adding Non-Determinacy Several possibilities Allow processes to test for empty queues Allow multiple processes to write to or read from one channel Allow processes to share a variable (memory) Synchronous Dataflow ( SDF ) Restriction of Kahn Networks to allow compile-time scheduling Each process reads and writes a fixed number of tokens each time it fires Firing is an atomic process Schedule can be determined completely at compile-time Steps: Establish relative execution rates (solving a system of linear balancing equations) Determine periodic schedule by simulating system for a single round 3. Mapping Application to Architecture System synthesis from specification Allocation: Select components ( HW ) Binding: Bind SW (Application) to HW (Components) Scheduling: Executing code Mapping = Binding + Scheduling Specification Examples Data-Flow Graph ( DFG ) x = * 3 a + B * b - c ; y = a + b * x ; z = b - c * ( a + b ); no loops parallelism Data Flow Graph on Wikipedia Control-Flow Graph ( CFG ) what_is_this { read ( a , b ); done = FALSE ; repeat { if ( a > b ) a = a - b ; else if ( b > a ) b = b - a ; else done = TRUE ; } until done ; write ( a ); } State machine Conditional branches Control Flow Graph on Wikipedia Architecture Specification Reflects structure and properties of underlying platform Can be done at different abstraction levels Mapping Specification Mapping: application and architecture specification binds processes to processors binds communication between processes to architecture communication paths specifies resource sharing and scheduling 4. System Partitioning Partitioning: Given a set of objects: Partition this set in subsets. Mapping: Subset is mapped to specific resource. Mapping at different Levels Low Level Register Transfer Level ( RTL ) or Netlist Level Split a digital circuit and map it to several devices (FPGAs, ASICs) System parameters (e.g., area, delay) relatively easy to determine High Level System Level Comparison of design alternatives for optimality (design space exploration) System parameters are unknown and difficult to determine to be estimated via analysis, simulation, (rapid) prototyping Cost Functions system cost ( C[$] ) latency ( L[sec] ) power consumption ( P[W] ) … Estimation Estimation is required to find C , L , P values, for each design point i.e $$f(C,L,P)= k_1 \\cdot h_C(C,C_{max})+ k_2 \\cdot h_L(L,L_{max})+ k_3 \\cdot h_P(P,P_{max})$$ \\(h_C\\) , \\(h_L\\) , \\(h_P\\) : denote how strong \\(C\\) , \\(L\\) , \\(P\\) violate design constraints \\(C_{max}\\) , \\(L_{max}\\) , \\(P_{max}\\) \\(k_1\\) , \\(k_2\\) , \\(k_3\\) : weighting and normalization The Formal Partitioning Problem Assign \\(n\\) objects \\(O= \\{ o_1, \\cdots , o_n \\}\\) to \\(m\\) blocks (also called partitions) \\(P= \\{ p_1, \\cdots , p_m \\}\\) , such that \\(p_1 \\cup p_2 \\cup \\cdots \\cup p_m = O\\) (all objects are assigned - mapped) \\(p_i \\cap p_j = \\{ \\}\\; \\forall i,j:i\\neq j\\) (an object is not assigned or mapped twice) and costs \\(c(P)\\) are minimized Partitioning Methods Exact vs. Heuristic Methods: Exact provides optimal solution (or set of solutions) Heuristic provides a ‘good' solution but not best Overview: Exact Methods Enumeration Integer linear programs ( ILP ) Heuristic Constructive Methods Random mapping Hierarchical clustering Iterative methods Kerninghan-Lin algorithm Simulated annealing Evolutionary algorithm Hierarchical Clustering Combine always 2 nodes Then recalculate Repeat until goal achieved Steps can be visualized in a tree diagram Integer Linear Programming ( ILP ) Exact method Objective function Constraints Objective: $$C = \\sum_{x_i\\in X}a_ix_i\\; with\\; a_i \\in \\mathbb{R}, x_i \\in \\mathbb{N}$$ Constraints: $$\\forall j \\in J: \\sum_{x_i \\in X}b_{i,j}x_i \\geq c_j \\; with \\; b_{i,j}, c_j \\in \\mathbb{R}$$ Integer programming problem: Minimize objective function subject to constraints Example for Partitioning optimize for a load balanced system Scheduling: Task t0 t1 t2 t3 Processor0 1 1 0 0 Processor1 0 0 1 1 Run Times: Run time t0 t1 t2 t3 Processor0 5 15 10 30 Processor1 10 20 10 10 Cost \\(c_{i,k}\\) : $$\\left.\\begin{matrix} c_{0,0} = 5\\\\ c_{0,1} = 10\\\\ \\cdots \\end{matrix}\\right\\} \\begin{matrix} 0 \\leq i \\leq 3\\\\ 0 \\leq k \\leq 1 \\end{matrix}$$ Binary variables \\(x_{i,k}\\) : $$\\left.\\begin{matrix} x_{0,0} + x_{0,1} = 1\\\\ x_{1,0} + x_{1,1} = 1\\\\ \\cdots \\end{matrix}\\right\\} \\sum_{k=0}&#94;1 x_{k,i} = 1\\; \\forall\\; 0 \\leq i \\leq 3$$ Approaches Approach: $$min\\left \\{ \\sum_{i=0}&#94;3 \\sum_{k=0}&#94;1 c_{i,k} x_{i,k} \\right \\}$$ 2. Approach: $$min\\left \\{ \\left | \\sum_{i=0}&#94;3 c_{i,0} x_{i,0} - \\sum_{i=0}&#94;3 c_{i,1} x_{i,1} \\right |\\right \\}$$ This is not a linear problem! Solutions 1. Solution: Solving 2 linear problems: split approach 2 into 2 linear problems: $$min \\left \\{ \\left | \\underbrace{\\sum_{i=0}&#94;3 c_{i,0} x_{i,0}}_{l_1} - \\underbrace{\\sum_{i=0}&#94;3 c_{i,1} x_{i,1}}_{l_2} \\right | \\right \\}$$ Becomes: $$\\begin{matrix} l_0 \\geq l_1: min\\left \\{ l_0 - l_1 \\right \\} \\\\ l_1 \\geq l_0: min\\left \\{ l_1 - l_0 \\right \\} \\end{matrix}$$ 2. Solution: Empirical approach: Run code and measure execution time and try to minimize it. Move tasks to different processors. $$\\begin{matrix} l_0 = \\sum_{(i)} c_{i,0} x_{i,0}\\\\ l_1 = \\sum_{(i)} c_{i,1} x_{i,1} \\end{matrix}$$ $$min \\{ B \\}$$ $$\\begin{matrix} B \\geq l_0 \\\\ B \\geq l_1 \\end{matrix} $$ Iterative Methods Often used principle: Start with some initial configuration (partitioning) search neighborhood (similar partitions) select a neighbor as candidate evaluate fitness (cost) function of candidate accept candidate using acceptance rule if not, select another neighbor Stop if: Quality is sufficient or no improvement can be found or after a fixed time Ingredients: initial configuration function to find a neighbor as candidate cost function acceptance rule stop criterion Disadvantages: local optimum as best result local optimum depends on initial configuration generally no upper bound on iteration length Kernighan-Lin as long as a better partition is found from all possible pairs of objects: virtually re-group the ‘best' (lowest cost of resulting partition) from the remaining (not yet touched) objects: virtually re-group the ‘best' pair continue until all objects have been re-grouped from these n/2 partitions, take the one with smallest cost and actually perform the corresponding re-group operation Simulated Annealing inspired by physical process replace existing solutions by (random) new feasible solutions form neighborhood always accept better solutions but allow for a guided acceptance for worse neighbors gradually cooling: gradually decrease the probability of accepting worse solutions Advantage: - Allowance for ‘uphill' moves potentially avoids local optima Possible implementation: temp = temp_start cost = c ( P ) while Frozen () == False : while Equilibrium () == False : P_new = RandomMove ( P ) cost_new = c ( P_new ) deltacost = cost_new - cost if Accept ( deltacost , temp ) > random ( 0 , 1 ) P = P_new cost = cost_new temp = DecreaseTemp ( temp ) Functions: RandomMove(P) : Choose a random solution in the neighborhood of P DecreaseTemp() , Frozen() : Cooling down. There are different choices. For example: initially: temp := 1.0 in any iteration: temp := alpha * temp (typically: 0.8 \\(\\leq\\) alpha \\(\\leq\\) 0.99) Frozen after a given time or if there is not further improvement Equilibrium() : Usually after a defined number of iterations Accept(deltacost, temp) : \\(e&#94;{- \\frac{deltacost}{k \\cdot temp}}\\) Complexity: From exponential to constant depending on Equilibrium() , DecreaseTemp() and Frozen() . 5. Multi-Criteria Optimization Part of system synthesis. Exploration of different solutions. Example: Network Processors Network processor: High-performance, programmable device designed to efficiently execute communication workloads Given: Specification of the task structure (task model): tasks to be executed for each flow Different usage scenarios (flow model): sets of flows Sought: Network processor implementation: architecture + task mapping + scheduling Objectives: Maximize performance Minimize cost Subject to: Memory constraint Delay constraint Basic Definition We intend to minimize a vector-valued objective function $$f= (f_1; f_2; ... ; f_n): X \\rightarrow R&#94;n$$ \\(X\\) denotes the decision space . i.e the feasible set of alternatives for the optimization problem The image of the decision space \\(X\\) using the objective function \\(f\\) is denoted as the objective space \\(Z \\subset R&#94;n\\) with $$Z = \\{f(x) |x \\in X \\}$$ A single alternative \\(x \\in X\\) is (sometimes) named solution and the corresponding object value \\(z = f(x) \\in Z\\) is named objective vector decision space → objective space Basic question: How do we define the minimum of a vector-valued function? Pareto Dominance Definition: A solution \\(a \\in X\\) weakly Pareto-dominates a solution \\(b \\in X\\) , denoted as \\(a \\preceq b\\) , if it is at least as good in all objectives, i.e: $$f_i(a) \\leq f_i(b)\\; \\forall 1 \\leq i \\leq n$$ Dominance is transitive A solution is named Pareto-optimal , if it's not Pareto-dominated by any other solution in \\(X\\) Optimization Alternatives Classical single objective optimization methods Simulated annealing Integer linear program other constructive or iterative heuristic methods → Decision making is done before the optimization Population based optimization methods Evolutionary / genetic algorithms → Decision making is done after the optimization Evolutionary Algorithms Basic scheme (many variations exist): A set of initial solution (initial population is chosen (usually at random). This set is called parent set. Solutions form parent set are selected ( mating selection ) Solutions from mating selection are changed using neighborhood operators ( cross-over and mutation operators). The resulting set is called children set. Determine union of the parent and the children sets Solutions of the set from 4. are selected based on their merit to construct the new parent set ( environmental selection ). continue at 2. Cross-over operator: Take best parts of two solutions Mutation operator: same as in simulated annealing (make small variations) How to choose solutions that should be removed from the population Solutions should be ‘close' to the (unknown) Pareto-optimal front (optimality) Solutions should cover large parts of the objective space (diversity) Hypervolume Indicator Environmental selection: Select subset of solutions that maximizes hypervolume indicator Given a set of solutions \\(A \\subseteq X\\) and a set of reference points \\(R \\subset \\mathbb{R}&#94;n\\) . Then the hypervolume indicator \\(I_H(A,R)\\) of \\(A\\) with respect to \\(R\\) is defined as $$I_H(A,R) = \\int_{z \\in H(A,R)} dz$$ where \\(H(A,R)\\) id the dominated space of \\(A\\) regarding \\(R\\) : $$H(A,R) = \\{ z \\in \\mathbb{R}&#94;n | \\exists a \\in A : \\exists r \\in R : (f(a) \\leq z \\leq r) \\}$$ \\(z\\) : Objective The hypervolume indicator leads to diversity and optimality Representation and Neighborhood A representation corresponds to an abstract data that encodes a solution Neighborhood operators work on representations Issues: Each solution has an encoding (completeness) All solutions are represented equally often (uniformity) Each encoding maps to a feasible solution (feasibility) Encoding: i.e order of tasks executing to explore scheduling. Tree Representation Solutions can be represented as tree structures Mutation: Grow subtree Shrink subtree Switch two subtrees Replace subtree Subtrees can be single nodes Handling Constraints Constraint: \\(g(x) \\geq 0\\) Feasible: \\(g \\geq 0\\) Infeasible: \\(g < 0\\) Approaches: Representation is chosen such that decoding always yields a feasible solution Construct initialization and neighborhood operators such that Infeasible solutions are not generated Add only feasible solutions to children population Preferably select feasible solutions in environmental selection Penalty function: Calculate constraint violation \\(g(x)\\) and incorporate it into objective function \\(penalty(x) > 0\\; if g(x) < 0,\\; penalty(x) = 0\\; if g(x) \\geq 0\\) i.e add penalty function to every objective (increase ‘price' for Infeasible solutions) Include the constraints as new objectives 6. System Simulation Estimation with simulation. Estimation: find out if a solution is good or bad. System and Model System: Combination of components that act together Model: Formal description of the system (covers selected information) State Contains all information of the system at \\(t_0\\) that is necessary to determine the output (for \\(t \\geq t_0\\) ) from the input (at (at \\(t \\geq t_0\\) ). Input + State = Output The set \\(X\\) of all possible states of a system is called it's state space Discrete/Continuous Systems Examples: Continuous state systems: Physical processes (usually) Electrical networks Mechanical systems Discrete state systems Finite state machines Queuing systems Computer systems Continuous time systems Physical processes (usually) Electrical circuits Asynchronous systems Discrete time systems Digital clocked system Equidistant sampling (z-transform) Synchronous system models Discrete Event Systems ( DES ) Driven by events Event: \\(e = (v, t)\\) is a touple of a value \\(v\\) and a tag \\(t\\) Tags are usually totally ordered If the tag denotes time , then the event is a timed event If the tag (only) denotes a sequence number , the event is an untimed event DES is an event-driven system Depends entirely on occurrence of discrete events Does not depend on evolution of time Can be defined in continuous of discrete time The state space \\(X\\) can be either discrete or continuous Signals or streams represent ordering/timing of events Processes are represented as functions acting on signals or streams Simulation Discrete-time, time-driven Simulated time is partitioned into equidistant time intervals Length of time intervals determined by The simulated system (i.e clock period) The intended precision (dicretization loss) The simulation effort A simulation step is performed even if nothing happens Event-driven simulation State change only at events Analysis and simulation possible in discrete or continuous time Discrete-Event Modeling and Simulation Concurrent processes are modeled using modules Behavior described as logic/algebraic expressions State described using persistent variables (inside module) Communication between modules through ports, via signals Synchronization through abstract events Modules can be hierarchical Event-Driven Components of Discrete-Event Simulation Simulation time Simulation clock represents the current value of simulation time In discrete-event simulations the clock advances to the next event start-time during simulation Event list Events are processed in order Event list is typically organized as priority queue Event lists may include simulation times when events will happen System modules Model subsystems of simulated system Are called by simulation engine if a relevant event is scheduled Simulation Engine Initialization routine Initialize the simulation model; Set initial states of subsystem modules Fill the event queue with initial events Timing routine Determine next event from the event queue Advance the simulation clock to the time when the event occurs Event routine Update system state when event occurs In a simulation cycle ‘cause' and ‘effect' can share the same time of occurrence Use zero duration virtual time interval called delta cycle ( \\(\\delta\\) ) SystemC System-level modeling language Several levels of abstractions from purely functional to cycle-accurate/pin-accurate Special attention to systems with embedded software C++ library for modeling concurrent systems HW -oriented data types Communication mechanisms Concurrency modeling Event-driven simulation kernel for executing models Free (open source) For Windows, Linux, OS X… Language Architecture Core language Minimal set of modeling constructs structural description concurrency communication synchronization Data types (separated from core language) On top of core language Communication mechanism (signals, FIFO …) Models of Computation (MoCs) Layer (tier) architecture Processes Processes are the basic units of functionality. SC_THREAD s Typically called once, run forever (in a while(true) loop) Can be suspended by calling wait() Keep the state of execution implicitly SC_METHOD s Execute repeatedly from the beginning to end Simulate faster Do not keep the state of execution implicitly Processes must be contained in a module Bun not every member function is a process Modules Building blocks of models Hierarchy Abstraction IP reuse Inter Process Communication Processes can communicate directly trough signals Events : Flexible, low-level synchronization primitive Channel : Container for communication and synchronization can have state (private data), transport data, transport events Implement one or more interfaces Interface : Set of access methods to the channel Interface methods need to be implemented Wait and Notify wait : halt process until an extent is raised wait() with arguments → dynamic sensitivity wait(sc_event) wait(time) wait(timeout, sc_event) notify : rise an event notify() with arguments → delayed notification my_event.notify(); : notify immediately my_event.notify(SC_ZERO_TIME); : notify next delta cycle my_event.notify(time); : notify after ‘time' Levels of Abstractions Simulation can be done on different levels of abstraction. Functional level (untimed/timed) Use: Model (un-)timed functionality Communication: shared variables, messages Typical languages: C/C++, Matlab Transaction level Use: MPSoC architecture analysis Early SW development Timing estimation Communication: method calls to channels Typical languages: SystemC Register transfer level / Pin level Use: HW design and verification Communication: wires and registers Typical languages: Verilog, VHDL 7. Design Space Exploration Example: Network Processor ( NP ) Different processing units: ARM9 , DSP , … Streams: ordered sequence of tasks Optimal Design of Network Processor \\(T\\) : Tasks \\(w\\) : Requests \\(M\\) : Mapping \\(R\\) : Resources \\(c\\) : Cost Mappings: \\(M \\subseteq T \\times R\\) all possible bindings of tasks i.e if \\((t,r) \\in M\\) then task \\(t\\) could be executed on resource \\(r\\) Request \\(w(r,t) \\geq 0\\) i.e execution of one packet in \\(t\\) would use \\(w\\) computing units of \\(r\\) Resource allocation cost \\(c(r) \\geq 0\\) Binding \\(Z\\) of tasks to resources \\(Z \\subseteq M\\) leading to actual implementation Subset of mappings \\(M\\) s.t every task \\(t \\in T\\) is bound to exactly one allocated resource \\(e \\in R alloc(r) = 1\\) and \\(r = bind(t)\\) Design constraints Delay constraints: e.g max time a packet is processed within network processor Throughput maximization: e.g packets per second Cost minimization: implementing with small amount of resources (processing units, memory, networks…) And conflicting usage scenarios … NP Design Space Exploration Allocation: determine HW components Binding: For each SW process choose allocated HW unit Scheduling: For tasks on specific resources choose scheduling policy Performance Estimation Global Picture Metric Time Power Area (integration on silicon) Cost Quality SNR … Method Analytic Simulation Statistic Abstraction Level Low-level: Register Transfer Level, Instruction Set Architecture Intermediary-level: Transaction Level-Model, Operating System, already linked to HW High-level: Functional, High-level language, independent of HW Subsystem to Analyze HW subsystem CPU subsystem Interconnect subsystem Multiprocessor System-on-Chip (MPSoC) Performance Estimation in Design Flow High-level Advantages: Short estimation time (fast) Implementation details not necessary Drawbacks: Limited accuracy e.g no information about timing Low-level Advantages: Higher accuracy Drawbacks: Long estimation time (slow) Many implementation details need to be known Multi-Processor System-on-Chip Simulation should be divided in to independent modules. So they can be exchanged. Composable in terms of: Subsystems Interactions Computation Communication Memory Scheduling Estimation should cover different metrics e.g: Delays Throughput Memory consumption Power Energy Temperature Cost … Communication Bus Network Hierarchical Bus Computation and Memory DRAM SDRAM DSP RISC FPGA muC HW Scheduling and Arbitration TDMA EDF FCFS proportional share fixed priority static Difficulty Why is MPSoC performance estimation difficult? Computation, communication and memory Non-deterministic computation in processing nodes Non-deterministic communication delays Non-deterministic memory access Complex resource interaction via scheduling/arbitration Cyclic timing dependencies Internal data streams interact on computation and communication resources Interaction determines stream characteristics Uncertain environment Different load scenarios Unknown, worst case inputs Making one component faster can make the complete system slower! If a component is replaced with a faster one it could block shared resources (e.g bus) for other components. Abstraction Technology: transistors, layouts Signal: gate, schematic, RTL Transaction: SW , HW systems Tokens: SW tasks, comm. backbones, IPs Simulators: SPICE VHDL SystemC / Instruction Set Simulator … Performance Estimation Methods How to evaluate a system? Measurements Use existing instance of the system to measure performance Simulation Develop a program which implements a model of the system and evaluate performance by running the program Statistics Develop a statistical abstraction of the system and derive statistic performance via analysis or simulation Formal analysis Develop a mathematical abstraction of the system and compute formulas which describe the system performance Static Analysis Models Describe computing, communication and memory resources by algebraic equations Describe system properties by parameters (e.g data rate) Combine relations $$comm\\_delay= \\left \\lceil \\frac{\\# words}{burst\\_size} \\right \\rceil \\cdot comm\\_time$$ Fast and simple estimation But generally inaccurate modeling (e.g resource sharing not modeled) Dynamic Analytic Models Combination between Static models possibly extended by non-determinism in run-time and event processing Dynamic models for describing e.g resource sharing mechanism (scheduling and arbitration) Existing approaches Classical real-time scheduling theory Stochastic queuing theory (statistical bounds) Analytic (non-deterministic) queuing theory (worst-/best-case bounds) Stochastic Queuing Systems A stochastic model for queuing systems is described by probability density functions (distributions) of Arrival rates Service mechanisms Queuing disciplines Performance measures are stochastic values (functions) Average delays in queue Time-average number of customers in queue Proportion of time servers is busy Markovian (exponential) distribution Worst-Case/Best-Case Queuing Systems A worst-/best-case queuing system is described by worst-/best-case bounds on system properties worst-/best-case bounds on arrival times worst-/best-case on server behavior resource interaction Performance measures worst-/best-case delay in queue worst-/best-case number of customers in queue worst-/best-case system delay Simulation Model Program implementing a model of the system (application, hardware, platform, mapping) Performance is evaluated by running the program Simulation Considers HW platform and mapping of application on that platform (virtual platform) Combines functional simulation and performance data Evaluates behavior for one simulation scenario Typically complex set-up and extensive run-times Accurate results and good debugging possibilities Trace-Based Simulation Abstract system-level simulation (without timing) Faster than low-level simulation But still based on a single input trace Abstraction Application: abstract execution traces → Graph of events: read , write and execute Architecture: ‘virtual machines' and ‘virtual channels' → Calibrated with non-functional properties (timing, power, energy) Trace-based simulation steps Build application abstract model → execution trace determined by functional application simulation Extending abstract model with architecture and mapping → Event graph extended by non-functional properties of virtual architecture elements Simulation of extended model 9. Worst Case Execution Time Analysis Calculation not simulation Measurement of running task Simulation would need accurate model Worst case formal analysis Program path analysis Hard Real-Time Systems Often in safety-critical applications Aeronautics Automotive Train industries Manufacturing control Embedded controllers are expected to finish tasks reliably within time bounds Task scheduling must be performed Execution time bounds Worst-Case Execution Time ( WCET ) Best-Case Execution Time ( BCET ) Works if: Worst-case input can be determined or Exhaustive measurement is performed Otherwise: Determine upper bound from execution times of instructions Industry's Best Practice Measurements: determine execution times directly by observing the execution of a simulation on a set of inputs Does not guarantee an upper bound to all executions in general Exhaustive execution in general not possible Too large space: ‘Input Domain' \\(\\times\\) ‘ Initial execution states' Compute upper bounds along the structure of the program Programs are hierarchically structured Statements are nested inside statements So compute upper bound of a statements from the upper bounds of its constituents Calculating Upper Bounds \\(ub(s)\\) : Upper bound for statement \\(s\\) Sequence of Statements $$A \\equiv A1;A2$$ Constituents of \\(A\\) : \\(A1\\) and \\(A2\\) Upper bound for \\(A\\) is the sum of the upper bounds for \\(A1\\) and \\(A2\\) $$ub(A) = ub(A1) + ub(A2)$$ Conditional Statement $$\\begin{align*} A \\equiv \\; & if \\; B \\\\ & then \\; A1 \\\\ & else \\; A2 \\end{align*}$$ Constituents of \\(A\\) : Condition \\(B\\) Statements \\(A1\\) and \\(A2\\) $$ub(A) = ub(b) + \\mathbf{max}(ub(A1), ub(A2))$$ Loops $$\\begin{align*} A \\equiv for\\; & i \\leftarrow 1\\; to\\; 100\\; do \\\\ & A1 \\end{align*}$$ Precondition: Number of loops needs to be known (e.g 100) $$ub(A) = ub(i \\leftarrow) + 100 \\times (ub(i \\leq 100) + ub(A1))+ ub(i\\leq100)$$ Modern Hardware Features Modern processors increase performance by using Caches Pipelines Branch prediction Speculation These features make WCET computation difficult (execution times of instructions vary widely) Best case: everything goes right: no cache miss, operands ready, resources free, branch predicted correctly Worst case: everything goes wrong: all loads miss cache, resources occupied operands not ready Span may be several hundred (even thousand) cycles Program Path Analysis Which sequence of instructions is executed in the worst-case (longest run time)? Problem: The number of possible program paths grow with the length of the program Number of loop iteration must be bounded Must be done at machine code instruction level Basic Blocks A basic block is a sequence of instruction where the control flow enters at the beginning and exits at the end, without stopping in-between or branching (except at the end). The branch instruction at the end of a basic block belongs to that block! t1 : = c - d t2 : = e * t1 t3 : = b * t1 t4 : = t2 + t3 if t4 < 10 goto L Determine basic blocks of a program Determine the first instructions of blocks: The first instruction Targets of jumps (conditional/unconditional) Instructions that follow jumps (conditional/unconditional) Determine the basic blocks There is a basic block for each block beginning The basic block consists of the block beginning and runs until the next block beginning (exclusive) or until the end of the program Calculation of the WCET A program consists of \\(N\\) basic blocks, where each basic block \\(B_i\\) has a worst-case execution time \\(c_i\\) and is executed for exactly \\(x_i\\) times: $$WCET = \\sum_{i=1}&#94;N c_i \\cdot x_i$$ The \\(c_i\\) values are determined using the static analysis Determine \\(x_i\\) Structural constraint given by the program structure Additional constraints provided by the programmer (e.g bounds for loop counters…) This image is taken from the lecture slides provided by Lothar Thiele. Flow Equations (sum of incoming edges equals sum of outgoing edges) $$\\begin{align*} d1 = d2 = x_1 \\\\ d2 + d8 = d3 + d9 = x_2 \\\\ d3 = d4 + d5 = x_3 \\\\ d4 = d6 = x_4 \\\\ d5 = d7 = x_5 \\\\ d6 + d7 = d8 = x_6 \\\\ d9 = d10 = x_7 \\end{align*}$$ Additional Constraints Execute the program only once: \\(d1 = 1\\) loop is executed at most 10 times: \\(x_3 \\leq 10 \\cdot x_1\\) Block 5 is executed at most one time: \\(x_5 <leq 1 \\cdot x_1\\) The more information the better the result Integer Linear Program for WCET Calculation ILP with structural and additional constraints $$WCET = max\\left \\{ \\sum_{i=1}&#94;N c_i \\cdot x_i | \\underbrace{d_1=1}_{\\text{program is executed once}} \\land \\underbrace{\\sum_{j\\in in(B_i)} d_j = \\sum_{k\\in out(B_i)}d_k=x_i, i=1\\ldots N}_{\\text{structural constraints}} \\;\\land \\;\\text{additional constraints} \\right \\}$$ Abstract Interpretation ( AI ) Semantics-based method for static program analysis Basic idea Perform programs computations using value descriptions (or abstract values) Start with the description of all possible inputs AI supports correctness proofs Ingredients: Abstract domain: related to concrete domain (by abstraction and concretization functions) e.g \\(L \\rightarrow \\text{Intervals}\\) where: \\(\\text{Intervals} = LB \\times UB, LB = UB = Int \\cup \\{-\\infty, \\infty\\}\\) instead of: \\(L \\rightarrow Int\\) Abstract transfer functions: for each statement type (abstract versions of their semantics) e.g \\(+ : Intervals \\times Intervals \\rightarrow Intervals\\) where: \\([a,b] + [c,d] = [a+c,b+d]\\) with: \\(+\\) extends to \\(-\\infty, \\infty\\) A join function combining abstract values from different control-flow paths e.g \\(\\cup : Interval \\times Interval \\rightarrow Interval\\) where: \\([a,b] \\cup [c,d] = [min(a,c),max(b,d)]\\) Value Analysis Motivation: - Provide access information to data-cache/pipeline analysis - Detect Infeasible paths - Derive loop bounds Method: Calculate intervals at all program points. Lower and upper bounds for the set of possible values occurring in the program (addresses, register contents, local and global variables). Caches Main memory to slow Speed gap between CPU and memory too large (and increasing) Caches work well in the average case: Programs access data locally (many hits) Programs reuse items (instructions, data) Access patterns are distributed evenly across the cache Caches: fast, small, expensive Memory: relatively slow (bottle neck), large, cheap Access cycles: CPU ↔ Cache: ~1 cycle Cache ↔ Memory: ~100 cycles How caches work: CPU wants to access (read/write) at memory address a Caches: Block m containing a is in the cache ( hit ): Request for a is served in next cycle Block m is not in the cache( miss ): m is transferred from main memory to the cache m may replace some block in the cache Request for a is served as soon as possible (while transfer still continues) Several replacement strategies: LRU : Least Recently Used PLRU : Pseudo- LRU FIFO … Cache Analysis Statically precompute cache contents: - Must Analysis - For each program point (and calling context) find out which blocks *are in the cache* - Determines safe information about cache hits - Predicted cache hit *reduces* **WCET** - May Analysis - For each program point (and calling context) find out which blocks *may be in the cache* - *Complement* says what is **not** in the cache - Determines safe information about cache misses - Each predicted cache miss *increases* **BCET** Must Cache Only variables that are in all caches Order: worst age (oldest) Number of variables in cache \\(\\leq\\) numbers of places in cache Join Intersection + maximal age Interpretation: Memory block a is definitely in the cache Always a hit May Cache All variables that could be in cache Minimal age Join Union Minimal age Interpretation: All block may be in cache None is definitely not in cache Cache Contribution to WCET Reference to variable s in code: if s is in must-cache : \\(t_{WCET} = t_{hit}\\) otherwise \\(t_{WCET} = t_{miss}\\) if s is in may-cache : \\(t_{BCET} = t_{hit}\\) otherwise \\(t_{BCET} = t_{miss}\\) Reference to variable s in loop body (max \\(n\\) iterations): First time miss, then always hit Loop unfolding within loop \\(n \\cdot t_{miss}\\) \\(n \\cdot t_{hit}\\) \\(t_{miss} + (n - 1) \\cdot t_{hit}\\) \\(t_{hit} + (n - 1) \\cdot t_{miss}\\) Pipelines Different stages in parallel Fetch Decode Execute … Pipeline hazards Data hazard: Operands not yet available Resource hazard: Consecutive instructions use same resource Control hazard: Conditional branch Instruction-cache hazard: Instruction fetch causes cache miss CPU as State Machine: Abstract Pipeline for basic blocks 10 Performance Analysis of Distributed Embedded Systems Embedded System = Computation + Communication + Resource Interaction Analysis ↔ Design Analysis: infer system properties from subsystem properties Design: build system from subsystems while meeting requirements Abstract model Concrete instance (real world) Abstract representation (model) Input stream Load model Processor Service model Task(s) running on processor Processing model System view Modular Performance Analysis ( MPA ) Mathematical view Real-Time Calculus ( RTC ) Min-Plus Calculus, Max-Plus Calculus Real-Time Calculus Real-Time Calculus can be regarded as a worst-/best-case variant of classical queuing theory It's a formal method for the analysis of distributed real-time (embedded) systems Algebraic Structure Set of elements \\(S\\) One ore more operators defined on elements of this set Algebraic structures with two operators : \\(\\boxplus\\) , \\(\\boxdot\\) plus-times: \\((S, \\boxplus, \\boxdot) = (R, +, \\times)\\) Our ‘normal' algebra min-plus: \\((S, \\boxplus, \\boxdot) = (R \\cup \\{ + \\infty \\}, inf, +)\\) Used for Real-Time calculus Infimum The infimum of a subset (of some set) is the greatest element (not necessary in the subset) that is less than or equal to all other elements of the subset $$\\begin{matrix}S_1 \\subseteq S \\\\ inf(S_1) = \\underset{i}{max}\\{i \\in S : i \\leq s\\; \\forall s \\in S_1\\}\\end{matrix}$$ Examples: \\(inf\\{[3,4]\\}=3\\) and \\(min\\{[3,4]\\}=3\\) \\(inf\\{(3,4]\\}=3\\) but \\(min\\{(3,4]\\} \\text{not defined}\\) Joint properties of \\(\\boxdot\\) : Closure: \\(a \\boxdot b \\in S\\) Associativity: \\(a \\boxdot (b \\boxdot c) = (a \\boxdot b) \\boxdot c\\) Commutativity \\(a \\boxdot b = b \\boxdot a\\) Existence of identity element: \\(\\exists v: a \\boxdot v = a\\) Existence of negative element: \\(\\exists a&#94;{-1}: a \\boxdot a&#94;{-1} = v\\) Identity element of \\(\\boxplus\\) absorbing for \\(\\boxdot\\) : \\(a\\boxdot \\epsilon = \\epsilon\\) Distributivity of \\(\\boxdot\\) w.r.t. \\(\\boxplus\\) : \\(a \\boxdot (b \\boxplus c) = (a\\boxdot b) \\boxplus (a \\boxdot c)\\) Joint properties of \\(\\boxplus\\) : Closure: \\(a \\boxplus b \\in S\\) Associativity: \\(a \\boxplus (b \\boxplus c) = (a \\boxplus b) \\boxplus c\\) Commutativity: \\(a \\boxplus b = b \\boxplus a\\) Existence of identity element: \\(\\exists \\epsilon : a \\boxplus \\epsilon = a\\) Differences \\(\\boxplus\\) : plus-times : Existence of negative element for \\(\\boxplus\\) : \\(\\exists (-a): a \\boxplus (-a) = \\epsilon\\) min-plus : Idempotency: \\(a \\boxplus a = a\\) Example: plus-times: \\(a \\times (b + c) = a \\times b + a \\times c\\) min-plus: \\(a + inf\\{b,c\\} = inf\\{a+b,a+c\\}\\) System Theories Plus-times system theory: signals, impulse response, convolution, time-domain \\(f(t)\\) : Signals \\(g(t)\\) : Impulse response \\((f*g)\\) : Convolution \\(t\\) : Time domain $$h(t) = \\underbrace{(f*g)}_{\\text{convolution}}\\underbrace{(t)}_{\\text{time-domain}}= \\int_0&#94;t f(t-s)\\cdot g(s) \\;ds$$ Min-plus system theory streams, variability curves, time-interval domain, convolution \\(R(t)\\) : Streams \\(g(\\Delta)\\) : Variability curves \\(\\Delta\\) : Time-interval domain \\((R\\otimes g)\\) : Convolution $$R'(t)\\geq \\underbrace{(R\\otimes g)}_{\\text{convolution}}(t)= \\underset{0\\leq \\lambda\\leq t}{inf}\\{R(t-\\lambda)+g(\\lambda) \\}$$ From Streams to Cumulative Functions Event streams → Arrival Curves ( \\(t \\rightarrow \\Delta\\) ) Event stream: number of events in \\(t=[0 .. T]\\) Arrival Curves \\(\\alpha = [\\alpha', \\alpha&#94;u]\\) : maximum and minimum arriving events in any interval of length \\(t\\) Resources → Service Curves ( \\(t \\rightarrow \\Delta\\) ) Resource Availability: available service in \\(t=[0 .. T]\\) Service Curves \\(\\beta = [\\beta', \\beta&#94;u]\\) : maximum and minimum available service in any interval of length \\(t\\) Arrival and Service Curves work on time intervals as domain ( \\(\\Delta\\) ) Overview of Methods and Algorithms Domain Notes Hierarchical Clustering Partitioning Integer Linear Programming Partitioning Simulated Annealing Partitioning Trace Based Simulation Estimation Evolutionary Algorithms Partitioning Multi Objective Optimization Tom Parks Algorithm Kahn Process Networks Kerninghan Lin Partitioning","tags":"Programming","url":"hardware-software_codesign.html","loc":"hardware-software_codesign.html"},{"title":"System Construction","text":"My Notes for the System Construction Course at ETH Minos on Raspberry PI 2 (Case Study 1) ARM A7 The ARM Architecture is not the same thing as the ARM Processor-Families Documentation There is a lot of good documentation for the ARM processors available. But it's difficult to find the right documents. Document Main Content ARM Architecture Reference Manual (ARMv7- AR , ARM ARM ) Possibility of implementing the processor. For compilers and tools. Partly used for system programming ARM Technical System Reference ( ARM Cortex-A7MPCore Family) Particular Implementation. Some redundant information to ARM ARM . System On Chip Implementation Manual ( BCM 2835) How the core is embedded on the chip with peripherals. Address map. Peripheral information. 6 Kinds Of Instructions Data Processing Branch Instructions Status Register Transfer Load and Store ( RISC ) Generic Coprocessor Instructions Exception Generation Load-/Store: No memory operands (not as x86) Load: from memory to register Store: from register to memory Multiple-Data-Transfer commands ( stmdb sp!,{fp,lr} , ! : Write-Back) Link Register: bl : Branch-and-Link (stores PC in link register) PC -Relative Addressing: Loading large constants (that have no space in instruction encoding) form code Execution Modes 7 Modes for exception handling Processor starts in supervisor mode Registers are shadowed (banked) in different modes System Mode: privileged but same registers as user mode Privilege Level Mode Description/Cause Exception/Normal Execution privileged Supervisor Reset / Software Interrupt exception privileged FIQ Fast Interrupt exception privileged IRQ Normal Interrupt exception privileged Abort Memory Access Violation exception privileged Undef Undefined Instruction exception privileged System Privileged Mode with same registers as in User Mode normal execution un privileged User Regular Application Mode normal execution Special Registers R15: PC (Program Counter) R14: LR (Return address for procedure) R13: SP (‘top' of the stack) R12: FP (by convention, stored SP before procedure call) CPSR (Processor Status Register) Mode Bits IRQ Disable Condition Flags … Typical Procedure Call Caller Pushes params BL #address : Stores PC of next instruction in LR Callee Save LR and FP on stack: stmdb sp!, {fp, lr} Set new FP : mov fp, sp Execute procedure content Reset stack pointer: mov lr, sp Restore FP and jump back to caller address: ldmia sp!, {fp, pc} Caller Clean up parameters from stack: add sp,sp, #n Exceptions Interrupt: asynchronous event triggered by a device signal Trap / Syscall: intentional exception Fault: error condition that a handler might be able to correct Abort: error condition that cannot be corrected Misc FIRQ is about having more stacked registers Return Link type: Different sizes because of pipe-line Raspberry PI 2 Quad Core 1 GB RAM 40 Pin GPIO , … UART , SPI , USB , Ethernet, … Powered from MicroUSB Booting Kernel is copied to address: 0x8000h and branches there (instead to 0x00 ) MMU is disabled Only one core is running MMU Memory translation is complicated due to two different MMU 's ARM 's memory mapped registers start from 0x3f000000 , opposed to 0x7e000000 in BMC manual GPIO Some GPIO Registers need Read-Modify-Write (i.e GPFSELn) Other Registers support setting or clearing single bits: i.e Set GPIO Pin: GPSETn i.e Clear GPIO Pin: GPCLRn Minos and Oberon History of Oberon Oberon Modules can be compiled separately Strongly typed Static type checking at compile time Run-time support for type guards / tests High Level (minimal Assembler code) Special low level functions in SYSTEM pseudo module (compiler directives) Oberon07 Minimal No type OBJECT* no member functions (methods) Language Constructs Program Units MODULE PROCEDURE Value, VAR (in-out) and CONST parameters Data Types BOOLEAN , CHAR , SHORTINT , INTEGER , LONGINT , HUGEINT , REAL , LONGREAL , SET , ADDRESS , SIZE Structured Types ARRAY , POINTER TO ARRAY RECORD (with type extension), POINTER TO RECORD Control Structures IF IF a = 0 THEN (* statement sequence *) END ; WHILE WHILE x < n DO (* statement sequence *) END ; REPEAT REPEAT (* statement sequence *) UNTIL x = n ; FOR FOR i := 0 TO 100 DO (* statement seq *) END ; Built-in Functions Increment and decrement INC ( x ); DEC ( x ); INC ( x , n ); DEC ( x , n ); Sets INCL ( set , element ); EXCL ( set , element ); Assert and Halt ASSERT ( b < 0 ); HALT ( 100 ); Allocation NEW ( x , ...); Shifts ASH ( x , y ); LSH ( x , y ); ROT ( x , y ); Conversion SHORT ( x ); LONG ( x ); ORD ( ch ); CHR ( i ); ENTIER ( r ); Arrays LEN ( x ); LEN ( x , y ); DIM ( t ); Misc ABS ( x ); MAX ( type ); MIN ( type ); ODD ( i ); CAP ( c ); Addresses and Sizes ADDRESS OF x ; ADDRESSOF ( x ); SIZE OF t ; SIZEOF ( t ); Modules PROCEDURE Write* ... : Exported ( * ) Module body is executed first and only once Module can be loaded only once (keeps state, can be unloaded) Procedures without params can be executed as commands Comments: (* ... *) Special Type: SET contains ints up to CPU bit size: i.e 1..32 ( {12, 5} ) SYSTEM Pseudo Module Unsafe! Memory Access SYSTEM.BYTE : raw binary data (with length and bound checks) Type Casts: can be unsafe! Procedure Flags: INTERRUPT : for ISR 's PCOFFSET=k : offset for returning from ISR Unsafe Pointer: can write to memory address Pseudo Module SYSTEM Direct Memory Access Functions SYSTEM . PUT ( a , x ); SYSTEM . GET ( a , x ); SYSTEM . PUT8 | 16 | 32 | 64 ( a , x ); x := SYSTEM . GET8 | 16 | 32 | 64 ( a ); SYSTEM . MOVE ( src , dest , length ); Data Type SYSTEM . BYTE Type Cast b := SYSTEM . VAL ( a , t ); Example of Low-Level access IMPORT SYSTEM ; PROCEDURE LetThereBeLight ; CONST GPSET0 = 03F20001CH ; BEGIN SYSTEM . PUT ( GPSET0 , { 21 }); END LetThereBeLight ; SYSTEM : ARM Specific Register Access SYSTEM . SP (); SYSTEM . FP (); SYSTEM . LNK (); SYSTEM . SETSP ( x ); SYSTEM . SETFP ( x ); SYSTEM . SETLR ( x ); SYSTEM . LDPSR ( b , x ); SYSTEM . STPSR ( b , x ); SYSTEM . LDCPR ( a , b , c ); SYSTEM . STCPR ( a , b , c ); SYSTEM . FLUSH ( x ); Floating Point SYSTEM . NULL ( x ); SYSTEM . MULD ( a , b , c ); Interrupt Procedures PROCEDURE Handler { INTERRUPT , PCOFFSET = k }; BEGIN (* k is the offset to the next instruction cf. table of exceptions *) END Handler ; Special Flags and Features Procedure without activation frame PROCEDURE { NOTAG } Procedure that is linked to the beginning of a kernel PROCEDURE { INITIAL } Inline assembler block CODE ... END Alignment of a symbol (i. e variable) symbol { ALIGNED ( 32 )} Pinning of a symbol symbol { FIXED ( 0x8000 )) Unsafe pointer that is assignment compatible with type ADDRESS POINTER { UNSAFE } TO ... Symbol that is invisible to a Garbage Collector symbol { UNTRACED } Type Declarations TYPE Device * = POINTER TO DeviceDesc ; (* Pointer to record (reference type)*) DeviceDesc * = RECORD (* Record: Value type *) id * : INTEGER ; Open * : PROCEDURE ( dev : Device ); Close * : PROCEDURE ( dev : Device ); next * : Device ; END ; (* Procedure type with signature *) TrapHandler * = PROCEDURE ( type , addr , fp : INTEGER ; VAR res : INTEGER ); NumberType * = REAL ; (* Type alias *) DeviceName * ARRAY DeviceNameLength OF CHAR ; (* Array Type *) Data * POINTER TO ARRAY OF CHAR ; (* Dynamic array type *) Inheritance Task * = POINTER TO TaskDesc ; TaskDesc * = RECORD task task task task task proc : PROCEDURE ( me : Task ); (* This procedure is executed in the task *) next : Task ; (* The next task in the list of tasks *) END ; PeriodicTask * = POINTER TO PeriodicTaskDesc ; PeriodicTaskDesc * = RECORD ( TaskDesc ) (* inherits TaskDesc *) priority : LONGINT ; (* The priority determines the execution order *) interval : LONGINT ; (* The task is executed every \"interval\" msecs *) END ; Type Test IF task IS PeriodicTask THEN ... END ; Type Guard (cast) IF task ( PeriodicTask ). priority = 1 THEN ... END ; Type Test and Guard WITH task : PeriodicTask DO ... END ; Run-time Support for Inheritance RTTI is supported Each type gets an unique type descriptor ( LONGINT ) The variables contain an array of up to 3 tags The tags point to the type descriptors Inheritance level is restricted to 3 Module Loading and Commands Modules are loaded on demand (first use) Statically linked modules are loaded at boot-time Exported Procedures without parameters act as commands Modification of modules needs reloading Unloading possible, if no other loaded module (static or dynamic) depends on it If a command of a not loaded module is executed the module is loaded first Compilation and Linking A module (.mod) is compiled to an executable file / object file (.arm) and a symbol file (.smb) The executable file contains a fingerprint The linker adds fingerprint to dependent object files (fix-up) Object File Format Compiler flags: Compiler.Compile -b=ARM --objectFile=Minos The object file is very compact Key: Fingerprint Fix-ups → Fix-up-root (relocation table) linked list to fix-ups Downsides: Module file is limited Not very maintainable This image is taken from the lecture slides provided by Felix Friedrich Boot-file Linked Modules of Kernel files (hierarchy) Predefined loading address and entry point (0x8000 for RPI2 ) Boot-Linking command in host system MinosLinker.Link minimalinit.img 108000H kernel.img OFSRamVolumes SerialLog Minos ~ Image header: minimalinit.img Start address: 108000H Image file name: kernel.img Object file names (compiled): OFSRamVolumes SerialLog Minos ~ Minos Kernel System Start-up Firmware ( RPI2 , GPU ) Initialise HW Copy boot image to RAM Jump to boot image (initializer) Initializer Set stack registers for all processor modes Setup free heap and module list Initialise MMU and page table Setup interrupt handler and vectors Start timer and enable interrupts Initializer UARTs, RAM disks, … Enter scheduling loop on OS The Minos Kernel is modular: Minos: Command interpreter and scheduler Modules: Module loader, dynamic linker File System: RamVolumes, OFS , … Kernel: Memory management, device drivers I/O: Kernel logging, … Run-time: Memory allocation (heaps), FPU emulation, … … Initialisation of Interrupts Set handlers of IRQ 's Write handlers for needed interrupts Put jumps to handlers into interrupt table Enable IRQ 's Usually Interrupts need to be configured in 3 parts: CPU (enable, masking…) Interrupt Controller Device The RPI2 has 3 memory-mapped IRQ registers (32-bit): Pending Bits indicate which interrupts are pending Need to be checked in the IRQ Trap Handler (1st level handler) Call IRQ Handler (2nd level) depending on pending bits Task Scheduling (Minos) 3 Task types: High Priority (every 5 ms) Low Priority (every 20 ms) Background tasks Preemption: High priority tasks preempt low priority and background tasks Low priority tasks preempt only low priority tasks Background tasks don't preempt any tasks Task Descriptors for Synchronous (periodic) tasks Asynchronous (background) tasks Stack (Minos) One stack for all processes Every task needs to run to completion Preemption is possible Preempting task needs to be completed before returning Scheduler (Minos) Recursive interrupt procedure Prolog: Interrupts masked Scheduling: Interrupts allowed Epilogue: Interrupts masked Scheduler procedure must be reentrant Register values on stack Private variables Assumptions for scheduler Linked list stores tasks sorted by period and priority Tasks run to completion within given period Serial Communication Directionally Simplex Half duplex Full duplex Synchrony Synchronous Asynchronous Examples: RS -232 RS -458 SPI ( SSP , Microwire) I \\(&#94;2\\) C 1-Wire USB SPI Very simple 4 Wires: SCLK : Serial bit-rate Clock MOSI : Master data Output, Slave data Input MISO : Master data Input, Slave data Output SS : Slave Select Configurations: Single slave mode: 1 Master, 1 Slave multiple slave mode: 1 Master, N Slaves Synchronous bidirectional data transfer Data transfer initiated by master Bandwidth some KBits/s up to several MBits/s Simple implementation in software possible (bit-banging) Master and Slave have shift registers for data (in and output) During communication data ‘circles around' between the two shift registers Communication Master pulls SS to low Master pushed data out of shift register Slave pushes data out of shift register at the same time Sampling happens at SCLK To finish communication master stops SCLK No acknowledgment mechanism No device interrupts Setup Polarity ( SCLK ): 0: Sampling happens on rising SCLK edge 1: Sampling happens on falling SCLK edge Phase ( SCLK ): 0: Rising SCLK edge in the middle of data 1: Rising SCLK edge on beginning of data UART Serial transmission (least significant bit first) Configurable Number of data bits: 5, 6, 7, 8 Parity: odd, even, none Stop-bits: 1, 1.5, 2 Transfer rate (bits per second): 75, 110, 300, …, 115200 Flow control exists in some implementations A2 (Case Study 2) Intel x86 Shared Memory (for all processors) Symmetrical Multiple Processors ( SMP ) Resources (x86 compatible HW ) osdev.org SDM : Intel® 64 and IA -32 Architectures Software Developer's Manual (4000 p., 3 volumes) Architecture Instruction Set Reference System Programming Guide MP Spec: Intel Multiprocessor Specification, version 1.4 (100 p.) ACPI Spec: Advanced Configuration and Power Interface Specification (1000 p.) PCI Spec: PCI Local Bus Specification Rev. 2.2 (322 p.) Interrupt System (x86) External IRQ 's (asynchronous) I/O Devices Timer Inter-processor interrupts Software IRQ 's (synchronous) Traps/Syscall: special instructions Processor exceptions (synchronous) Faults (restartable): i.e page fault Aborts (Fatal): i.e machine check APIC Each CPU has local APIC (local interrupts) I/O APIC for all CPU 's (external interrupts) Messages to processors Start Processor: Activation and Initialisation of individual processors Halt Processor: Deactivation of individual processors Halt Process, schedule new process: Interrupt in order to transfer control to scheduler Local timers Periodical interrupts MultiProcessor Specification Standard by Intel ( MP Spec 1.4) Hardware Specification Memory Map APIC Interrupt Modes MP Configuration Table Processor, Bus, I/O APIC Table address searched via \"floating pointer structure\" Exception Numbers Vector # Description Source 0 Div error div / idiv instruction 1 Debug Code / data reference 2 NMI Non maskable external IRQ 3 Breakpoint int 3 instruction 4 – 19 Other processor exceptions E.g. page fault etc. 20-31 reserved 32-255 Maskable Interrupts External Interrupts from INTR pin INT n instruction Configuring APIC Local Vector Table (for local interrupt sources) Vector Number, Trigger Mode, Status, Interrupt Vector Mask Timer Mode (one shot / periodic) Command Register: Inter Processor Interrupt with vector number, delivery mode: fixed, nmi, init, startup (..) logical / physical destination (including self and broadcasts with / without self PCI Local Bus Connect Peripheral Components Standardised Configuration Address Space for all PCI Devices Interrupt Routing Configuration Access mechanism PCI BIOS : offers functionality such as \"find device by classcode\". Presence determined by floating data structure in BIOS ROM Addressable via in / out instructions operating on separate I/O memory address space PCI Express now Memory Mapped I/O Active Oberon Language Locks vs. Monitors Lock based shared data protected from concurrent access by locks Monitor based locks code (methods) that access shared data No direct locking of data structures needed Threads vs. Active Objects Threads Concurrently running code Active Objects Objects that contain threads Threads in form of Monitors Object Model (Active Oberon) EXCLUSIVE : Protection (mutual exclusion) Methods tagged EXCLUSIVE run under mutual exclusion As synchronized in Java AWAIT : Synchronisation Wait until condition of AWAIT becomes true ACTIVE : Parallelism Body marked ACTIVE executed as thread for each instance Signal-Wait Implementations Signal-and-Continue Java uses this Race conditions can occur Signal-and-Exit Active Oberon uses this Can be achieved in Java by looping on wait condition Active Oberon: Semaphore = OBJECT number := 1 : LONGINT ; PROCEDURE enter ; BEGIN { EXCLUSIVE } AWAIT number > 0 ; DEC ( number ) END enter ; PROCEDURE exit ; BEGIN { EXCLUSIVE } INC ( number ) END exit ; END Semaphore ; Equivalent Java code: class Semaphore { int number = 1 ; synchronized void enter () { while ( number <= 0 ) // while needed! try { wait ();} catch ( InterruptedException e ) { }; number -- ; } synchronized void exit () { number ++ ; if ( number > 0 ) notify (); /* notifyAll() needed if different threads evaluate different conditions */ } } Monitors in Active Oberon No notify() (or notifyAll() ): Every EXCLUSIVE procedure triggers reevaluation of AWAIT when it returns Downsides of Monitors: Wasting of processor time on looping on ‘locks' Ordering can not be influenced Active Object System (A2) Modular Kernel Structure Cover: Kernel Activity Scheduler: Objects Module Loader (Memory Management): Modules, Heaps Hardware Abstraction: Machine Atomic Operations ( HW Support) The supported operations are typically a lot slower than simple read and write operations. Intel (x86) From AMD64 Architecture Programmer's Manual: CMPXCHG mem, reg \" compares the value in Register A with the value in a memory location If the two values are equal, the instruction copies the value in the second operand to the first operand and sets the ZF flag in the flag registers to 1. Otherwise it copies the value in the first operand to A register and clears ZF flag to 0\" Lock Prefix \" The lock prefix causes certain kinds of memory read-modify- write instructions to occur atomically\" ARM From ARM Architecture Reference Manual: LDREX <rd>, <rn> \" Loads a register from memory and if the address has the shared memory attribute, mark the physical address as exclusive access for the executing processor in a shared monitor\" STREX <rd>, <rm>, <rn> \" performs a conditional store to memory. The store only occurs if the executing processor has exclusive access to the memory addressed\" Overview Some typical instructions for atomic operations and implementation examples. Test-And-Set ( TAS ) TSL register, flag (Motorola 68000) Compare-And-Swap ( CAS ) LOCK CMPXCHG (Intel x86) CASA (Sparc) Load Linked / Store Conditional LDREX / STREX ( ARM ) LL / SC ( MIPS ) These hardware instructions are often much slower than simple read and write. Caches can't be exploited (direct access to memory)! Compare-And-Swap is the most universal instruction Compare-And-Swap ( CAS ) Atomic operation implemented in processor. Compares memory location with an value. If it's same a new (given) value is written at the memory address. Returns the previous value at memory position in any case. int CAS ( int * a , int old , int new ) If value old is at memory location of a : safe new at a Return previous value at a in any case Implementation of a spinal using CAS (* Initialisation *) Init ( lock ) lock = 0 ; (* Acquire Lock *) Acquire ( var lock : word ) repeat res := CAS ( lock , 0 , 1 ); until res = 0 ; (* Release Lock *) Release ( var lock : word ) CAS ( lock , 1 , 0 ); (* atomicy not needed but visibility/ordering *) Boot Procedure (A2) Start BIOS Firmware Load A2 Boot-file Initialise modules Module Machine Module Heaps … Module Objects Setup scheduler and self process Module Kernel Start all processors … Module Boot-console read configuration and execute boot commands This all happens on the Boot Processor ( BP ) Processor Startup Start processor P (Boot-processor) Setup boot program ( Machine.InitProcessors ) Enter processor IDs into table ( Machine.InitBootPage ) Send startup message to P via APIC ( Machine.ParseMPConfig ) Wait with timeout on started flag by P ( Machine.StartProcessor ) Boot program (For each processor) Set 32-bit run-time environment ( Machine.EnterMP ) Initializer control registers, memory management, interrupt handling, APIC Set started flag ( Machine.StartMP ) Setup Scheduler ( Objects.Start ) Boot-processor proceeds with boot console A2 Activities States Ready: ready to be scheduled Running: currently scheduled Waiting Condition ( AWAIT ): waiting until condition is met Lock ( EXCLUSIVE ): waiting to enter monitor Terminated: Activity finished executing Java doesn't make difference between waiting on a condition or a lock. Run-Time Data Structures Running Array/List One entry for each processor Index: id of processor Object header (for each object) List of conditions (for each condition) List of locks (for each monitor) Ready Queues/Heap Idle Low Medium High Garbage Collector ( GC ) Real-time ( RT ) Interrupt Array (first level IRQ 's) Index: IRQ number Stack Management Virtual addressing Allocation in page units Page fault for detecting stack overflow Deallocate process stack via garbage collector (in process finalizer) Life cycle: CreateProcess: Allocate first frame Page fault: Allocate another frame Finalize: Deallocate all frames Active Oberon has one shared memory for all processes! One Heap for all processes (no heavy-weight processes) Each process has own stack All processes share same address space 8‘000 processes can be allocated 4 GB address space 4 kB pages 1024 pages per process ↳ ~ 8‘000 processes Context Switch Synchronous Explicit A process terminates Yield Implicit Mutual exclusion AWAIT Asynchronous Preemption Priority handling Time-slicing Coroutines (Synchronous) Synchronous context switch Context switch: Replace SP and FP of old process with SP and FP of new process PC needs also to be adjusted Stack can be used to identify process Asynchronous Context Switch Needs to save much more than for a synchronous context switch Save all registers (copy state) Synchronisation Object Locking Object descriptors (added by system): Object with mutual exclusion contain additional fields headerLock: BOOLEAN; lockedBy: Process; awaitingLock: ProcessQueue; awaitingCondition: ProcessQueue; Locking (Procedure Lock ) Try to acquire object when we have it, we can change the data structure if we don't have it (we need to give up control) Synchronous switching to an other process Select and SwitchTo Unlocking (Procedure Unlock ) When giving up a lock all conditions need to be evaluated again (signal-and-exit) Otherwise the opposite of locking Wait Conditions ( AWAIT ) Internally boxed into a procedure ( PROCEDURE $Condition(fp: ADDRESS): BOOLEAN; ) Stack frame is needed in condition: FP AWAIT code: put condition on await queue Side-effects in AWAIT conditions are dangerous! Priority Inversion Example: 3 processes, 1 shared resource P1: low P2: high P3: medium R: shared resource P1 locks R P2 tries to lock R (needs to wait) P3 preempts P1 P1 can't release R P2 can't continue until P1 releases R This image is taken from the lecture slides provided by Felix Friedrich Priority Inheritance One possibility to cope with priority inversion The priority of each process holding a lock to a resource is increased to the highest priority of all waiting processes (for the lock) Need to walk the graph of locks and processes Lock-Free Programming Problems with Locks Deadlocks Livelocks Starvation Different Goals Parallelism Progress Guarantees Reentrancy Granularity Fault Tolerance Definition of Lock-Freedom At least one algorithm (process, thread) makes progress, even if others run concurrently, fail or get suspended. Starvation can still happen Definition of Wait-Freedom Each algorithm makes eventually progress. Implies freedom from starvation Progress Conditions: Blocking Non-Blocking Someone make progress Deadlock-free Lock-free Everyone makes progress Starvation-free Wait-free Lock-free programming basically makes loop around CAS Overflows (i.e INTEGER ) is critical A2 Goals Lock Freedom Progress Guarantees Reentrant Algorithms Portability Hardware Independence Simplicity, Maintenance Guiding Principles Use implicit cooperative multitasking no virtual memory possible optmizations are limited Co-design of OS , Compiler and Programming Language Lock-Free Kernel Needs lock-free queue Compare-And-Swap ( CAS ) is implemented wait-free in hardware Memory Model for Lock-Free Active Oberon Data shared between two or more activities at the same time has to be either protected by EXCLUSIVE blocks or read or modified using the compare-and-swap operation Changed shared data is visible to other activities after leaving an EXCLUSIVE block or executing a compare-and-swap operation CAS is an operation in the programming language Declaration: PROCEDURE CAS ( variable , old , new : BaseType ): BaseType Performance of CAS On the HW level CAS triggers a memory barrier Performance suffers with increasing number of contenders (contention) ABA Problem The ABA Problem occurs when one thread fails to recognise that a memory location was modified temporarily by another thread and therefore erroneously assumes that the overall state has not been changed. This image is taken from the lecture slides provided by Felix Friedrich The ABA Problem makes it difficult to reuse nodes in a stack structure Possible to allocate always new memory, but it's expensive Solution: hazard pointers Hazard Pointers ABA Problem because of reuse of pointers Pointer P that has been read by one thread X but not yet written by same thread Pointer P is written by other thread Y between reading and writing of first thread X [Hazard pointers](https://en.wikipedia.org/wiki/Hazard_pointer: Each lock-free data structure has an array (hazard array) of the size of number of threads (n=number of threads) Before X reads P , it marks it hazardous in the hazard array of data structure (e.g. the stack) When finished (after the CAS ), process X removes P from the hazard array Before a process Y tries to reuse P , it checks all entries of the hazard array Hazard pointers don't solve problem when several pointers need to be changed at same time i.e Enqueue/Dequeue Solution: use sentinel Notion of helping other threads Employ Hazard pointers Cooperative Multitasking (implicit) Compiler automatically inserts code for cooperative multitasking (implicit) Each process has a quantum At regular intervals, the compiler inserts code to decrease the quantum and calls the scheduler if necessary sub [ rcx + 88 ], 10 ; decrement quantum by 10 jge skip ; check if it is negative (jump if greater) call Switch ; perform task switch skip: ; ... Uncooperative block ( UNCOOPERATIVE ): Guarantee that no scheduling happens Not like a lock different processors can execute the code in parallel like disabling interrupts Cons Small overhead of inserted code Sacrifice register ( rcx ) Guarantees Max number of parallel execution of uncooperative code is number of processors Hazard pointer can be associated with processor (instead of processes) Search time constant: thread-local storage → processor local storage Interrupts Interrupt handlers are modeled as virtual processors M = # of physical processors + # of potentially concurrent interrupts Queue Data Structures This image is taken from the lecture slides provided by Felix Friedrich Hazard Pointers: in use (writing) Pool: Reuse nodes that are not used anymore Lock-free but not wait-free (starvation possible) if not possible to en-/dequeue: help other threads (processors) Scheduling (Activities) TYPE Activity * = OBJECT { DISPOSABLE } ( Queues . Item ) (* Queues.Item accessed via activity register *) VAR (* access to current processor *) (* stack management *) (* quantum and scheduling *) (* active object *) END Activity ; Task Switch Finalizer Finest granular protection makes races possible that did not occur previously: Need to pass information to the new thread. current := GetCurrentTask () next := Dequeue ( readyqueue ) Enqueue ( current , readyqueue ) (* Here an other thread can dequeue and run (on the stack of) the currently executing thread! *) SwitchTo ( next ) When switching to new thread Enqueue runs on new thread Call finalizer of previous thread Solution with finalizer: SwitchTo ( nextActivity , Enqueue , (* Enqueue runs on new thread *) ADDRESS OF readyQueue [ currentActivity . priority ]); FinalizeSwitch ; (* Calls finalizer of previous thread *) Stack Management Stacks organized as Heap Blocks Stack check instrumented at beginning of procedure Stack expansion is possible Possibilities to expand stack: Allocate more memory for stack, copy old stack to beginning of new (pointers to stack need to be updated VAR parameters) Manage stack in a linked list, link to new portion of stack from the old one: ReturnToStackSegment function needed to go back to previous stack segment Interrupts First Level IRQ by non-portable CPU module Second level IRQ handling with activities Wait for interrupt: Interrupts . Await ( interrupt ); First level IRQ code affecting scheduler queues runs on a virtual processor PROCEDURE Handle ( index : SIZE ); BEGIN { UNCOOPERATIVE , UNCHECKED } IF previousHandlers [ index ] # NIL THEN previousHandlers [ index ] ( index ) END ; Activities . CallVirtual ( NotifyNext , ADDRESS OF awaitingQueues [ index ], processors [ index ]); END Handle ; Very powerful to write IRQ handlers in to levels Possible with cooperative multitasking Lock-Free Memory Management Allocation/Deallocation by lock-free algorithms Buddy system: old approach but simple when returning blocks and merging them in free memory Mark-and-sweep Traverse heap and mark used blocks Remove all unmarked blocks Multiple garbage collectors can run in parallel Precise: doesn't delete used blocks by accident (can happen in GC with heuristics) Optional Incremental: Possible to run on a subset all blocks (on different parts of heap) Concurrent: GC can run in concurrency of a mutating thread Parallel: Several instances of the GC can run at once Data Structures: Global Per Object Mark Bit Cycle Count Cycle Count Marklist Marked First Next Marked Watchlist Watched First Next Watched Root Set Global References Local Refcount Cycle Count: used to mark visited objects Mark List: all objects that were marked previously Watch List: all candidates that could be garbage collected Root Set: where to start to find all reachable object Portability Lock-free A2 kernel written exclusively in a high-level language No timer interrupt required (cooperative multitasking): scheduler hardware independent No virtual memory no separate address spaces everything runs in user mode, all the time Hardware-dependent functions ( CAS ) are pushed into the language Almost completely portable: Some minimal stub written in assembly code to initialize memory mappings and initialize all processors Oberon RISC (Case Study 3) Project Oberon RISC architecture Oberon OS Motivation: Build system from scratch Understanding a big amount of details Commercial systems are far from perfect Need good tools for programming Competence for building from Scratch: HW , application, how it really works ‘ lean systems' approach Build from scratch reduce complexity: no ‘baggage' choices of different implementation design based only on problem domain and experience less surprises! flexible solutions more than customer needs competitive advantages Why not build from scratch re-inventing the wheel fundamental knowledge required more actual work (the first time) restricted component choices not for short-term! team work: communication! Configurable Hardware PAL 's / GAL 's / CPLD 's LUT and interconnect Current: FPGA loadable configuration (not like ASIC / VLSI ) Introduction to HDL : Simulation / Synthesis Verilog / VHDL Developed at ETH : Lola, ActiveCells Very different from Programming Languages Things happen in parallel (not serial)! Very difficult for high frequencies RISC architecture (overview) Documentation Developed by N. Wirth Used for book \"Compiler Construction\" by N. Wirth RISC vs. CISC Harward vs. Von Neumann Registers vs. Stack Machine FPU ALU Shifter (Barrel-Shifter) Implemented in Verilog 3 kinds of instructions - arithmetic/logic - load/store - floating-point RISC processor executes SW RISC0 : Memory addressed as words RISC5 : Memory can be addressed as bytes Adds external static RAM FPU SPI , PS /2, GPIO , RS -233, 1 ms timer Top of stack: -64 Flags: Not equal Zero Carry/Borrow Overflow Co-design: OS / HW /Compiler Minimalistic Best practices from CS Architecture/Features Fast file system, module loader, garbage collector High-level, type-save language files Module.Command[params] execution from any visual text self hosted, small (~200kB) User Interface Mouse oriented, three buttons, \"interclicking\" Oberon Core System Inner/Outer core File system: FileDir, File Networking Outer core: input, viewers, fonts, text, … Compiler: recursive descent, single pass RS -232, SCC , Net no memory protection possible to draw over other windows no auto save Boot loader loads boot file (inner core) form SD -card (or serial port) Simple to build own tools build on Oberon System RISC emulator available Active Cells (Case Study 4) Custom design of Multi-Processor System Cores Caches Bus Memory Each process owns it's core Scheduling unnecessary Caches unnecessary some restrictions / some improvements Tiny Register Machines ( TRM ) TRM interconnects SW / HW Co-design Active Cells Tool-chain Multi-core Systems Challenges Cache coherence Shared memory communication bottleneck Thread synchronization overhead Difficult to predict behaviour and timing Hard to scale Partitioned Global Address Space Operating System Challenges Processor Time Sharing Interrupts Context Switches Thread Synchronisation Memory Sharing Inter-process: Paging Intra-process, Inter-Thread: Monitors Project Supercomputer in the Pocket Focus: Streaming Applications (i.e ECG ) Stream Parallelism (pipelining) Task parallelism / data parallelism On-chip distributed system Replace shared memory by local memory Message passing for interaction between processes Separate processor for each process Very simple processors No scheduling, no interrupts Application-aware processors Minimal operating system Conceptually no memory bottleneck Higher reliability and predictability by design Tiny Register Machine ( TRM ) Extremely simple processor on FPGA Hardware architecture Two-stage pipeline Each TRM contains ALU and shifter 32-bit operands and results stored in a bank of 2 * 8 registers Local data memory: d * 512 words of 32 bits Local program memory: i * 1024 instructions with 32 bits 7 general purpose registers Register H for storing the high 32 bits of a product 4 conditional registers: C , N , V , Z no chaches TRM Machine Language Machine language: binary representation of instructions 18-bit instructions Three instruction types: Type a: arithmetical and logical operations Type b: load and store instructions Type c: branch instructions (for jumping) Instruction Encoding: Offset and Immediate values have restricted width Long jumps constructed from chains Immediate store/load with shifts Variants of TRMs FTRM : includes FPU VTRM : includes vector processing unit 8 x 8-words registers available with/without FPU TRM with SW configurable instruction width First Experiment: TRM12 Message passing architecture Bus based on chip interconnect Not scalable Second Experiment: Ring of 12 TRMs TRM ↔ Adapter ↔ Ring Not scalable Large delays Active Cells Computing model, programming language Compiler, synthesizer, hardware library, simulator Programmable HW ( FPGA ) One tool-chain for SW and HW Consequences No global memory No processor sharing No peculiarities of specific processor No predefined topology (NoC) No interrupts No operation system Computation units: Cells Different parallelism levels: Communication Structure (Pipelining, Parallel Execution) Cell Capabilities (Vector Computing, Simultaneous Execution) Inspired by: Kahn Process Networks, Dataflow Programming, CSP Active Cell Component Active Cell Object with private state space Integrated control thread(s) Connected via channels Cell Net Network of communication cells Active Cells Scope and environment for a running isolated process Cells do not share memory Defined as types with port parameters Example: TYPE Adder = cell ( in1 , in2 : port in ; result : port out ); (* communication ports *) VAR summand1 , summand2 : integer ; BEGIN in1 ? summand1 ; (* blocking receive *) in2 ? summand2 ; result ! summand1 + summand2 ; (* non-blocking send *) END Adder ; Cell Constructors to parameterize cells during allocation time: TYPE Filter = cell ( in : port in ; result : port out ); VAR ...; filterLength : integer ; PROCEDURE & Init ( filterLength : integer ) (* constructor *) BEGIN self . filterLength := filterLength END Init ; BEGIN (* ... filter action ... *) END Filter ; VAR filter : Filter ; BEGIN .... new ( filter , 32 ); (* initialization parameter filterlength = 32 *) Cells can be parametrized with capabilities or non-default values: TYPE (* Cell is a VectorTRM with 2k of Data Memory and has access to DDR2 memory *) Filter = cell { Vector , DataMemory ( 2048 ), DDR2 } ( in : port in ( 64 ); result : port out ); (* in port is implemented with width of 64 bits *) VAR ... BEGIN (* ... filter action ... *) END Filter ; Hierarchic Composition: Cell Nets Allocation of cells: new statement Connection of cells: connect statement Ports of cells can be delegated to the ports of the net: delegate statement Terminal or closed Cellnets (i.e Cellnets without ports) can be deployed to hardware Terminal Cellnet Example cellnet Example ; import RS232 ; TYPE UserInterface = cell { RS232 }( out1 , out2 : port out ; in : port in ) (* ... *) END UserInterface ; Adder = cell ( in1 , in2 : port in ; out : port out ) (* ... *) END Adder ; VAR interface : UserInterface ; adder : Adder BEGIN new ( interface ); new ( adder ); connect ( interface . out1 , adder . in1 ); connect ( interface . out2 , adder . in2 ); connect ( adder . result , interface . in ); END Example . Communication between cells ( CSP ) Receiving is blocking ( ? ) Sending is non-blocking ( ! ) Engine Cell Made From Hardware (in HW library for FPGA ) -Hardware Library - Computation Components - General purpose minimal machine: TRM , FTRM - Vector machine: VTRM - MAC , Filters etc. - Communication Components (FIFOs) - 32 * 128 - 512 * 128 - 32, 64, 128, 1k * 32 - Storage Components - DDR2 controller - configurable DRAMs - CF controller - I/O Components - UART controller - LCD , LED controller - SPI , I2C controller - VGA , DVI controller Programming Language vs. HDL Programming Language Sequential execution No notion of time Example: :::modula2 VAR a,b,c: INTEGER ; a := 1; ( unknown ) b := 2; ( mapping to ) c := a + b; ( machine cycles ) Hardware Description Language Continuous execution (combinational logic): wire [ 31 : 0 ] a , b , c ; assign c = a + b ; // no assign a = 1 ; // memory assign b = 2 ; // associated Synchronous execution (register transfer): reg [ 31 : 0 ] a , b , c ; always @ ( posedge clk ) begin a <= 1 ; // synchronous at b <= 2 ; // rising edge c <= a + b ; // of the clock end ; Single/Cycle Datapath ( TRM ) Instruction Fetch Get value of PC Get data from instruction memory (36-bit) keep only upper or lower part (16-bit) depending on value of PC On clock if reset: PC := 0 if stall: PC := PC else: PC := pcmux (set new PC ) Register Read Read source operands from register file wire [ 2 : 0 ] rd , rs ; wire regWr ; wire [ 31 : 0 ] rdOut , rsOut ; source register // register file // ... assign irs = IR [ 2 : 0 ]; // source register assign ird = IR [ 13 : 11 ]; assign dst = ( BL ) ? 7 : ird ; // destination register For BL the destination register is always the link register (7) Otherwise the registers are read from the instuction register ( IR ) ALU Compute the result via ALU wire [ 31 : 0 ] AA , A , B , imm ; wire [ 32 : 0 ] aluRes ; assign A = ( IR [ 10 ]) ? AA: { 22 ' b0 , imm }; // bit 10: immediate or register assign minusA = { 1 ‘ b0 , ~ A } + 33 ‘ d1 ; assign aluRes = ( MOV ) ? A: ( ADD ) ? { 1 ‘ b0 , B } + { 1 ‘ b0 , A } : ( SUB ) ? { 1 ‘ b0 , B } + minusA : ( AND ) ? B & A : ( BIC ) ? B & ~ A : ( OR ) ? B | A : ( XOR ) ? B &#94; A : ~ A ; Control Path Control Path assign vector = IR [ 10 ] & IR [ 9 ] & ~ IR [ 8 ] & ~ IR [ 7 ]; assign op = IR [ 17 : 14 ]; assign MOV = ( op == 0 ); assign NOT = ( op == 1 ); assign ADD = ( op == 2 ); assign SUB = ( op == 3 ); assign AND = ( op == 4 ); assign BIC = ( op == 5 ); assign OR = ( op == 6 ); assign XOR = ( op == 7 ); assign MUL = ( op == 8 ) & ( ~ IR [ 10 ] | ~ IR [ 9 ]); assign ROR = ( op == 10 ); assign BR = ( op == 11 ) & IR [ 10 ] & ~ IR [ 9 ]; assign LDR = ( op == 12 ); assign ST = ( op == 13 ); assign Bc = ( op == 14 ); assign BL = ( op == 15 ); assign LDH = MOV & IR [ 10 ] & IR [ 3 ]; assign BLR = ( op == 11 ) & IR [ 10 ] & IR [ 9 ]; IR [17:14] Function 0000 B := A 0001 B := ~A 0010 B := B + A 0011 B := B – A 0100 B := B & A 0101 B := B & ~A 0110 B := B 0111 B := B &#94; A Write Result Back to Rd wire [ 31 : 0 ] regmux ; wire regwr ; // ... assign regwr = ( BL | BLR | LDR & ~ IR [ 10 ] | ~ ( IR [ 17 ] & IR [ 16 ]) & ~ BR & ~ vector )) & ~ stall0 ; assign regmux = ( BL | BLR ) ? {{{ 32 - PAW }{ 1 'b0 }}, nxpc } : ( LDR & ~ IoenbReg ) ? dmout: // data memory out ( LDR & IoenbReg ) ? InbusReg: // from IO ( MUL ) ? mulRes [ 31 : 0 ] : ( ROR ) ? s3: ( LDH ) ? H: aluRes ; TRM Stalling Stop fetching next instruction, pcmux keeps the current value Disable register file write enable and memory write enable signals to avoid changing the state of the processor Only LD and MUL instructions stall the processor dmwe signal is not affected regwr signal is affected Load ( LD ) wire [ 31 : 0 ] dmout ; wire [ DAW: 0 ] dmadr ; wire [ 6 : 0 ] offset ; reg IoenbReg ; // register file // ... // src register = lr (7) ignored: harward architecture Assign dmadr = ( irs == 7 ) ? {{{ DAW - 6 }{ 1 'b0 }}, offset } : ( AA [ DAW: 0 ] + {{{ DAW - 6 }{ 1 'b0 }}, offset }); assign ioenb = & ( dmadr [ DAW: 6 ]); // I/O space: uppermost 2&#94;6 bytes in data memory assign rfWd = ... ( LDR & ~ IoenbReg ) ? dmout: ( LDR & IoenbReg ) ? InbusReg: //from IO ...; always @( posedge clk ) IoenbReg <= ioenb ; Store ( ST ) wire [ 31 : 0 ] dmin ; wire dmwr ; // register file // ... DM #(. BN ( DMB )) dmx (. clk ( clk ), . wrDat ( dmin ), . wrAdr ({{{ 31 - DAW }{ 1 'b0 }}, dmadr }), . rdAdr ({{{ 31 - DAW }{ 1 'b0 }}, dmadr }), . wrEnb ( dmwe ), . rdDat ( dmout )); assign dmwe = ST & ~ IR [ 10 ] & ~ ioenb ; assign dmin = B ; Set Flag Registers always @ ( posedge clk , negedge rst ) begin // flags handling if ( ~ rst ) begin N <= 0 ; Z <= 0 ; C <= 0 ; V <= 0 ; end else begin if ( regwr ) begin N <= aluRes [ 31 ]; Z <= ( aluRes [ 31 : 0 ] == 0 ); C <= ( ROR & s3 [ 0 ]) | ( ~ ROR & aluRes [ 32 ]); V <= ADD & (( ~ A [ 31 ] & ~ B [ 31 ] & aluRes [ 31 ]) | ( A [ 31 ] & B [ 31 ] & ~ aluRes [ 31 ])) | SUB & (( ~ B [ 31 ] & A [ 31 ] & aluRes [ 31 ]) | ( B [ 31 ] & ~ A [ 31 ] & ~ aluRes [ 31 ])); end end end Branch instructions PC <= PC + 1 + off PC <= Rs PC <= PC + 1 (by default) PC <= PC (if stall) PC <= 0 (reset) Code: // pcmux logic assign pcmux = ( ~ rst ) ? 0 : ( stall0 ) ? PC: ( BL ) ? {{ 10 { IR [ BLS - 1 ]}}, IR [ BLS - 1 : 0 ]} + nxpc : ( Bc & cond ) ? {{{ PAW - 10 }{ IR [ 9 ]}}, IR [ 9 : 0 ]} + nxpc : ( BLR | BR ) ? A [ PAW - 1 : 0 ] : nxpc ; Separate Compilation ( TRM ) Development environment with separate compilation important for commercial products state of the art Very limited architecture 18-bit instructions 10-bit immediates 7-bit memory offset 10bit (signed) conditional branch offset 14-bit (+/- 8k) Branch&Link long jumps (chained) immediates -> expressed by several instructions or put to memory (fixups) fixups, problematic if large global variables are allocated -far procedure calls Solution: Compilation to Intermediate Code, back-end application at link time Compilation Steps Scanner & Parser Checker Intermediate Back-end Intermediate Object File Active Cells Back-end Active Cells Specification, Intermediate Assembler Target Back-end Generic Object File Linker Problems with Active Cells 1 What if several same HW components available? How to extend HW without rewriting compiler each time? Generic Communication Interface Peer-to-Peer Use of AXI4 Stream interconnect standard from ARM Generic, flexible Non-redundant Active Cells 2 Flexible parameterization of the components using interpreted code in the component specification XML -based specification (object persistency) Active Cells 3 Parameterization and Description of Hardware completely in one programming language Platform specific settings clock sources, pin locations etc Component specific settings: dependencies (Verilog-Files) parameters port names Can be made very generic with plugins This image is taken from the lecture slides provided by Felix Friedrich","tags":"Programming","url":"system_construction.html","loc":"system_construction.html"},{"title":"Distance Functions and Metrics","text":"Distant Functions A distant function works on two elements (i.e. vectors, sets…). A given distance function \\(d(s,t)\\) needs to fulfill properties: \\(d(s,t) \\geq 0\\) (non-negative) \\(d(s,s) = 0\\) (distance to itself is zero) \\(d(s,t) = d(t,s)\\) (symmetric) \\(d(s,t) + d(t,r) \\geq d(s,r)\\) ( triangle inequality ) Metric A metric requires an additional property in addition to the four properties of a distant function: \\(d(x, y) = 0\\) if and only if \\(x = y\\) (coincidence axiom) Two given elements with no distance to each other must be the same element Jaccard Similarity \\(Sim(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}\\) Jaccard Distance \\(d(A,B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}\\)","tags":"Mathematics","url":"distance_functions_and_metrics.html","loc":"distance_functions_and_metrics.html"},{"title":"ARM Stack Frame","text":"This page contains notes about the usual stack frame layout (calling convention) on ARM processors. Most information is taken from ARM procedure calling conventions and recursion . The conventions are defined by ARM and are called PCSAA . Link Register Contains the return address of an function call Caller Code that calles a function Callee Function that is called by other code Frame Pointer Points to the actual stack frame (Base Pointer on x86) Stack Pointer Points always to the top of the stack General Notes BL and BLX use R14 ( RL ) as link register Simple functions (i.e. leaf functions) use MOV PC, LR to return to the caller function R0 - R3 are used to pass arguments to the callee (can be overwritten by callee, caller saved) R0 contains often the result R4 - R10 must be saved by the called function (callee) if needed They must be unchanged after return Callee saved Restore before return R13 ( SP ) contains the address of top of the stack (last filled poition) Stack is Full-Descending: SP points to the last filled location Stack grows downwards (from higher to lower addresses) PUSH / POP : STMFD / LDMFD (Store multiple Full-Descent, Load multiple Full-Descent) LDMFD SP , { R0 - R3 } is equvalent to: LDR R0 , [ SP ] LDR R1 , [ SP , # 4 ] LDR R2 , [ SP , # 8 ] LDR R3 , [ SP , # 12 ] To alter (update) SP : LDMFD SP ! , { R0 - R3 } LR needs to be saved on stack for non -leaf functions ::: nasm _func: STMFD SP ! , { R4 - R6 , LR } ; ... code of func LDMFD SP ! , { R4 - R6 , PC } ; Pushing stacked LR directly to PC => return to caller The order of registers in STMFD and LDMFD is always the same: lower regisers at lower addresses More general stack frame: Quick clear down of stack frame: MOV SP, FP","tags":"Programming","url":"arm_stack_frame.html","loc":"arm_stack_frame.html"},{"title":"List of Design Patterns","text":"Overview of Design Patterns Creational Patterns Structural Patterns Behavioral Patterns Other Patterns Abstract Factory 1 2 Adapter 1 2 Chain of Responsibility 1 Business Delegate Builder 1 Composite 1 2 Command 1 2 Data Access Object Factory Method 1 2 Bridge 1 Interpreter 1 Data Transfer Object Prototype 1 Decorator 1 2 Iterator 1 2 Dependency Injection Singleton 1 2 Facade 1 2 Mediator 1 Inversion of Control Flyweight 1 Memento 1 Model View Controller 2 Proxy 1 2 Null Object Model View Presenter Observer 1 2 Plugin State 1 2 Fluent Interface Strategy 1 2 Functor Template Method 1 2 Filter-Pipe Visitor 1 See also here for a list of OO -Principles. And Wikipedia:Software design pattern for a good overview. Design Patterns (GoF) ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ Head First Design Patterns ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩ ↩","tags":"Programming","url":"list_of_design_patterns.html","loc":"list_of_design_patterns.html"},{"title":"Vector Calculus","text":"Some notes about Vector Calculus. Mainly Divergence, Gradient and Curl. Nabla Operator ( \\(\\nabla\\) ) $$\\nabla = \\left (\\frac\\partial{\\partial x_1},\\ldots, \\frac\\partial{\\partial x_n}\\right)$$ For a 3-dimensional cartesian coordinate system: $$\\nabla = \\left(\\frac\\partial {\\partial x}, \\frac\\partial {\\partial y}, \\frac\\partial {\\partial z}\\right) = \\vec e_x \\frac\\partial {\\partial x} + \\vec e_y \\frac\\partial {\\partial y} + \\vec e_z \\frac\\partial {\\partial z}$$ Sometimes \\(\\vec\\nabla\\) is written instead of \\(\\nabla\\) to show that it's basically a vector. The Nabla Operator can be seen as \"Vector of partial derivative operators \". The dimension is same as the input dimension. Operator: takes as input a function an outputs an other function. Gradient ( \\(\\operatorname{grad}\\) ) The gradient represents the slope of the tangent of the graph of a function. Input: A Scalar field Output: A Vector field The direction of the gradient at one position shows the direction of the biggest change (steepest ascent) in the scalar field. The vectors are pointing \"uphills\". The length (absolute value) of the gradient at that point is a measurement of the change (slope, steepness). $$\\nabla f = \\frac{\\partial f}{\\partial x_1}\\mathbf{e}_1 + \\cdots + \\frac{\\partial f}{\\partial x_n }\\mathbf{e}_n$$ \\(e_i\\) : The orthogonal unit vectors pointing in the coordinate directions The Gradient is always perpendicular to the Contour lines. Divergence ( \\(\\operatorname{div}\\) ) The divergence is a vector operator that measures the magnitude of a vector field's source or sink. It is a scalar field with signed components (positive values for sources, negative values for sinks). Input: A Vector field Output: A Scalar field The divergence is the scalar product between the Nabla operator \\(\\nabla=(\\tfrac{\\partial}{\\partial x_1},\\ldots,\\tfrac{\\partial}{\\partial x_n})\\) and the vector \\(\\vec F = (F_1, \\ldots,F_n)\\) . $$\\operatorname{div}\\,\\vec F = \\nabla\\cdot\\vec F =\\frac{\\partial F_1}{\\partial x_1}+\\frac{\\partial F_2}{\\partial x_2}+\\cdots +\\frac{\\partial F_n}{\\partial x_n} = \\sum_{i=1}&#94;n\\frac{\\partial F_i}{\\partial x_i}$$","tags":"Mathematics","url":"vector_calculus.html","loc":"vector_calculus.html"},{"title":"Undo Committed Changes with SVN","text":"To undo changes that are already committed in Subversion a back-merge can be applied: svn merge - r [ current_version ] : [ previous_version ] [ repo_url ]","tags":"Programming","url":"undo_committed_changes_with_svn.html","loc":"undo_committed_changes_with_svn.html"},{"title":"Executable Loader","text":"Most information of this page is taken from Calling conventions for different C++ compilers and operating systems . Some other good information sources are: Linkers and Loaders Wikipedia:Linker Wikipedia:Loader Wikipedia:Relocation Book: Computer Systems: A Programmer's Perspective, Randal E. Bryant, David R. O'Hallarom Most explanations on this page are for the x86 platform! Modern OS 's allow to load executable code at runtime. This can be complete programms or (dynamic) libraries. On UNIX a dynamic library is called shared object . On Windows it's called dynamic link library (dll) . The OS needs to load the code to a specific position in memory and it needs to adjust any internal references and addresses. This loading is done in a simmilar way in most OS . The relocation of the executable image is done according to the image base. The image base is the virtal memory address at which the image is loaded. The load-time relocation can be avoided if the image base is fixed. The most common values for the image base are: Windows (32-bit, 64-bit): 0x400000 Linux (32-bit): 0x8048000 Linux (64-bit): 0x400000 Mach-O (32-bit): 0x1000 Mach-O (64-bit): 0x100000000 Dynamic libraries can't be loaded at a fixed image base. There are three aproaches for loading executables without fixed image base: The executable (or library) contains a relocation table. This contains all cross-references in the file. The loader adjusts all these cross-references if the executable is loaded at another base address then the the one chosen by the linker. This aproach is used by Windows (32-bit, 64-bit) The executable is designed position independant. All addresses are calculated at runtime relative to IP . This aproach is used on OS X (Intel) The shared object contains a Global Offset Table ( GOT ). In the GOT are the addressrs of static objects stored. The addresses in the GOT are afjusted by the loader according to the base address. Code that accesses static data points to the GOT and the GOT then points to the data. This aproach is often used in Linux (32-bit) and BSD Difference between Relocation Table and Global Offset Table ( GOT ) Relocation Table Relocation table is not changed when shared object is loaded. The relocation table points to data references in the code. These references are modified in place. Global Offset Table ( GOT ) The GOT is changed by the loader. So the entries in the GOT point to the new (relocated) addresses. Static data references in the code still point to the GOT but the entries in the GOT were updated. Function calls across shared objects (DLLs) Shared objects can have references to functions in other shared objects. Usually calls to these functions go through an import table in the executable (shared object) file. Either the loader fills the import table at load time or it is lazy initialized for each function when it is called the first time.","tags":"Programming","url":"executable_loader.html","loc":"executable_loader.html"},{"title":"Preprocessor Defines","text":"Preprocessor Defines This page lists commonly used preprocessor defines for C/C++ Programming Language Language Define C++ __cplusplus C 99 __STDC__ Compiler Compiler Define GNU __GNUC__ Visual Studio _MSC_VER_ Processor Processor Defines x86 _M_IX86 , __INTEL__ , __i386__ x86-64 _M_X64 , __x86_64__ IA64 _IA64__ Endianness Byte Order Define Little Endian __LITTLE_ENDIAN__ Big Endian __BIG_ENDIAN__ OS OS Defines Windows 32 bit _WIN32 , __WINDOWS__ Windows 64 bit _WIN64 , _WIN32 Linux 32 bit __unix__ , __linux__ Linux 64 bit __unix__ , __linux__ , __LP64__ BSD __unix__ , __BSD__ , __FREEBSD__ OS X __APPLE__ , __DARWIN__ , __MACH__ Debugging Purpose Defines Visual Studio Debug Code _DEBUG Disable assert() NDEBUG Qt Qt has it's own preprocessor defines: Global Qt Declarations Endianness Byte Order Define Little Endian Q_LITTLE_ENDIAN Big Endian Q_BIG_ENDIAN Q_BYTE_ORDER is set to either Q_LITTLE_ENDIAN or Q_BIG_ENDIAN . Compiler Compiler Define GNU C++ (and clang?) Q_CC_GNU Microsoft Visual C/C++ or Intel C++ for Windows Q_CC_MSVC Intel C++ (Linux, Windows) Q_CC_INTEL There are a lot more defines for different compilers. OS OS Define Android Q_OS_ANDROID BSD 4.4 Q_OS_BSD4 Cygwin Q_OS_CYGWIN OS X, iOS, Darwin Q_OS_DARWIN FreeBSD Q_OS_FREEBSD iOS Q_OS_IOS Linux Q_OS_LINUX OS X, iOS (not OSS Darwin) Q_OS_MAC NetDSB Q_OS_NETBSD OpenDSB Q_OS_OPENBSD OS X Q_OS_OSX QNX Neutrino Q_OS_QNX Any UNIX BSD / SYSV system Q_OS_UNIX 32-bit and 64-bit Windows (not Windows CE ) Q_OS_WIN32 64-bit Windows Q_OS_WIN64 All supported Windows Q_OS_WIN Windows CE Q_OS_WINCE Windows Phone 8 Q_OS_WINPHONE Windows Runtime Q_OS_WINRT There are a lot more defines for supported OS 's by Qt. Processors Processor Defines Intel x86 (32-bit, 64-bit) Q_PROCESSOR_X86 Intel x86 (32-bit) Q_PROCESSOR_X86_32 Intel x86 (64-bit) Q_PROCESSOR_X86_64 Inel IA64 (Itanium, Itanium 2) Q_PROCESSOR_IA64 ARM (V5, V6, V7) Q_PROCESSOR_ARM (defined on all ARM architectures) ARMv5 Q_PROCESSOR_ARM_V5 ARMv6 Q_PROCESSOR_ARM_V6 , Q_PROCESSOR_ARM_V5 ARMv7 Q_QPROCESSOR_ARM_V7 . Q_PROCESSOR_ARM_V6 , Q_PROCESSOR_ARM_V5 AVR 32 Q_PROCESSOR_AVR32 There are a lot more defines for supported processors by Qt.","tags":"Programming","url":"preprocessor_defines.html","loc":"preprocessor_defines.html"},{"title":"Object File Formats","text":"Most information of this page is taken from Calling conventions for different C++ compilers and operating systems . Overview Object Format Platform COFF / PE Windows 32 ( PE ), Windows 64 ( PE32 +) ELF Linux, UNIX , BSD … Mach-O OS X (Darwin) a.out Older versions of UNIX Common Object File Format ( COFF , PE ) First used in UNIX System V, later superseeded by ELF . PE (Portable Executable) is a modified version of COFF from Microsoft that is used for Windows. The same format is used for object files and for executables. Compilers that use COFF / PE : Visual C++ Intel (Win) GCC (Win) COFF uses many different data structures. This makes it difficult to handle. Alignment handling of the data structures can be difficult. Properties: Can contain debug information Different implementations ( COFF , PE , XCOFF , ECOFF ) are not compatible Limitations: Segment word size: 32 or 64 bits. Max number of sections: 32k. Max file size: 4 GB . Max section size: 4 GB . Max relocations per section: 64k. Max library size: 4 GB . Executable and Linkable Format ( ELF ) Replaced older formats like a.out and COFF in Linux and BSD . Compilers: GCC (Linux, BSD , Win…) Clear and robust design. Limitations: Segment word size: 32 or 64 bits. Max number of sections: 64k. Max file size: 4 GB for 32 bits, \\(2&#94;{64}\\) for 64 bits. Max section size: 4 GB for 32 bits, \\(2&#94;{64}\\) for 64 bits. String table size: 4 GB . Max number of symbols: 16M for 32 bits, 4G for 64 bits. Max library size: 4 GB . Mach-O Format This format is used by OS X. Only the format for Intel Mac OS X is described here. Can be used for object files and executable files. Object files have only one segment record that contains several section records. Executable files contain several segment records. Mach-O allows address specifications relative to any reference point in any section. This is used for position-independent code. Limitations: Section name length: 16 characters. Max file size: 4 GB . Max section size: 16 MB for position-indep code, 4 GB for 32 bits, \\(2&#94;{64}\\) for 64 bits. Max number of sections: 16 M. Max number of symbols: 16 M. Max library size: 4 GB . a.out Format Stands for ‘Assembler Output'. Is used in older UNIX systems. ‘ a.out' is often the default output file name of linkers. Even if the output format is an other one. Some tools still support the a.out format. Data Endianness All systems based on x86 (16-bit, 32-bit and 64-bit) use little endian.","tags":"Programming","url":"object_file_formats.html","loc":"object_file_formats.html"},{"title":"Concepts of Object Oriented Programming","text":"Core Requirements to OOP Languages Highly dynamic execution model Cooperating program parts with well defined interfaces Classification (hierarchy) and specialisation (reuse) Quality/Correctness The Object Model A software system is a set of cooperating objects Objects have state (fields) and processing ability (methods) Objects exchange messages (methods) Objects have: State Identity Lifecycle Location Behavior Objects are different form values! Values don't have the above properties. Interfaces and Encapsulation Objects have well-defined interfaces Publicly accessible fields Publicly accessible methods Implementation is hidden behind interface Encapsulation Information hiding Interfaces are the basis for describing behavior Classification and Polymorphism Classification: Hierarchical structuring of objects (‘is-a'-Relation) Objects belong to different classes simultaneously Substitution principle : Subtype objects can be used wherever supertype objects are expected Child <: Parent Classification We can classify objects or fields (?) Classifications can be trees or DAGs Classifications of objects form \"is-a\" relation Classes can be abstract or concrete Substitution Principle: Objects of subtypes can be used wherever objects of supertypes are expected Polymorphism Subtype Polymorphism Direct consequence of substitution principle Run-time (dynamic) Polymorphism Dynamic (late) binding Parametric Polymorphism Generic types Uses type parameters One implementation can be used for different types Type missmatch detected at compile time C++ Templates, Generics (Java, C#) Method Overloading Ad-Hoc Polymorphism Overloading: Methods with same name but different arguments Spezialization Start from general objects/types Extend these objecs (fields and methods) Behaviour of specialized objects need to be compliant to more general objects! (Substitution Principle) Progam parts that work for the genral objects work also for specialized objects Methods can be overridden Types and Subtyping Types Type systems can be analyzed in three dimensions: Weak and Strong Type Systems Nominal and Structural Types Static and Dynamic Type Checking Definition: A type is a set of values sharing some properties. A value v has type T if v is an element of T . T is a set that contains all possible values v . Weak and Strong Type Systems How strongly or weakly typed a language is concerns casting (implicit and explicit). It's mainly used to compare languages to each other about the possible castings, type safety and information loss. Untyped Languages Not classifying values into types, just bit patterns i.e. Assembler Weakly Typed Languages Classifying values into types No strict enforcement of restrictions, i.e Multiplying two pointers is possible i.e. C, C++ Strongly Typed Languages Classify values into types Enforcing that all operations are applied to values of appropriate type Strongly-typed languages prevent certain erroneous or undesirable program behavior i.e. Java, Python, Scala, Smalltalk, Eiffel, C# Most Dynamic Languages (i.e Python, JavaScript) are Strongly Typed Nominal and Structural Types Nominal Types Based on type names i.e. C++, Java, Eiffel, Scala Structural Types Based on available methods and fields i.e. Python, Ruby, Smalltals Type Checking Type checking prevents certain errors in programm. When happens the type checking? Static: Compile time Dynamic: Run time Static Type Checking Types of variables and methods are declared explicitly or inferred Types of expressions can be derived from the types of their constituents Type rules are used at compile time to check whether a program is correctly typed A programming language is called type-safe if its design prevents type errors Pros: Static safety Readability (type annotations are a good documentation) Efficiency Dynamic Type Checking Variables, methods, and expressions of a program are typically not typed (types not declared) Every object and value has a type Run-time system checks that operations are applied to expected arguments Also static languages need to performe some checks dynamically at run-time (i.e type-casting) Dynamic languages are usually more expressive (no type annotations) Pros: Expressiveness Low overhead (no type annotations) Much simpler Overview of Type Systems in OO -Languages Static Dynamic Nominal C++, Java, Eiffel, Scala, C# certain features of statically-typed languages Structural Research languages: O'Caml, Moby… Python, JavaScript, Ruby, Smalltalk Dynamic and Structural is often called \"duck typing\". Subtyping Substitution principle Objects of subtypes can be used wherever objects of supertypes are expected Syntactic classification: Subtypes understand at least the messages of their supertypes. Nominal languages: Subtype has a wider (or same) interface as supertype Overriding methods must not be less accessible Semantic classification: Subtypes provide at least the behaviour of their supertypes. A type is a set of values. Subtype relation corresponds to subset relation . In nominal programming languages the programmer decides about subtype relation In structural programming languages the type checker decides about subtype relation Variance (Covariance, Contravariance and Invariance) Based on substitution principle. Covariance Ordering of types from more specific to more generic (in direction of inheritance hierarchy) Contravariance Ordering of types from more generic to more specific (in oposite direction of inheritance hierarchy) Variance for OOP Coursera:Scala Variance Input Arguments Contravariant Return Values and Exceptions Covariant In- and Output Arguments (Mutable Reference Arguments) Nonvariant SuperReturnType Super :: foo ( SubParamType p ); // | &#94; contra- // .. &#94; // | | variant // v .. // | co- | SubReturnType Sub :: foo ( SuperParamType p ); // v variant | Super is more general than Sub . Sub is more specific than Super . Programming Language Return Type Argument Type C++, Java, Scala, D… Covariant Nonvariant Eiffel Covariant Covariant C# Nonvariant Nonvariant In Java and C# arrays are covariant! Behavioral Subtyping (Contracts) What are the properties shared by the values of a type? Properties should also include the behavior of the object. This is expressed as interface specifications (contracts) Precondition: Have to hold before the method is executed Postcondition: Have to hold after the method has terminated Old-expressons: Can be used to refer to prestate values from the postcondition Invariant: Have to hold in all states in which an object can be accessed by other objects Subtyping and Contracs Subtypes must fulfill contracts of supertypes Overriding method of subtypes may have weaker preconditions than the supertype method Overriding method of subtypes may have stronger postconditions than the supertype method Subtypes may have stronger invariants than supertypes Subtypes may have stronger history constrains than supertype Specification Inheritance (Inherit Contracts from Subtypes) Subtype needs to satisfy the contract of the supertype (inheriting contracts) Invariant inheritance: Conjunction ( AND ) of own contract and contracts of all supertypes History inheritance: same as for invariants Precondition inheritance: Disjunctions ( OR ) of own contract and contracts of all supertypes $$PreEff_{S.m} = Pre_{S.m} || Pre_{T.m} || Pre_{T'.m} || ...$$ Postcondition inheritance: Satisfy each postcondition for which the corresponding precondition holds Precondition needs to be evaluated with old state $$PostEff_{S.m} = (old(Pre_{S.m}) => Post_{S.m}) \\&\\& (old(Pre_{T.m}) => Post_{T.m}) \\&\\& ...$$ Types as Contracts Types can be seen as a kind of contracts. Overriding Methods must: Behavioral Subtyping (contracts) Nominal/Structural Subtyping (variance) Weaker Preconditions Covariant Parameters Stronger Postconditions Covariant Results Stronger Invariants Invariant fields This doesn't apply exactly to: Invariants/Fields History constraints Inheritance Difference between inheritance and subtyping Inheritance allows to reuse the code (specialization) inside a class (member variables and method definitions). Subtyping supports reuse externally. It's used for polymorphism in form of the substitution principle. Subtyping expresses classification. Subtypeing depends only on the interface of objects and not on their implementations. In most existing OOP languages inheritance also is subtyping. C++ allows private (or protected) inheritance which does not result in subtyping. With interfaces (Java, C++…) it's possible to create subtypes without inheritance (no reuse of code from parent class). Usually the term \"inherit from an interface\" is used even if it's not correct. Subclassing = Subtyping + Inheritance Inheritance is not a core concept of OOP . OOP can do without inheritance, but not without subtyping! Aggregation vs. Private Inheritance (C++) Both solutions allow code reuse without establishing a subtype relation No subtype polymorphism No behavioral subtyping equirements Aggregation causes more overhead Two objects at run-time Boilerplate code for delegation Access methods for protected fields Private inheritance may lead to unnecessary multiple inheritance Static and Dynamic Method Binding Static binding: Methods are selected based on the static type of the receiver at compile time Dynamic binding: Methods are selected based on the dynamic type of the receiver object at run time Dynamic method binding enables specialization and subtype polymorphism Drawbacks Performance: Overhead of method look-up at run-time Versioning: Dynamic binding makes it harder to evolve code without breaking subclasses Defaults Dynamic binding: Eiffel, Java, Scala, dynamically-typed languages Static binding: C++, C# Static Method binding in Java Java binds methods statically in 3 cases: Static Methods Private Methods Method calls on super Rules for proper Subclassing Use subclassing only if there is an ‘is-a' relation Syntactic and behavioral subtypes Do not rely on implementation details Use precise documentation ( contracts where possible) When evolving superclasses, do not mess around with dynamically-bound methods Do not add or remove calls, or change order of calls Do not specialize superclasses that are expected to change often Binary Methods Binary methods take one explicit argument and receiver (this) Often behavior should be specialized depending on the dynamic types of both arguments. Recall that covariant parameter types are not statically type-safe! (?) Dynamic binding for specialization based on dynamic type of receiver How to specialize on the dynamic type of the explicit argument ? Visitor Pattern: tedious to write, requires modification of superclass Some Languages Support Multiple Dispatch: Method calls are bound on dynamic types of several arguments. Performance overhead Extra requirements are needed to ensure there is a \"unique best method\" for every call Multiple Inheritance All OOP languages support multiple subtyping: One type can have several supertypes Subtype relation forms a DAG Some languages support multiple inheritance. Problems with multiple inheritance Ambiguities: Superclasses may contain fields and methods with identical names and signatures Which version should be available in the subclass? Repeated inheritance (diamonds): A class may inherit from a superclass more than once How many copies of the superclass members are there? How are the superclass fields initialized? Mixins and Traits TODO … Parametric Polymorphism Java, C#, … Subtype relation not always desiderable Generics (Java, Scala, C#) Upper bounds ( extends ): Subtype of upper bound required Guarantees that a specific method can be called Modular check of implementation of Generic code Generics (in Java, C#) are non-variant Covariance is unsafe when client writes to generic type argument (‘input') Mutable fields Method arguments Contravariance is unsafe when client reads from generic type argument (‘output') Fields Method results Non-variance is sometimes too restrictive Scala allows variance-annotation Positive positions (‘output', covariant ): + Result type Type of immutable fields Negative positions (‘input', contravariant ): - Parameter type C# uses keywords in and out Methods can also have type arguments (i.e static <T> void printAll(Collection<T> c) {...} ) Wildcards Wildcards can be seen as an Existential Type : static void printAll ( Collection <?> c ) { for ( Object e : c ) { System . out . println ( e ); } } There exits a type argument T such that c has type collection<T> Wildcards can have a upper bounds and lower bounds (correspond to co- and contravariance ) upper bound for reading and method invocation: extends lower bound writing : super lower bounds are not supported on type parameters (only on wildcards) in Java Instantiation of wildcards can change over time: class Wrapper { Cell <?> data ; } // client code: Wrapper w = new Wrapper (); w . data = new Cell < String > (); // w.data has type Cell<String> w . data = new Call < Object > (); // now w.data has type Cell<Object>! Generics with wildcards (and possibly with bounds) have a subtype relation if the type parameters have a relation See Java documentation Type Erasure (Java, Scala) For backwards compatibility (in JVM ) Generic type information is erased in compiler (not available in bytecode anymore) C<T> is translated to C T is translated to its upper bound Casts are added wher nessecary (i.e reading values from generic type) Only one classfile and one class object for all instantiations of a generic class Run-time type information ( instanceof , List<String>.class ) is missing Arrays of generic types are not possible ( new List<String>[10] ) Static fields are shared by all instantiations of a generic class Lower bounds for type parameters would require support in JVM (bytecode verification) No Type Erasure in C# Run-type type information is available Arrays of generic types are possible C++ Templates Classes and Methods (Functions) can be parametrized Also value types can be used as templates paramters Instantiation generates (internally) a new class Type checking is done on of the generated class, not on the template (different to Java, C#, …) Type check only of the parts of code that are used Type errors are not detectes before instantination No bounds needed No subtype releation between instantiations of a template No run-time support needed (templates are a compilation concept) Templates can be specialized Improvement for feature C++ standard (C++17): Concepts Lite Structural upper bounds (C++ type system is nominal) Template Meta Programming Is touring-complete! Information Hiding and Encapsulation In the literature Information Hiding and Encapsulation are often used synonymously. But they are distinct but related. Information Hiding Information hiding is used to reduce dependencies between modules. The client is provided only the information needed. Concerns static parts of program (code) Syntactic and semantic: Contracts are part of the exported interfaces Reduce dependencies Classes can be studied in isolation Classes only interact in well-defined ways Client Interface of a Class Class Name Type parameters (Generics) and their bounds Super-interfaces Signatures of exported methonds and fields Client interface of direct superclass Other Interfaces Subclass interface (i.e protected ) Friend interface ( friend in C++, default access in Java) Inner classes … Java Access Modifiers public : client interface protected : subclass and friend interface default access: friend interface private : implementation Modifier Class Package Subclass World public Yes Yes Yes Yes protected Yes Yes Yes No ‘ default ' Yes Yes No No private Yes No No No Safe Changes Renaming of hidden elements Modification of hidden implementation (functionally needs to be preserved) Access modifiers specify what classes might be affected by a change Exchanging Implementation Behaviour needs to be preserved Exported fields limit modification Use getters and setters Uniform access (Eiffel, Scala) Modification is critical: Fragile baseclass problem! Object structures Bug: Method Selection in Java Bug was present in JSL1 . It's fixed now! Compile time: Determin static declaration (find the method in the receiver class, method can be inherited) Check accessibility Determine invocation mode (virtual / non-virtual) At run-time: Compute receiver reference Locate method to invoke (based on dynamic type of receiver object) that overwrites Rules for overriding Access modifier of overriding method must provide at least as much access as the overridden method default access → protected → public private methods can't be overridden: Hiding Encapsulation Encapsulation is used to guarantee that data and structural consistency by capsules with well defined interfaces. Data consistency: i.e value is not negatice, … Structural consistency: i.e tree is balanced, list is doubly linked, … Concerns dynamic parts of code (execution) Context of a module can be changed but module behaves same Levels of Encapsulation Capsules can be: Individual objects Object structures: i.e doubly-linked list A class (with all its objects): i.e all threads in Java All classes of a subtype hierarchy A package with all of its classes and their objects) Several packages Internal representation of capsule that needs to be proteced: invariant or history constraint Hiding fields are useful for: Information Hiding Encapsulation Achieving Consistency of Objects Apply information hiding wherever possible Make consistency criteria explicit Contracts Informal documentation Check interfaces (also subclass methods, i.e protected ) Make sure they preserve documented consistency criteria Object Structures An object structure is a set of objects that are connected via references. Examples Array-Based Lists Doubly-Linked Lists ( java.util ) Aliasing A reference to memory location Aliasing occures if more than one variable allows access to the same memory location Static/Dynamic Aliases Static alias: all involved variables are in the heap Dynamic aliasing: some involved variables are stack-allocated (others can be in the heap) Intended Aliasing Efficiency Objects need not to be copied, when passed or modified Sharing Share the same object between different places Consequence of objects identity Unintended Aliasing Capturing Get a reference from outside and store it Often in constructors that take reference arguments Leaking Passing a reference to an (internal) data structure to the outside More frequent then capturing Problems with Aliasing Aliases can be used to by-pass interface Interfaces and contracts remains unchanged but observable behaviour can change! Consistency of Object Structures Consistency of object structures debend on several fields (not only one) Checking invariance on beginning and end of method is not enough State can be changed in between by an alias Other Problems with Aliasing Synchronization in concurrent programs Lock protects data structure Locking a reference dosen't lock aliases Distributed programming i.e Remote Method Invocation References (intended aliases) are lost Optimizations i.e Inlining is not possible for aliased objects Alias Control in Java LinkedList : All fields are private Entry is private inner class of LinkedList References are not passed out Subclasses cannot manipulate or leak Entry objects ListItr is private inner class of LinkedList Interface ListIterator provides controlled access to ListItr objects ListItr objects are passed out in a controlled way Subclasses cannot manipulate or lead ListItr objects Subclassing is restricted! String All fields are private References to internal char-array are not passed out Subclassing is prohibited ( final ) Readonly Types Restrict access to shared objects Common: grant read-only access Cloning can prevent aliasing in some cases (but is inefficient) The reference can be marked as readonly The object itself is not readonly Requirements for Readonly Access Mutable objects Only some clients can mutate object Access restrictions apply to references (not whole objects) Prevent field updates, calls of mutating objects Transitivity Possible solution: wrap objects in readonly objects or use a readonly interface Not practical Not safe: no compiler checks, readwrite alias can still occur, … Readonly access in C++ ( const ) is not transitive Pure Methods Pure methods are side-effect free. Must not contain field updates Must not invoke non-pure methods Must not create objects (on heap) Can be only overridden by pure methods Stronger constraints than const methods in C++ Pure methods are very restrictive: Not possible to get an iterator (which is created on heap) to iterate over collection Caches can't be implemented Lazy initialization is not possible Readwrite and Readonly Types Concerns only reference type (object type is always mutable) Readwrite type: T Readonly type: readonly T Subtype relation: T <: readonly T Not same as relation between mutable and non-mutable types (which have no relation) Readonly is transitive Transitivity of Readonly Types The type of Field access Array access Method invocation is determined by type combinator: ► ► rw T ro T rw S rw T ro T ro S ro T ro T Type Rules: Readonly Access Readonly types can't be receiver of: Field update Array update Invocation of pure method Readonly types must not be cast to readwrite types. Leaking can be prevented Capturing can still occure Ownership Types Prevents capturing. Object Topologies Distinguish internal references from other references. Roles in Object Structures Interface objects: used to access the structure Internal representation: must not be exposed to outside (clients) Arguments of the object structure: must not be modified by the structure (i.e entries in a list) Ownership Model Each object has one (or zero) owner An object belongs to the internal representation of the owner Ownership relation is acyclic (forrest of ownership trees) Context: all objects that have the same owner Ownership relation is not transitive Type Invariant: The static ownership information (declared in code) reflects the run-time ownership of the referenced object Ownership Types peer : in the same context, same owner as owner of this rep : references to objects owned by this (in the context of this ) any : in any context (I don't care) lost : specific owner but not known (I care but don't know) self : only for the this literal (special because ownership relative to this ) lost and self are internal (hidden) type modifiers. No keywords. Traversing hierarchy: rep : go down in herarchy peer : go across on same level in hierarchy any : jump somewhere, could even be outside of hierarchy Type Safety RTTI contains: The class of each object The owner of each object Type invariant: static ownership reflects run-time owner any and lost are extistential types. any T o ; There exitst an owner such that o ist an istance of T and has that owner. Subtyp Relation between Ownership Types rep types and peer types are subtypes of corresponsing any types. rep T <: any T peer T <: any T Casts: any can be cast to rep or peer (with runtime checks) Viewpoint Adaption Ownership relation is expressed relative to this . If this object (viewpoint) changes, the ownership changes. When creating an object the ownership has to be set new rep Entry() new peer Entry() any is not allowed for new Ownership can't be changed later ► peer T rep T any T peer S peer T lost T any T rep S rep T lost T any T any S lost T lost T any T lost S lost T lost T any T self S peer T rep T any T Field Access and Method Invocation (Type Rules) \\(\\tau(a)\\) : Type of a Field Read or Method Parameters : v = e . f ; Is correctly typed if: $$\\tau(v) :> \\tau(e) \\blacktriangleright \\tau(f)$$ Field Write or Method Result : e . f = v ; $$\\tau(e) \\blacktriangleright \\tau(f) :> \\tau(v)$$ And $$\\tau(e) \\blacktriangleright \\tau(f)$$ is not lost . Aliasing rep : internal representation no unwanted sharing leaking as rep : viewpoint-adaptation in client gets lost method argument rep capturing as rep : gets lost , can't assign to lost Example: class Person { private rep Address addr ; // part of internal representation public rep Address getAddr () { return addr ; // clients get lost-reference } public void setAddr ( rep Address a ) { addr = a ; // cannot be called by clients (lost) only by this bject } public void setAddr ( any Address a ) { addr = new rep Address ( a ); // cloning necessary, can't assign any to rep } } Owner-as-Modifier Readonly Typesystem: leaking is safe (only readonly leaking) Ownership Typesystem: capturing is safe (declaring internal references as rep ) leaking can happen only as any or lost Combining both Typesystem any and lost : Readonly Additional enforced rules: Field write e.f = v; is valid only if \\(\\tau(e)\\) is self , peer or rep Method call e.m(...); is valid only if \\(\\tau(e)\\) is self , peer or rep , or called method is pure A method can modify directly at most all the objects that have the same owner as this . Everything further down in the hierarchy can only be changed indirectly (via method calls). When debugging: if an object changes the changing method is going to be on the call stack Changing methods need to go through all the owners transitively Owner is like a gate keeper (interface object) Stronger concept for encapsulation than private-protected-public leaking only happens as readonly (‘something' ► rep : lost ) Standard (Java): default modifier would be peer , flat datastructures ‘ shared ownership' is not possible. i.e List owns nodes and modifying Iterator would need readwrite access to nodes. List would need The system can be combined with Generics: rep List<peer Address> Also possible: merge entire contexts to new owner. i.e concat two lists. Achievements Encapsulate whole object structures Can not be violated Subclassing is no restriction Invariants of object o can depend on: Encapsulated fields of o (as usual) Fields of objects transitively owned by o Initialization and Null-References Main Usages of Null-References Terminate recursion, list, … Initialization (i.e lazy initialization) ‘ Result not found' as a return value of a function (absence of an object) Most (80%) of all variables in an OOP programm are non-null after initalization Real need for null value is rare Theoretical type system: Non-null tye: T! (references to T -Object) Possibly-null type: T? (references to T -Object plus null ) Subtype relations (S <: T) S! <: T! S? <: T? T! <: T? Dereferencing only possible with non-null type ( T! ) Possible casts: Implicit: From non-null to possibly-null ( T! nn = ...; T? pn = nn; ) Downcasts (explicit) are possible but need run-time checks ( T? pn = ...; T! nn = (T!)pn; Shortcut for (T!) : (!) ) Additional type rules (compared to Java): Expressions whose value gets dereferenced need non-null type Receiver of: field access, array access, method call Expressions of a throw statement Dataflow Analysis Check if a value at a given position in code can or can't be null Tracks values of local variables but not of objects on the heap Tracking heap locations is non-moduler Other threads could modify heap locations Object Initialization All fields are initialized to null (Java, C#, …) Invariant of non-null types is violated at beginning of constructor (it's initialized to null by default) Make sure that all non-null fields are initialized when constructor terminates Similar to Definite Assignment Rule that check that local variables are assigned before first use (Java, C#) Needes checks: Dereferces non-null fields have non-null types non-null arguments are passed non-null method parameters Not possible to check for all cases: escaping the constructor The simple Definite Assignement Rule is only sound if partly-initialized object do not escape from constructor Overly restrictive: on partly-initialzed objects Dont call methods Don't pass as argument to methods Dont's store in fields or an array Better type-system: track initialization (construction types) Initialization Phases (3 types per class/interface) free type : objects under construction (free to violate invariants, free to have null in non-null variables) committed type : object construction is completed (type of object is chaged at run-time when object is fully constructed) unclassified type : super-type of free type and committed type Type Invariant: An object is initialized if all fields have non-null values (transitively). There could also be other invariants that have to hold after object is initialized, but can be broken before the object is fully initialized. Type Rules Most type rules of Java remain unchanged Additional requirement: dereferencing needs a non-null type Receiver of field access Receiver of array access Receiver of method call Expression of a throw statement Dereferencing of a non-null type can be checked statically (compile time) Escaping constructor is an issue Combining non-null type system with construction types 6 types non-null possibly-null comitted T! T? free free T! free T? unclassified unc T! unc T? No downcasts from unclassified to free or committed (no reasonable run-time checks). Local Initialization An object is locally initialized: all non-null fields have non-null values Static type committed : locally initialized at run-time Field access: e . f Field access f: ! f: ? e: commited ! ? e: free ? ? e: unc ? ? Transitive Initialization Committed has to be transitive An object is transitively initialized if all reachable objecs are localy initialized static type committed : transitively initialized at run-time Cyclic Structures In initialization code (i.e constructor) it's allowed to assign committed types to fields of free types Type Rules Field declaration has no consturction-type modifier non-null ( ! ) or possibly-null ( ? ) modifiers are possible It's determined if it's free or committed when dereferencing ( e.f ) Field declaration is the only place in a programm that has not construction-type modifier committed is transitive! It's not allowed to have a committed and a free reference to the same object (no cross-type aliases) The free reference could assign a pointer in the object to an uninitialized field It's critical when an reference changes from free to committed Field Write A field write a . f = b is well-typed if - a and b are well-typed - a ‘ s type is a non-null type ( ! ) - b ‘ s class and non-null type conforms to a.f - a ‘ s type is free or b ‘ s type is committed type of a \\ type of b committed free unc committed ✓ ✘ ✘ free ✓ ✓ ✓ unc ✓ ✘ ✘ Field Read A field read e.f is well-typed if: e is well-typed e ‘ s type is a non-null type ( ! ) Field ( f ) has no construction-type modifier The type of e.f is: type of e \\ type of f T! T? S! T! T? free S! unc T? unc T? unc S! unc T? unc T? Consturctors Constructor signatures: each parameter has declared construction-type (default: committed ) and null-ness type this in cunstruction has implicitly: free non-null Definite assignment check for complete constructor Constructors are free by default The most permessive type for arguments in a constructor declaration is unc This should be chosen if possible Method Calls Method signatures: each parameter has declared construction-type (and null-ness type) Method signatures can contain construction-type for this Construction-type for this : String ! free getId ( String ! n ) { return ...; } Overriding requires usual co- and contravariant rules The receiver ( this ) counts as parameter Method arguments should not be declared as unclassified if possible otherwise overriding methods need to cope with unclassified too Object Construction At the end of a constructor the object is not nessecary fully initialized (i.e constructors of deriving classes are not run yet) End of a new expression: constructed object might not yet be fully constructed new expressions can have only references to committed objects outside of the new expression committed reference to the new expression is not possible new not yet finished (not committed , free ) But committed objects can't have pointers to free Nested new expression: if the ‘inner' new expression is fully (transitively) initialized reference to and from the outside the ‘outer' new expression can point to the ‘inner' new expression new Expression After ‘outer' new expression (only committed arguments) finishes all ‘inner' new expressions have references to locally initialized objects. All references inside the ‘outer' new expression point to transitively initialized objects. The type of a new expression is committed if the static types of all arguments of the constuctor are committed Otherwise it's free It's not relevant what the declared types of the constructor arguments are! It depends on the new expression It's almost not possible to create uninitialized objects Lazy Initialization Access lazy initialized field always through getter method i.e class Demo { private Vector ? data ; // possibly-null public Vector ! getData () { // getter guarantees for non-null Vectror ? d = data ; // needed for data flow analysis if ( d == null ) { d = new Vector (); data = d ; } return d ; } } Arrays // Elements // | // v Person ! []! a ; // Non-null array with non-null elements Person ? []! b ; // Non-null array with possibly-null elements Person ! []? c ; // Possibly-null array with non-null elements Person ? []? d ; // Possibly-null array with possibly-null elements (default in Java) // &#94; // | // Array Arrays have no constructors Problem: Array initialization is often done with loops Definite assignment cannot be checked by compiler Possible solutions: Array initializers ( String! []! s = {\"Array\", \"of\", \"non-null\", \"Strings\"}; ) Eiffel: pre-filling array (default objects not better than null ) Run time assert provided by programmer (Spec#) NonNullType.AssertInitialized(arr); (run time assert function) Only committed elements can be stored in array Data flow analysis knows semantic of run time assert function Run time assert function changes type from free to committed Summary Can be combined with Generics Invariant: non-nullness Avoid calling virtual methods on this in uninitialized objects (constructors, init methods) Don't let escape uninitialized objects At end of constructor: object might not yet be constructed (i.e subclass constructors) Initialization of Global Data OO -Programs have also global data i.e Flightweight-Pattern, Singleton, Caches… Requirements Must be initialized before first use (non-nullness) Handle mutual dependencies Lazy initialization C++ Global vars can have initializers Initializers are executed before main function Implicitely called by run-time system No support for lazy initialization Order of execution as in apearance in code No mechanism for dependancies (has to be donne by programmer) Java (similar in C#) Static initializer for static fields Lazy initialized No mechanism for mutual dependencies (call to not initialized reference possible) Static initializer can have side effects (no modular reasoning about initialization) Scala Language support for singletons Singletons can extend classes and traits but can't be specialized Internally translated to Java initialization Summary No solution really guarantees that global data is initialized before use No solution handles dependancies Carefull with global data (as programmer)! Reflections Program can observe and modify its structure and behavioral at run-time Simples form: RTTI (i.e for casting) Introspection Get methods, fields… from classes (or individual objects) Checks are done at run-time instead of compile-time (exceptions) Type checking Accessibility checks Accessibity / information hiding can be weakend (security issue) Helpful for debugging JUnit's test driver works with introspection Visitor pattern is much simpler with introspection Reflection API defines (and throws) checked exceptions If underlying code (i.e throws an exception it is transformed into a unchecked exception Example: public T newInstance ( ) throws InstantiationException , IllegalAccessException ; If the constructor called internally by newInstance throws an exception (even a checked exception) it gets swalowed and rethrown as an unchecked exception. Visitor Pattern (Double Invocation) can be implemented much simpler with reflections Second dynamic dispatch is implemented via reflection accept methods in element classes Flexible Not statically safe: error handling code needed Slower than ‘traditional' implementation Reflective Code Generation Examples: Java class loading Expression tree in C# Represents Abstract Syntax Tree of C# AST can be created like any other data structure at run-time Can be compiled at run-time with Compile method Generation and compilation of code at run-time is expensive but pays if generated code is called often Dynamic Code Manipulation Usually only available in dynamically typed languages (Python, Lisp…) Makes code difficult to understand Summary Very flexible (plug-ins) Serialization / persistance Design Patterns Dynamic code generation Not staticially safe! Information hiding can be compromized Hard to understand and debug Performance can be wery bad Reflection and Typechecking: Degree of Reflection Type Checking Introspection Code can be checked once, when compiled Reflective Code Generation Code can be checked once, when generated Dynamic Code Manipulation Requires typically dynamic type checking","tags":"Programming","url":"concepts_of_object_oriented_programming.html","loc":"concepts_of_object_oriented_programming.html"},{"title":"Arrays in C and C++","text":"Arrays are simple but quite powerful constructs in C/C++. They are merely a representation of consecutive data in memory. They don't contain any meta data (like size). But since they are stored in memory in such a simple manner processors can handle them very efficiently. // rows columns // | | // v v char data [ 3 ][ 2 ] = {{ 0 , 1 }, { 2 , 3 }, { 4 , 5 }}; The values of the array data are stored row for row in memory: 0,1,2,3,4,5 An other possibility to see it is that multi-dimensional arrays in C/C++ are arrays of arrays. So data is like an array (with size 3 ) of arrays of char (with size 2 ). Here is a good explanation. The same applies to more than 2 dimensions.","tags":"Programming","url":"arrays_in_c_and_cpp.html","loc":"arrays_in_c_and_cpp.html"},{"title":"ARM Cortex-M3 Architecture","text":"This page collects my notes about the Cortex-M3 architecture. In particular I use the EFM32TG840F32 processor on a STK3300 starter kit by Silicon Labs . Most information on this page is taken from the documentation by ARM . General CPU Design RISC Endianness bi-endian (little as default) Type Load/Store EFM32TG Overview Feature CPU 32 MHz ARM Cortex-M3 (r2p1) Flash 32 kB RAM 4 kB SPI 1 I2C 1 USART 2 I2S 1 Dig. Pins 56 ADC 12-bit, 8 ch, 1 Msps DAC 12-bit, 2 ch IRQs 23 LCD Yes MPU No ETM No Package QFN64 9x9 mm Registers All registers are 32-bit wide. Register Use R0 - R12 General Purpose Registers R13 Stack Pointers (SP_main, SP_process) R14 Link Register ( LR ) R15 Program Counter ( PC ) General Purpose Registers Some 16-bit Thumb instruction can only access R0 - R7. The reset values of R0 - R12 are random. Stack Pointers ( SP , R13, MSP , PSP ) The Cortex-M3 contains two stack pointers: Main Stack Pointer (SP_main): The default stack pointer, used by the operating system and exception handlers Process Stack Pointer (SP_process): Used by application code In thread mode CONTROL bit[1] indicates the used stack pointed: 0: MSP 1: PSP The two Stack Pointers are banked. Only one is visible at a time through R13. The lowest 2 bits of the stack pointers are always 0. So they are always word aligned. It's not necessary to use both stack pointers (SP_main and SP_process). Simple applications use only SP_main. PUSH and POP work with the actual SP (R13). The stack is 32-bit aligned. PUSH { R3 } ; R13 = R13-4, memory[R13] = R3 POP { R3 } ; R3 = memory[R13], R13 = R13+4 It's also possible to to push and pop multiple registers in one instruction. my_function PUSH { R0 - R3 , R3 , R7 } ; Save registers ... POP { R0 - R3 , R3 , R7 } ; Restore registers BX R7 ; Return to caller It's possible to use SP instead of R13 for accessing the actual Stack Pointer. For accessing a particular Stack Pointer the mnemonic MSP (for SP_main) or PSP (for SP_process) exist. Link Register (R14, LR ) The Link Register contains the return address of a subroutine/function. On function entry ( BL ) the return address is automatically saved on LR . main BL my_func ; call funct. with branch and link. PC=my_func, LR=next instr in main ... my_func ... BX LR ; return to address in LR Program Counter (R15, PC ) This register holds the current program position. It can be written to for controlling the program flow (jumps). But then LR is not updated. Since the Cortex-M3 has a pipelined architecture the PC can be ahead of the actual executed instruction (normally by 4). On reset, the processor loads the PC with the value of the reset vector, which is at address 0x00000004. The least-significant bit of each address loaded into PC (with BX , BLX , LDM , LDR , or POP ) must be 1 (indicating thumb mode). Otherwise an exception will occur on the Cortex-M3. Program Status Registers The special-purpose program status registers ( xPSR ) provide arithmetic and logic flags (zero and carry flag), execution status and current executing IRQ number. Bit APSR IPSR EPSR 31 N - - 30 Z - - 29 C - - 28 V - - 27 Q - - 25-26 - - ICI / IT 24 - - T 20-23 - - - 16-19 GE [3:0] - - 9-15 - - ICI / IT 0-8 - Exception Number - Application Program Status Register ( APSR ) Flags that can be set by application code (unprivileged mode). N (bit[31]) : Negative condition flag. Set if result of instruction is negative. Z (bit[30]) : Zero condition flag. Set if result of instruction is zero (0). C (bit[29]) : Carry (or borrow) condition flag. Set if instruction results in a carry condition (i.e unsigned overflow on addition) V (bit[28]) : Overflow condition flag. Set if the instruction results in a an overflow condition (i.e. signed overflow on addition) Q (bit[27]) : Set if a SSAT or USAT instruction changes the input value for the signed/unsigned range of the result (saturation). GE [3:0] (bits[19:16]) : DSP extension only. Otherwise reserved. Interrupt Program Status Register ( IPSR ) Handler Mode: This register holds the exception number of the exception that is currently processed. Thread Mode: If no exception is processed the value is zero (0). Execution Program Status Register ( EPSR ) T bit[24] : Defines the instruction set. The Cortex-M3 supports only Thumb-2. So it must be 0. An fault is caused if this bit is set to 0. ICI / IT : TBD Composite views of the xPSR registers The commands MSR and MRS can use the mnemonics APSR , IPSR , and EPSR directly or combined mnemonics for the Program Status Registers. Mnemonic Registers IAPSR IPSR and APSR EAPSR EPSR and APSR IEPSR IPSR and EPSR PSR All three xPSR registers Special-Purpose Mask Registers This registers are set to 0 at reset. They can only be written if in privileged level. PRIMASK : Disable interrupts except nonmaskable interrupt ( NMI ) and hard fault. FAULTMASK : Disable interrupts except nonmaskable interrupt ( NMI ). BASEPRI : Disable interrupts of given priority level or lower priority level. Control Register Define privileged status and select stack pointer. Control bit[0] (nPRIV, privilege level) This bit has only a meaning in thread mode. 0: Privileged Level 1: Unprivileged (user) Level In handler mode the processor operates in privileged mode. This bit is only writable with privileged level. Control bit[1] ( SPSEL , stack selection) This bit makes only sense in thread mode. 0: Use SP_process 1: Use SP_main In handler mode this bit must be 0 (SP_main is used). This bit is only writable if in thread mode with privileged level. Switching Privilege Level To switch from privileged level to user level the Control bit[0] can be written directly. The switch from user level to privilege level heeds to be performed within an exception handler. Reset Values and Required Access Privileges Name Read/Write Required privilege Reset value R0-R12 RW Both Undef MSP (R13) RW Privileged value from address 0x00000000 PSP (R13) RW Both Undef LR (R14) RW Both 0xFFFFFFFF PC (R15) RW Both value of the reset vector ASPR RW Both 0x00000000 IPSR R Privileged 0x00000000 EPSR R Privileged 0x01000000 PRIMASK RW Privileged 0x00000000 FAULTMASK RW Privileged 0x00000000 BASEPRI RW Privileged 0x00000000 CONTROL RW Privileged 0x00000000 Modes The Cortex-M3 has two modes: Thread Mode Handler Mode In Thread Mode the processor can run in two privilege levels: User Level Privileged Level In Handler Mode only the Privileged Level is available. Application Code Exception Handler Unprivileged (user) Level Thread Mode - Privileged Level Thread Mode Handler Mode Nested Vectored Interrupt Controller ( NVIC ) Nested All external and some system interrupts can be assigned to different priority levels. Current handled interrupts can only be disrupted by interrupts of higher priority. Vectored The addresses of the interrupt service routines (ISRs) are stored in a vector. If an interrupt occurs the look-up of the routine is fast and the handling of the interrupt is not delayed by look-up code. Dynamic Priority Setting The priority of an interrupt can be changed at run time. Memory Layout The Cortex-M3 has a defined memory map. So most built in peripherals are accessible by their memory address. Thus it's easy to access it in C/C++ code. Address Range Use Description 0x00000000 - 0x1FFFFFFF CODE Program Code. Exception vector table after start up. 0x20000000 - 0x3FFFFFFF SRAM Used as static RAM . 0x40000000 - 0x5FFFFFFF Peripherals For integrated peripherals. 0x60000000 - 0x9FFFFFFF External RAM For external connected RAM . 0xA0000000 - 0xDFFFFFFF External Peripherals For external connected peripherals. 0xE0000000 - 0xFFFFFFFF System NVIC , MPU , Debug… Bit-Banding Bit-Banding is a feature that allows to access certain bits individually. The access to the bits are atomically. The access to the individual bits are accomplished by mapping each bit from the bit-banding region to an address in a region called bit-banding alias. Each bit-banding region is 1 MB big. The bit-banding alias regions are 32 MB big (but it addresses in total only the 1 MB of the bit-banding region). There are two bit-band region. One for memory and the other for peripherals. Bit-Band region Bit-Band alias region Memory 0x20000000 - 0x200FFFFF 0x22000000 - 0x23FFFFF Peripherals 0x40000000 - 0x400FFFFF 0x42000000 - 0x43FFFFF To access one bit through the bit-banding alias the mapping is performed by this code: #define BIT_BANDING_ALIAS_OFFSET(byteOffsetInBitBandRegion, bitNumber) ((byteOffsetInBitBandRegion * 32) + (bitNumber * 4)) #define BIT_BANDING_MEMORY_ALIAS_BASE 0x22000000 #define BIT_BANDING_PERIPERAL_ALIAS_BASE 0x42000000 #define BIT_BANDING_MEMORY_TO_ALIAS(byte, bit) (BIT_BANDING_MEMORY_ALIAS_BASE + BIT_BANDING_ALIAS_OFFSET(byte, bit)) #define BIT_BANDING_PERIPHERAL_TO_ALIAS(byte, bit) (BIT_BANDING_PERIPHERAL_ALIAS_BASE + BIT_BANDING_ALIAS_OFFSET(byte, bit)) It's also possible to calculate the address in the other direction from the alias region to the bit-banding region. But it's usually not nessecary to do the address translation in this direction. Here is a small example: // Set pI to point to the first 32-bit value in the memory bit-banding region volatile uint32_t * pI = ( uint32_t * )( 0x20000000 ); // Set the value where pI is pointing to 0 * pI = 0 ; // Get the bit-banding alias address of bit 2 of the first byte in the bit-banding region volatile uint32_t * pIAlias = ( uint32_t * ) BIT_BANDING_MEMORY_TO_ALIAS ( 0 , 2 ); // Seet this bit to one * pIAlias = 1 ; // *pI == 4. The bit two ot *pI is now set Bus Interfaces The Cortex-M3 has multiple bus interfaces: Code Memory Bus: For fetching instructions from code memory. System Bus: For static RAM and peripherals. Private Peripheral Bus: For access to a part of the system memory and for debugging. Data Types Following data types are supported in memory: Byte: 8 bits. Half-word: 16 bits. Word: 32 bits. The registers are 32 bit wide. The instruction set supports following data types in the registers: 32-bit pointers. Unsigned and signed 32-bit integers. Unsigned 16-bit and 8-bit integers in zero-extended form. Signed 16-bit and 8-bit integers in sign-extended form. Unsigned and signed 64-bit integers held in two registers (limited direct support). Signed data is represented using two's complement format. Exceptions Exception number Exception Priority 1 Reset -3 2 NMI -2 3 Hard Fault -1 4 MemManage configurable 5 BusFault configurable 6 UsageFault configurable 7 - 10 Reserved - 11 SVCall configurable 12 DebugMonitor configurable 13 Reserved - 14 PendSV configurable 15 SysTick configurable 16 - 16+N External interrupt 0 .. N configurable Reset There are two levels of reset: Power Reset: Resets processor, System Control Space and debug logic. Local Reset: Resets processor and System Control Space (except some fault and debug resources). The exception can't be disabled. Reset exception has a fixed priority of -3 . Non Maskable Interrupt ( NMI ) NMI is the highest priority exception after reset. NMI can be generated by hardware or software can set the NMI exception to the Pending state. It can't be disabled. The fixed priority is -2 . HardFault HardFault is a generic exception. It usually means that the system is not recoverable anymore. But in some cases it might be recoverable. HardFault can't be disabled. It has a fixed priority of -1 . MemManage MemManage indicates a violation of memory access protected by the MPU (Memory Protection Unit). It can occur for data and instruction memory transactions. This fault can be disabled by software. Then a MemManage escalates to a HardFault. The priority can be configured. BusFault This are memory faults other than MemManage faults. They can occur for data and instruction transactions. They can occur synchronous or asynchronous and arise usually by errors on system buses. This fault can be disabled by software. Then a BusFault escalates to a HardFault. The priority can be configured. UsageFault UsageFaults can have different causes: Undefined instruction. Invalid state when executing instruction. Error on exception return. Attempt to access co-processor when it's unavailable (or disabled). The reporting of the following UsageFaults can be activated: Word/half-word access on unaligned memory address. Division by zero. This fault can be disabled by software. Then a UsageFault escalates to a HardFault. The priority can be configured. DebugMonitor The DebugMonitor is a fault and it's a synchronous exception. It occurs when halting debug is disabled and the DebugMonitor is enabled. The priority is configurable. A debug watch-point is asynchronous and behaves as an interrupt. SVCall This exception is used as supervisor calls it's caused by the instruction SVC . It's permanently activated. The priority is configurable. Interrupts The Cortex-M3 has two levels of system interrupts and up to 496 external interrupts (the EFM32TG has 23). System-Level Interrupts PendSV Used for system calls (Supervisor call) generated by software. An application uses a Supervisor call if it uses resources from the OS . The Supervisor call caused by PendSV executes when the processor takes the PendSV interrupt. For a synchronous Supervisor call (with application code), software uses the SVC instruction. This generates an SVCall exception. PendSV is permanently enabled, and is controlled using the ICSR . PENDSVSET and ICSR . PENDSVCLR bits. SysTick Generated by the SysTick Timer (integral part of Cortex-M3). SysTick is permanently enabled, and is controlled using the ICSR . PENDSTSET and ICSR . PENDSTCLR bits. Software can suppress hardware generation of the SysTick event. The Vector Table The Vector Table contains the reset value of SP_main and the addresses of each exception handler function. Offset in Table (32-bit words) Description 0 Reset value of SP_main Exception Number Address of exception handler The least-significant bit of each exception handler address (vector) must be 1, indicating that the exception handler is in Thumb code. The position of the Vector Table is defined by the Vector Table Offset Register ( VTOR ) . Reset Sequence At start up the Vector Table is located at memory position 0 (flash). It can later be relocated to an other memory position by software. After reset the processor does: Set main Stack Pointer Load 32-bit value from address 0x00000000 Save that value to SP_main Run start up code Read the next entry in the Vector Table (0x00000004, reset vector) Jump to that address and start running code there Since the Cortex-M4 has a full descending stack the initial stack address has to be 0x04 bigger the the beginning of the stack. Assembler Most assembler examples use the GCC Assembler (gas) . Addressing Modes addr : Absolute addressing mode (memory address of operator is given directly). %Rn : Register direct (the value given in the register is used as operator). [%Rn] : Register indirect or indexed (the value given in the register is used as address to the operator). [%Rn,#n] : Register based with offset (the address of the operand is calculated by the content of the register plus a constant). #imm : Immediate data (the operator is given directly as a constant). Rn can be any of the numbered registers. Suffixes Some instructions can have suffixes. Write Flags Suffix The Suffix S indicates that the instruction updates the flags (in APSR ) according to the result. For example ADDS R0, R0, R1; updates the flags. Conditional Suffixes This suffixes can be used for branching instructions but also for conditional execution commands. Suffix Meaning Tested Flags EQ Equal Z \\(=\\) 1 NE Not equal Z \\(=\\) 0 CS or HS Unsigned higher or same (carry set) C \\(=\\) 1 CC or LO Unsigned lower (or carry clear) C \\(=\\) 0 MI Negative (minus) N \\(=\\) 1 PL Positive or zero (plus) N \\(=\\) 0 VS Signed overflow (V set) V \\(=\\) 1 VC No signed overflow (V clear) V \\(=\\) 0 HI Unsigned higher (C \\(=\\) 1) AND (Z \\(=\\) 0) LS Unsigned lower or same (C \\(=\\) 0) OR (Z \\(=\\) 1) GE Signed greater than or equal N \\(=\\) V LT Signed less than N \\(\\neq\\) V GT Signed greater than (Z \\(=\\) 0) AND (N \\(=\\) V) LE Signed less than or equal (Z \\(=\\) 1) OR (N \\(\\neq\\) V) AL (or omitted) Always executed None In the instruction examples <c> is used to indicate that one of the conditional suffixes can be used. Instruction Width Qualifier This assembler qualifier is used to select a 16-bit or 32-bit instruction encoding. .N : Narrow: Assembler must select a 16-bit instruction. .W : Wide: Assembler must select a 32-bit instruction. If the instruction is not available in the requested encoding the assembler produces an error. If no qualifier is given the assembler selects the 16-bit encoding if it is available. In the instruction examples <q> is used to indicate that .N or .W can be used. Data Transfer Commands Move Data between Registers ( MOV , MVN ) Moves a value (or it's negated value) from one register to an other. Moving immediate value directly into a register is also possible. There are also command for moving shifted data. In this case the MOV commands are aliases for the shifting commands. Immediate MOVS < Rd > , # < imm8 > /* Outside IT bl ock */ MOV < c > < Rd > , # < imm8 > /* Inside IT bl ock */ MOV { S } < c > .W < Rd > , # < const > MOVW < c > < Rd > , # < imm16 > Register MOV < c > < Rd > , < Rm > MOVS < Rd > , < Rm > MOV { S } < c > .W < Rd > , < Rm > Examples: MOV R3 , R2 ; /* Move the value from R2 to R3 */ MVN R5 , R6 ; /* Move the negated value of R6 to R5 */ Move to top half-word of Register ( MOVT ) Moves an immediate value to the top half of the given register. The bottom half of the register is not written. MOVT < c >< q > < Rd > , # < imm16 > Move Special Register to Register ( MRS ) MRS < c > < Rd > , < sp ec_reg > MRS is a system level instruction except when accessing the APSR or CONTROL register. Move Register to Special Register ( MSR ) MSR < c > < sp ec_reg > , < Rn > MRS is a system level instruction except when accessing the APSR or CONTROL register. Arithmetic Commands Addition ( ADD ) Adds two values. Immediate ADDS < Rd > , < Rn > , # < imm3 > /* Outside IT bl ock */ ADD < c > < Rd > , < Rn > , # < imm3 > /* Inside IT bl ock */ ADDS < Rdn > , # < imm8 > /* Outside IT bl ock */ ADD < c > < Rdn > , # < imm8 > /* Inside IT bl ock */ ADD { S } < c > .W < Rd > , < Rn > , # < const > ADDW < c > < Rd > , < Rn > , # < imm12 > ADD { S } < c >< q > { < Rd > , } < Rn > , # < const > ADDW < c >< q > { < Rd > , } < Rn > , # < const > Register The second register operand can be shifted. ADDS < Rd > , < Rn > , < Rm > /* Outside IT bl ock */ ADD < c > < Rd > , < Rn > , < Rm > /* Inside IT bl ock */ ADD < c > < Rdn > , < Rm > ADD { S } < c > .W < Rd > , < Rn > , < Rm > { , < shift > } ADD { S } < c >< q > { < Rd > , } < Rn > , < Rm > { , < shift > } SP plus Immediate Adds an immediate value to the SP , writes the result to the destination register. ADD < c > < Rd > , SP , # < imm8 > ADD < c > SP , SP , # < imm7 > ADD { S } < c > .W < Rd > , SP , # < const > ADDW < c > < Rd > , SP , # < imm12 > ADD { S } < c >< q > { < Rd > , } SP , # < const > ADDW < c >< q > { < Rd > , } SP , # < const > SP plus Register Adds an (optionally-shifted) register value to the SP , writes the result to the destination register. ADD < c > < Rdm > , SP , < Rdm > ADD < c > SP , < Rm > ADD { S } < c > .W < Rd > , SP , < Rm > { , < shift > } ADD { S } < c >< q > { < Rd > , } SP , < Rm > { , < shift > } Addition with Carry ( ADC ) Adds values with carry. Immediate ADC { S } < c > < Rd > , < Rn > , # < const > Register The register operand can be shifted. ADCS < Rdn > , < Rm > /* Outside IT bl ock */ ADC < c > < Rdn > , < Rm > /* Inside IT bl ock */ ADC { S } < c > .W < Rd > , < Rn > , < Rm > { , < shift > } ADC { S } < c >< q > { < Rd > , } < Rn > , < Rm > { , < shift > } Multiply ( MUL ) Multiplies two registers. The least significant 32-bits are copied to the destination register. Can update flags. Writing flags can reduce performance! MULS < Rdm > , < Rn > , < Rdm > /* Outside IT bl ock */ MUL < c > < Rdm > , < Rn > , < Rdm > /* Inside IT bl ock */ MUL < c > < Rd > , < Rn > , < Rm > MUL { S } < c >< q > { < Rd > , } < Rn > , < Rm > Logical Commands And ( AND ) Immediate Bit-wise AND of register and immediate value. AND { S } < c > < Rd > , < Rn > , # < const > AND { S } < c >< q > { < Rd > , } < Rn > , # < const > Register Bit-wise AND of a register and a second (optionally-shifted) register. The flags can be updated based on the result. ANDS < Rdn > , < Rm > /* Outside IT bl ock */ AND < c > < Rdn > , < Rm > /* Inside IT bl ock */ AND { S } < c > .W < Rd > , < Rn > , < Rm > { , < shift > } AND { S } < c >< q > { < Rd > , } < Rn > , < Rm > { , < shift > } Bit Commands Bit Field Clear ( BFC ) Clear a number of adjacent bits in a register. BFC < c >< q > < Rd > , # < lsb > , # < width > Where: <lsb> : The least significant bit that is cleared (Range: 0-31). <width> : Number of bits to clear. Bit Field Insert ( BFI ) Insert given number of the lowest bits from source register to a given position in the destination register. BFI < c >< q >< Rd > , < Rn > , # < lsb > , # < width > <lsb> : The least significant bit in destination where bits are copied to (Range: 0-31). <width> : Number of bits to copy from source. Bit Clear ( BIC ) Immediate Performs a bit-wise AND of register and the complement of the immediate value. The flags can be updated. BIC { S } < c > < Rd > , < Rn > , # < const > BIC { S } < c >< q > { < Rd > , } < Rn > , # < const > Register Performs a bit-wise AND one register and the complement of a second register. The second register can be shifted. The flags can be updated. BICS < Rdn > , < Rm > /* Outside IT bl ock */ BIC < c > < Rdn > , < Rm > /* Inside IT bl ock */ BIC { S } < c > .W < Rd > , < Rn > , < Rm > { , < shift > } BIC { S } < c >< q > { < Rd > , } < Rn > , < Rm > { , < shift > } Count Leading Zeros ( CLZ ) Returns the number of leading zero bits of a register. CLZ < c >< q > < Rd > , < Rm > Shift and Rotate Commands Arithmetic Shift Right ( ASR ) Immediate Shifts a register right by an immediate value. Shifts in copies of it's sign bit . Can update flags. ASRS < Rd > , < Rm > , # < imm5 > /* Outside IT bl ock */ ASR < c > < Rd > , < Rm > , # < imm5 > /* Inside IT bl ock */ ASR { S } < c > .W < Rd > , < Rm > , # < imm5 > ASR { S } < c >< q > < Rd > , < Rm > , # < imm5 > Register Shifts a register by a variable number of bits, shifting in copies of it's sing flag. The number of bits to shift is read from the bottom byte of a register. Flags can be set. ASRS < Rdn > , < Rm > /* Outside IT bl ock */ ASR < c > < Rdn > , < Rm > /* Inside IT bl ock */ ASR { S } < c > .W < Rd > , < Rn > , < Rm > ASR { S } < c >< q > < Rd > , < Rn > , < Rm > Compare Commands Compare ( CMP ) Immediate Compare values by subtracting an immediate value from a register. Updates the flags but discards result. CMP < c > < Rn > , # < imm8 > CMP < c > .W < Rn > , # < const > CMP < c >< q > < Rn > , # < const > Register Compare values by subtracting an (optionally shifted) register from an other register. Updates the flags but discards result. CMP < c > < Rn > , < Rm > /* < Rn > and < Rm > both from R0 - R7 */ CMP < c > .W < Rn > , < Rm > { , < shift > } CMP < c >< q > < Rn > , < Rm > { , < shift > } Compare Negative ( CMN ) Immediate Compare values by adding a register and an immediate value. Updates the flags but discards result. CMN < c > < Rn > , # < const > CMN < c >< q > < Rn > , # < const > Register Compare values by adding to registers. The second register can be shifted. Updates the flags but discards result. CMN < c > < Rn > , < Rm > CMN < c > .W < Rn > , < Rm > { , < shift > } CMN < c >< q > < Rn > , < Rm > { , < shift > } Branch Commands B : Branch (immediate) BL : Branch with link (immediate) BX : Branch indirect (register) BLX : Branch indirect with link (register) All these branch to a label, or to an address given by the operand. In addition: The BL and BLX instructions write the address of the next instruction to LR . The BX and BLX instructions can change the instruction set ( ARM ↔ Thumb). The BX and BLX instructions result in a UsageFault exception on the M3 if bit[0] of the target address is 0. B is the only conditional instruction that can be inside or outside an IT block. All other branch instructions can only be conditional inside an IT block, and are always unconditional otherwise. Branch ( B ) Branch to a target addressed. B < c > < label > B < c > .W < label > B < c >< q > < label > Branch with Link ( BL ) Calls a function at PC -relative address. BL < c >< q > < label > <label> : The label to jump to. The assembler calculates the offset of the BL instruction and the label. Branch and Exchange ( BX ) Calls a function at an address and instruction set ( ARM /Thumb) specified in a register. It's the same as BXL but it doesn't save the next instruction in LR . Be aware that the Cortex-M3 only supports Thumb! (see BLX ) BX < c > < Rm > /* Outside or last in IT bl ock */ BX < c >< q > < Rm > Exceptions: UsageFault Branch with Link and Exchange ( BLX ) Calls a function at an address and instruction set ( ARM /Thumb) specified in a register. It saves the next instruction (after BLX ) in LR . Be aware that the Cortex-M3 only supports Thumb! An attempt to change to ARM Mode will cause an UsageFault exception. BLX < c > < Rm > /* Outside or last in IT bl ock */ BLX < c >< q > < Rm > Exceptions: UsageFault Compare and Branch Commands Compare and Branch on Non-Zero and Compare and Branch on Zero ( CBNZ , CBZ ) Compares the value in register with zero, and conditionally branches forward a constant value. The condition flags are not affected. these instructions are not allowed inside an IT block. CB { N } Z < Rn > , < label > /* Not permitted in IT bl ock */ CB { N } Z < q > < Rn > , < label > <Rn> : Register must be in range R0 - R7 . <label> : The assembler calculates the offset from the CB{N}Z instruction to the label. Permitted offsets are even numbers in the range 0 to 126. Other Commands Calculate Address ( ADR ) Calculate PC relative address. Add immediate value to PC and store result in register. ADR < c > < Rd > , < label > ADR < c > .W < Rd > , < label > /* < label > before current instruction */ SUB < Rd > , PC , # 0 . /* Sp ecial case for zero offset */ ADR < c >< q > < Rd > , < label > ADD < c >< q > < Rd > , PC , # < const > SUB < c >< q > < Rd > , PC , # < const > /* Sp ecial case */ Break-point ( BKPT ) Causes a DebugMonitor exception or a debug halt. It is a unconditional instruction and can be executed inside or outside an TI block. BKPT < q > # < imm8 > <imm8> is a 8-bit value that is ignored by the hardware but can be used to store some information by a debugger. Exceptions: DebugMonitor Debug Hint ( DBG ) Provides a hint to debug and trace systems. The debug architecture defines the use (if any). DB G < c >< q > # < option > // Any decoding of 'option' is sp ecified by the debug system No Operation ( NOP ) The NOP does nothing. NOP < c > NOP < c > .W","tags":"Programming","url":"arm_cortex-m3_architecture.html","loc":"arm_cortex-m3_architecture.html"},{"title":"Intel Architecture","text":"On this page I write down some notes about the Intel architecture (x86). I learned most of it in school few years ago. Part of the notes here are for Intel 80186. But some sections are extend with information about modern Intel processors ( IA -32 , x86-64 ). Some information (especially about x86-64) is taken from x86-64 Assembly Language Programming with Ubuntu by Ed Jorgensen. I'm trying to keep all code examples in NASM syntax. There is a good overview of the x86 instructions on Wikipedia. I keep some examples on GitHub . General CPU Design CISC Endianness little Type Register-memory Operation Modes Mode Introduced in Real Mode 8086 Protected Mode 80286 Virtual 8086 mode 80386 Long Mode x86-64 Memory Models The memory models define how data and code is manged in memory. Most information in this section is from Calling conventions for different C++ compilers and operating systems . There is also a good Wikipedia page . The memory is byte addressable. Data is stored in little endian format. This means that the least significant byte ( LSB ) is saved on the smallest memory address. Real Mode Memory Models (16-bit) This memory models are used in DOS for example. Tiny Code and data in the same segment (64 kB). Code starts at 0x100 relative to segment. Executable has ending .com (instead of .exe ). Small One segment for code and one segment for data and stack. Both segments have max. size of 64 kB. Medium The code can exceed 64 kB (multiple segments). Far function calls are needed. One segment (of max. 64 kB) for data and stack. Compact Code is limited to one segment (64 kB). Stack is limited to one segment (64 kB). Data can exceed 64 kB. Far pointers are needed for data. Large Code can exceed 64 kB. Data can exceed 64 kB. Stack is limited to one segment (64 kB). Far pointers are needed for code and data. Huge Same as large. A data structure can exceed 64 kB by modifying segment and offset when a pointer is incremented. Protected Mode Memory Models (16-bit) Win 3.x uses Protected Mode and similar memory models as in Real Mode. Segment registers contain Segment Selectors instead of physical addresses. To access data structures bigger than 64 kB the 8 has to be added to the segment descriptor for each 64 kB increment. On a 32-bit processor a 32-bit offset is used. 32-bit Memory Models 32-bit OS 's (Windows, Linux, BSD , Intel-Mac) use the Flat memory model. Application code uses only one (max. 2 GB ) segment. Pointers are 32-bit signed addresses. Negative addresses are reseved for kernel and drivers. 64-bit Memory Models Windows The size of code and static data together is limited to 2 GB . So it's possible to use RIP -relative addresses. The image base of an executable binary is usually below \\(2&#94;{31}\\) . Absolute 32-bit addresses are not often used. Stack and dynamically allocated data (data on heap) can exceed 2 GB . Pointers are usually 64 bits (sometimes 32 bits). Negative addresses are reserved for the kernel. Linux and BSD Small Code an static data is limited to 2 GB and stored at addresses below \\(2&#94;{31}\\) . The compiler can use absolte signed 32-bit addresses. Stack and dynamically allocated data can exceed 2 GB . Pointers are 64 bits. Default memory model in Linux (x64) and BSD . Medium Static data bigger than the ‘large-data-threshold' is stored in a data section that can exceed 2 GB . Code and smaller static data are limited to addresses below \\(2&#94;{31}\\) . Large Code and data can exceed 2 GB . Addresses are 64 bits. Kernel Used to compile the kernel and device drivers. Addresses must be negative between \\(-2&#94;{31}\\) and \\(0\\) . OS X (Darwin) The default memory model of the intel-based darwin kernel limits code and static data together to 2 GB . So 32-bit RIP -relative addresses can be used. Code is loaded to addresses above \\(2&#94;{32}\\) by default. Addresses below \\(2&#94;{32}\\) are blocked (pagezero). Stack and dynamically allocated data can exceed 2 GB . Pointers are 64 bits. Pointer tables can use 32-bit signed addresses relative to any reference point. Certain system functions can be accessed in the commpage . It's possible to reduce the size of pagezero to place code below \\(2&#94;{31}\\) so that absolute 32-bit addresses can be used. Registers General-Purpose Registers 64-bit 32-bit 16-bit 8-bit Purpose Notes rax eax ax al General-Purpose Register ( GPR ) Accumulator for IN / OUT ( AX or AL ). Can be used as 8-bit registers ( AH / AL ). rbx ebx bx bl General-Purpose Register ( GPR ) Base index (array). Can be used as 8-bit registers ( BH / BL ). rcx ecx cx cl General-Purpose Register ( GPR ) Only register that can be used for LOOP . Can be used as 8-bit registers ( CH / CL ). rdx edx dx dl General-Purpose Register ( GPR ) Needs to contain port address for IN / OUT . Extend precision of accumulator. Can be used as 8-bit registers ( DH / DL ). rdi edi di dil Destination Index Destination for string operations. rsi esi si sil Source Index Source for string operations. rbp ebp bp bpl Base Pointer Often used as Frame Pointer (pointing to current stack frame). rsp esp sp spl Stack Pointer Points to the top of the stack. r8-r15 r8d-r15d r8w-r15w r8w-r15w GPRs x86-64 adds new GPRs. The first four GPRs can be accessed as two 8 bit registers. i. e: bx's high byte can be accessed as bh and low byte as bl . Segment Registers These registers are used in real mode and protected mode for memory segmentation. Register Purpose cs Code Segment ds Data Segment ss Stack Segment es Extra Segment Special Registers Register Purpose Notes rFlags Status Register Carry Flag, Overflow Flag, Zero flag… rip Instruction Pointer Points to the next instruction (cannot be directly accessed) Values after Reset Register Value IP 0x0000 CS 0xffff DS 0x0000 ES 0x0000 SS 0x0000 All other registers have a random value after reset. XMM Registes There are registers for 64-bit and 32-bit floating point operations, for single Instruction Multiple Data ( SIMD ) and SSE . There are 16 XMM registers with a size of 128 bits. They are called xmm0-xmm15. Flags Register Bit Mnemonic Meaning 15 - Reserved 14 NT Nested Task Flag (286+) 13 and 12 IOPL I/O Privilege Level (286+) 11 OF Overflow Flag 10 DF Direction Flag 9 IF Interrupt Enable Flag 8 TF Trap Flag (single step) 7 SF Sign Flag 6 ZF Zero Flag 5 - Reserved 4 AF Adjust Flag 3 - Reserved 2 PF Parity Flag 1 - Reserved 0 CF Carry Flag Segmentation To allow access to 20-bit addresses with 16-bit registers the 8086 uses segmentation. $$physical\\_ address = segment\\_ register \\times 10_{hex} + offset$$ \\(\\times 10_{hex}\\) means a 4-bit shift to left Each addressable segment is 64 kB big. 20-bit address bus: Total \\(2&#94;{20}\\) bytes addressable (1‘048‘576 bytes = 1 MB ). 16-bit offset: \\(2&#94;{16}\\) bytes addressable (65‘536 bytes = 64 kB) per segment. Needed segments: \\(2&#94;{20}/2&#94;{16} = 2&#94;4 = 16\\) segments needed to access complete address space. Segments can overlap or there can be gaps between them. Addressing Intel processors have 5 different addressing modes. Immediate The operand (constant) is given with the command. i.e: MOV CL, 42 ; move the value 42 to register CL Implicit Some commands work always with the same register/address. i.e: PUSH / POP work always with SP register. Register The operand is held in a register INC CH ; Increment value in CH register Direct The address of the operand value comes directly after the command. i. e: MOV CX, counter ; counter holds the address of the value Register-Indirect The operand is given indirectly by one or two registers. A segment register and a constant offset value can be supplied. The calculated value acts as a pointer (address to a memory location). i.e: MOV [BX + DI] , CH; calculate the operand with the values from BX and DI Address Real Mode To calculate an address in processors with segmentation the following scheme is used: $$Offset := \\begin{Bmatrix}-\\\\CS:\\\\DS:\\\\SS:\\\\ES:\\end{Bmatrix}\\begin{Bmatrix}-\\\\BX\\\\BP\\end{Bmatrix} +\\begin{Bmatrix}-\\\\SI\\\\DI\\end{Bmatrix} + \\begin{Bmatrix}-\\\\displacement_8\\\\displacement_{16}\\end{Bmatrix}$$ \\(-\\) means that this element is not used. The three possible address parts are: Basis Register ( BX or BP ): Contains usually the start address of a data structure. A segment prefix can be given. Index Register ( SI or DI ): Can contain an index (i.e Array index) that can be calculated at run-time. It's 16-bit unsigned. Displacement: A signed constant value (8-bit or 16-bit) that gives an offset. This addressing scheme gives a total of 27 addressing combinations. But only 24 combinations are allowed. The following three are not allowed : No address at all: MOV AX, []; or MOV AX, ; Only 8-bit displacement: Only memory 0-255 could be addressed. Only BP : BP points to stack. No practical use. MOV AX, [BP]; Segment Prefix A segment prefix ( CS : , DS : , ES : or SS : ) defines which segment register will be used for calculating the address. Default for most registers is DS . But for BP the default is SS . x86-64 The general for calculating a memory address is: $$[baseAddress + (indexRegister \\cdot scaleValue) + displacement]$$ Where: baseAddress : any GP register or variable name indexRegister : any GP register scaleValue : immediate value of 1 , 2 , 4 or 8 ( 1 does nothing) displacement : 8-bit or 32-bit constant Examples: mov eax , dword [ var ] mov rax , qword [ rbx + rsi ] mov ax , word [ lst + 4 ] mov bx , word [ lst + rdx + 2 ] mov rcx , qword [ lst + ( rsi * 8 )] mov al , byte [ buffer - 1 + rcx ] mov eax , dword [ rbx + ( rsi * 4 ) + 16 ] Because addresses are always of 64-bit size ( qword ), a 64-bit register is needed for memory addressing. Even when accessing smaller sized values. Operand Size ( WORD , DWORD …) In some cases the size of an operand can be given (for some cases it is even mandatory). Size types: BYTE , WORD , DWORD , QWORD , TBYTE , FAR … Even if the operand size is not mandatory it's good programming practice to incule it. Examples MOV DX , [ BX ] ; MOV AL , [ BX + 4 ] ; MOV CX , [ CS : BX + SI ] ; MOV ES , [ BX + DI + 2 ] ; MOV WORD [ ES : BX + DI + 8 ], AX ; Addressing Memory For addressing the memory the immediate, direct and indirect method can be used. A variable name without brackets is used to get the address of the variable. With brackets the value stored in the variable is taken. mov rax , qword [ var1 ] ; value of var1 in rax mov rax , var1 ; address of var1 in rax Data Transfer Commands Move Command ( MOV ) Moves (copies) a value from a source to a destination. MOV dest , src dest can be a memory variable or a register (but not CS or IP ) and not an immediate. src can be a memory variable, a register or a constant. Only one memory operand can be used. Then the other one needs to be a register or a constant. destination and source operands must be of the same size. For double-word destination and source operands the upper part of the quad-word destination register is set to 0! Example: qFirstVal dq 0xffffffffffffffff ; inital 64-bit value dVar32 dd 0xabcdefab ; 32-bit value wVar16 dw 0xbdbd ; 16-bit value ; 16-bit example mov rax , qword [ qFirstVal ] ; initialize rax mov ax , word [ wVar16 ] ; write lowest 16 bit ; now rax has value 0xffffffffffffbdbd: the upper part of the register is kept ; 32-bit example mov rax , qword [ qFirstVal ] ; initialize rax mov eax , dword [ dVar32 ] ; write lowest 32 bit ; now rax has value 0x00000000abcdefab: the upper part of the register is cleared! Exchange Command ( XCHG ) Exchanges the values of the two operands (memory/registers). XCHG op1 , op2 Addressing memory/register is same as with MOV . Segment Register and Immediate addressing is not possible. Input-/Output Commands ( IN / OUT ) For reading and writing data to/from ports Input/Output can only be done with accumulator register ( AX / AL ). Port address needs to be written to DX before calling the IN -/ OUT -Command. As special case a 8-bit port address can be given directly. IN AL , DX ; OUT DX , AX ; IN AX , 42h ; OUT 16h , AL ; It's not possible communicate directly between memory and ports. For this a DMA (Direct Memory Access) Hardware would be needed. Load Effective Address ( LEA ) Loads an address: lea < reg64 > , < mem > ; address of <mem> in reg64 Examples: lea rcx , byte [ bvar ] lea rsi , dword [ dVar ] Conversion Instructions Narrowing Conversions No special instructions are needed mov instruction is used Programmer is responsible that narrowing conversions are sane The instruction will just strip upper part of register or variable Widening Conversions Upper-order bit (sign) must be set based on original value Unsigned Conversions ( MOVZX ) Upper part of register or memory location must be set to zero Instruction movzx can be used movzx does not allow a quad-word destination with double-word source operand A mov with a double-word destination register and with double-word source operand will zero out the upper double-word of the quad-word destination register (s.a. MOV ) Only one memory operand is allowed Destination can not be an immediate Signed Conversions ( CBW , CWD , …) Widening conversion for singed values need adjustment of the upper order bits This is needed to keep the two's complement format The upper order bits must be set to 0's or 1's depending if original value was negative or positive There are general instructions: movsx and movsxd Only one operand can be memory Destination can not be immediate movsxd required for 32-bit to 64-bit extension There are special instructions that convert values in a register : convert byte to word cbw , convert word to double-word cwd , … These work only on the A register sometimes using D register for result Instructions movsx and movsxd : movsx < dest > , < src > movsxd < dest > , < src > Special instructions: Instruction Source Size Implicit Source Destination Size Implicit destination cbw byte al word ax cwd word ax double-word dx:ax cwde word ax double-word eax cdq double-word eax quadword edx:eax cdqe double-word eax quad-word rax cqo quadword rax double-quadword rdx:rax Arithmetic Commands The result of arithmetic commands is written to the first operand (the original value is lost). Both operands (of binary arithmetic commands) need to be of same size. Only one operand is allowed to be a memory operand. Addition ( ADD ) Adds the two operands and writes the result into the first one. The first operand can not be a constant (immediate) Both operands need to be of same size Only one operand can be memory Affected Flags Carry: with unsigned operands Overflow: with signed operands Zero: if result is zero Sign: if signed result is negative Parity: if parity is even Addition with Carry ( ADC ) Adds the two operands and the carry flag. The result is written into the first operand. The first operand can not be a constant (immediate) Both operands need to be of same size Only one operand can be memory The adc instruction should directly follow an inital add instruction otherwise the carry bit can be lost Addition of big signed operands can be splitted into several ADC commands. Affected Flags Carry: with unsigned operands Overflow: with signed operands Zero: if result is zero Sign: if signed result is negative Parity: if parity is even Increment ( INC ) Adds one to the operand. The result is saved in the given operand. The operand can not be an immediate Affected Flags Overflow: with signed operands Zero: if result is zero Sign: if signed result is negative Parity: if parity is even The carry-flag is not affected! An overflow can be recognized with the zero-flag. INC can be used to increment a control variable in a loop without affecting the carry-flag. Subtraction ( SUB ) Subtracts the second operand from the first. The result is written into the first operand. The first operand can not be a constant (immediate) Only one operand can be memory Subtraction work same for signed and unsigned data Affected Flags Carry: with unsigned operands Overflow: with signed operands Zero: if result is zero Sign: if signed result is negative Parity: if parity is even Subtraction with Borrow ( SBB ) Subtracts the second operand and the carry-flag (borrow) from the first operand. The result is written into the first operand. The first operand can not be a constant. Affected Flags Carry: with unsigned operands Overflow: with signed operands Zero: if result is zero Sign: if signed result is negative Parity: if parity is even Subtraction of big signed operands can be splitted into several SBB commands. Decrement ( DEC ) Subtracts one from the given operand. Affected Flags Overflow: with signed operands Zero: if result is zero Sign: if signed result is negative Parity: if parity is even The carry-flag is not affected! An overflow can be only recognized with checking the result for 0xFF. DEC can be used to decrement a control variable in a loop without affecting the carry-flag. Negate a signed Number ( NEG ) Changes a negative into a positive number and vice versa. It's basically subtracting the operand from zero (0). Affected Flags Carry: set if operand wasn't zero (not very useful) Overflow: if no positive representation exist (operand was biggest negative number) Zero: if result is zero Sign: if signed result is negative Parity: if parity is even Multiplication ( MUL , IMUL ) Multiplicates unsigned ( MUL ) or signed ( IMUL ) numbers. There is a explicit operand given after the command and an implicit operand in the A ( AL , AX , ..) register. The explicit operand sets the size and defines the used implicit register. It can be either a register or a memory location but not an immediate. The result is always twice as big as the operands. It's either the accumulator ( AX ) or the extended accumulator ( DX / AX ). MUL 0x0 ; Use AL as implicit operand. Result is saved in AX. IMUL BX ; Use AX as implicit operand. Result is saved in DX/AX. Sizes ( mul ): Size Registers Byte ax = al * <src> Word dx:ax = ax * <src> Double-word edx:eax = eax * <src> Quad-word rdx:rax = rax * <src> imul allows more operands: imul < source > imul < dest > , < src / imm > imul < dest > , < src > , < imm > For single operand (same as mul ): Size Registers Byte ax = al * <src> Word dx:ax = ax * <src> Double-word edx:eax = eax * <src> Quad-word rdx:rax = rax * <src> Note: <src> operand can not be immediate For two operands: <reg16> = <reg16> * <op16/imm> <reg32> = <reg32> * <op32/imm> <reg64> = <reg64> * <op64/imm> For three operands: <reg16> = <op16> * <imm> <reg32> = <op32> * <imm> <reg64> = <op64> * <imm> Affected Flags Carry: set if operand extended accumulator is needed for saving result ( DX / AX ) Zero: changed (undefined) Sign: changed (undefined) Parity: changed (undefined) The 8086 can not multiply with constants (immediate). Division ( DIV , IDIV ) There are different division operations for unsigned ( DIV ) and signed ( IDIV ) numbers. $$quotient = \\frac{dividend}{divisor}$$ The explicit operand (given directly after the command) defines the size of the operands. The dividend must be larger than the divisor. Setting the dividend correctly requires often to set two registers ( D register for the upper part, A register for the lower part). For idiv singed conversion of the operand might be necessary. The operand can be a register or a memory location but not immediate. DIV BX ; Use DX/AX as implicit operand. Result is saved in AX. Remainder is saved in DX. IDIV 0x34 ; Use AX as implicit operand. Result is saved in AL. Remainder is saved in AH. Unsigned and signed division ( div , idiv ): Size Registers Byte al = ax / <src> , remainder in ah Word ax = dx:ax / <src> , remainder in dx Double-word eax = edx:eax / <src> , remainder in edx Quad-word rax = rdx:rax / <src> , remainder in rdx Don't divide by zero! Affected Flags All flags are changed to undefined ! If the result is too big for the register AL resp. AX a interrupt ( division error ) is caused. If it is not handled the program can crash (undefined behaviour). Floating Point Instructions There are special instructions for floating point numbers. Here are the most important x86-64 floating point instructions shown. They differ from the 32-bit floating point instructions. Values 32-bit: single precision ( float in C/C++) 64-bit: double precision ( double in C/C++) Single precision instuctions have a s postfix and double precision instuctions have a d postfix. Registers There are 16 XMM registers xmm0 - xmm15 to be used for floating point instructions. They are 128 bits (on later processors 256 bits) long. Data Transfer Commands ( movss , movsd ) Copies a value to or from a XMM register. movss dest , src movsd dest , src Only one operand can be memory Operands can not be an immediate Conversion Instructions Conversions float (32-bit) double (64-bit) integer (32 bit) float (32-bit) - cvtss2sd cvtss2si double (64-bit) cvtsd2ss - cvtsd2si integer (32 bit) cvtsi2ss cvtsi2sd - Arithmetic Instructions Addition ( addss , addsd ) addss dest , src addsd dest , src Destination operands must be an XMM register Source operand can't be an immediate Substraction ( subss , subsd ) subss dest , src subsd dest , src Destination operands must be an XMM register Source operand can't be an immediate Multiplication ( mulss , mulsd ) mulss dest , src mulsd dest , src Destination operands must be an XMM register Source operand can't be an immediate Division ( divss , divsd ) divss dest , src divsd dest , src Destination operands must be an XMM register Source operand can't be an immediate Square Root ( sqrtss , sqrtsd ) sqrtss dest , src sqrtsd dest , src Destination operands must be an XMM register Source operand can't be an immediate Control Instructions There are ordered and unordered floating point compare instructions. Here only the unordered floating point instructions shown. ucomiss op1 , op2 ucomisd op1 , op2 Operands are not changed op1 must be an XMM register op2 may be XMM register or memory No immediate operands are allowed Logical Commands And, Or and Xor ( AND , OR , XOR ) Bit-wise and , or or xor operation. The first operand can be a register or a memory address The second operand can be a register, a memory address or a constant Only one operand can be memory The first operand is overwritten with the result. Affected Flags Carry and Overflow: are always reset (0) Zero and Sign: are set according to the result Not ( NOT ) Bit-wise not (inverse) operation. The operand can be a register or a memory address but not an immediate. The operand is overwritten with the result. Affected Flags No flags are changed Rotation Commands Rotate ( ROL , ROR ) Rotate left ( ROL ) or right ( ROR ). Rotates the first operand (memory or register) by the constant 1 (immediate) or by the value given in CL . ROL 0x34 , 1 ; Rotate by one. ROR AX , CL ; Rotate by value in CL. Affected Flags Overflow: For one-bit rotation: Overflow set if MSB is changed by the rotation. Otherwise it's not set. For other cases: Overflow is undefined Carry has the value of the bit that was shifted from one end to the other. Rotate with Carry ( RCL , RCR ) Rotate left ( RCL ) or right ( RCR ) with carry as MSB . The first operand (memory or register) is extended with the carry bit as the MSB . It is then rotated by the constant 1 (immediate) or by the value given in CL . Affected Flags Overflow: For one-bit rotation: Overflow set if MSB is changed by the rotation. Otherwise it's not set. For other cases: Overflow is undefined Carry has the value of the bit that was shifted from one end to the other. Manipulate the Carry Flag ( CLC , STC , CMC ) Clear Carry Flag ( CLC ): CF = 0. Set Carry Flag ( STC ): CF = 1. Complement Carry Flag ( CMC ): CF = ! CF . Shift Commands The shift commands can be used to multiply with or divide by a power of two (2, 5, 8, …). For a multiplication or a division of a unsigned number the logical shift operators need to be used. For a multiplication or a division of a signed number the arithmetic shift operators need to be used. Direction Logical Arithmetic Notes Left shl sal Both instructions are identical Right shr sar Instructions work diffently Logical shift (unsigned shift): Spaces filled with zero Arithmetic shift (signed shift): Spaces filled so that sign is preserved Right: Spaces are filled with sign bit. Left: Spaces are filled with zero (doesn't affect sign), thus same as logical shift Left Shift Commands ( SHL , SAL ) Both left shift operators are functional identical. They muliplicate the operator by 2 or by \\(2&#94;{CL}\\) . Shifts the given operator (memory or register) left by the constant 1 (immediate) or by the value given in CL . Affected Flags Zero Sign Offset For one-bit shift: Carry: For unsigned Operands a set carry flag means overflow Overflow: For signed Operands a set overflow flag means overflow Right Shift Commands ( SHR , SAR ) The logical shift right ( SHR ) divides a unsigned value by 2 or by \\(2&#94;{CL}\\) . The arithmetic shift right ( SHR ) divides a unsigned value by 2 or by \\(2&#94;{CL}\\) . The sign stays unchanged. The two right shift instructions are not equivalent (not as the two left shift instructions). The first operand is a memory location or a register. The second operand is the constant 1 or the register CL . Affected Flags CF : For one-bit shifts the carry flag holds the remainder of the division ZF SF ( OF ) Jump Commands Jump commands can be divided by different topics. Conditional/Unconditional Jumps Unconditional jumps are always performed. Conditional jumps are only performed if a check gives an expected result (i.e flags). Short, Near and Far Jumps Short: signed 8-bit distance Near: unsigned 16-bit distance Far: 32-bit (Segment and Offset) Direct and Indirect Jumps Direct: The target address is given directly after the command Indirect: The position is given indirectly by a register or a memory position Absolute and Relative Jumps Absolute jumps: The target address is an absolute address Relative jumps: The target address is given relative to the actual position ( IP ) The 8086 has following possible jump commands: Intra- and Inter-Segment Jumps All jumps change the IP register. Far jumps also change the CS register. Intra-Segment Jump Short- and Near-Jumps change only the IP register. The target is always inside the actual code segment ( CS ). Near Jump Direct Near-Jumps are always relative to IP . Indirect Near-Jumps are always absolute to the actual code segment. Short Jump Short Jumps are always relative to IP . Inter-Segment Jump Far jumps change CS and IP . All far jumps are absolute. Unconditional Jumps ( JMP ) 8-bit displacement is added to IP as signed number: JMP displ8 16-bit displacement is added to IP as unsigned number: JMP displ16 The constant is the absolute 32-bit FAR -addressed: JMP const32 IP is loaded with the value of the register: JMP reg16 IP is loaded with the value given by the memory position: JMP mem16 CS and IP are loaded with the value at the memory position: JMP mem32 The unconditional jump are not limited in range. Conditional Jumps Conditional jumps check one or more flags and jump to a given address if a condition is met. Before the jump can be performed the flags need to be set by a logical or arithmetic command. Alternatively the commands CMP and TEST can be used. Conditional jumps can be divided into following groups: Arithmetic jumps: The jump depends on size difference of two operands. The two operands have to be compared in advance. One or more flags have to be checked. Flag oriented jumps: A jump is performed if one given flag is set or deleted. Arithmetic Jumps The jump is performed if the size relation between two given operands is as expected. The relation is expressed differently for signed and unsigned operands: Relation unsigned signed \\(=\\) equal equal \\(<\\) below less \\(>\\) above greater Arithmetic \"unsigned\" Jumps Relation Command Explanation Condition \\(=\\) JE Jump if equal ZF \\(=\\) 1 \\(\\neq\\) JNE Jump if not equal ZF \\(=\\) 0 \\(<\\) JB Jump if below CF \\(=\\) 1 \\(\\ngeq\\) JNAE Jump if not above or equal CF \\(=\\) 1 \\(\\ge\\) JAE Jump if above or equal CF \\(=\\) 0 \\(\\not<\\) JNB Jump if not below CF \\(=\\) 0 \\(\\le\\) JBE Jump if below or equal CF \\(=\\) 1 or ZF \\(=\\) 1 \\(\\not>\\) JNA Jump if not above CF \\(=\\) 1 or ZF \\(=\\) 1 \\(>\\) JA Jump if above CF \\(=\\) 0 and ZF \\(=\\) 0 \\(\\not\\le\\) JNBE Jump if not below or equal CF \\(=\\) 0 and ZF \\(=\\) 0 Arithmetic \"signed\" Jumps Relation Command Explanation Condition \\(=\\) JE Jump if equal ZF \\(=\\) 1 \\(\\neq\\) JNE Jump if not equal ZF \\(=\\) 0 \\(<\\) JL Jump if less OF \\(\\neq\\) SF \\(\\ngeq\\) JNGE Jump if not greater or equal OF \\(\\neq\\) SF \\(\\ge\\) JGE Jump if greater or equal OF \\(=\\) SF \\(\\not<\\) JNL Jump if not less OF \\(=\\) SF \\(\\le\\) JLE Jump if less or equal OF \\(\\neq\\) SF or ZF \\(=\\) 1 \\(\\not>\\) JNG Jump if not greater OF \\(\\neq\\) SF or ZF \\(=\\) 1 \\(>\\) JG Jump if greater OF \\(=\\) SF and ZF \\(=\\) 0 \\(\\not\\le\\) JNLE Jump if not less or equal OF \\(=\\) SF and ZF \\(=\\) 0 Flag oriented Jumps Command Explanation Condition JZ Jump if zero ZF \\(=\\) 1 JNZ Jump if not zero ZF \\(=\\) 0 JC Jump if carry CF \\(=\\) 1 JNC Jump if no carry CF \\(=\\) 0 JS Jump if sign SF \\(=\\) 1 JNS Jump if no sign SF \\(=\\) 0 JO Jump if overflow OF \\(=\\) 1 JNO Jump if no overflow OF \\(=\\) 0 JP Jump if parity PF \\(=\\) 1 JNP Jump if no parity PF \\(=\\) 0 JPE Jump if parity even PF \\(=\\) 1 JPO Jump if parity odd PF \\(=\\) 0 Comparing Commands ( CMP , TEST ) Since the different jump commands depend on flags set or not there are special commands that only affect the flags. CMP is a subtraction of the operands (8-bit or 16-bit) but it only changes the flags. The result is not written anywhere. TEST is a AND operation of the operands (8-bit or 16-bit) that only changes the flags. The result is not written anywhere. Both commands accept a register or a memory location as first operand and a register, a memory location or a constant (immediate) as second operator. Only one memory operand is allowed. The operands need to be of the same size. Loop Commands ( LOOPx , JCXZ ) All loop commands accept an displacement operator (8-bit). None of the loop commands affects any flags! Loop ( LOOP ) Decrements RCX by one (1). If RCX is not zero ( RCX \\(\\neq\\) 0) it performs the jump. Loop while equal and Loop while zero ( LOOPE , LOOPZ ) LOOPE and LOOPZ are different mnemonics for the same command. Decrements CX by one (1). Performs the jump if if Zero Flag is set and CX is not zero (0). Jump if: ZF \\(=\\) 1 and CX \\(\\neq\\) 0 Loop while not equal and Loop while not zero ( LOOPNE , LOOPNZ ) LOOPNE and LOOPNZ are different mnemonics for the same command. Decrements CX by one (1). Performs the jump if if Zero Flag is not set and CX is not zero (0). Jump if: ZF \\(=\\) 0 and CX \\(\\neq\\) 0 Jump if CX zero ( JCXZ ) Performs the jump if CX is zero. Jump if: CX \\(=\\) 0 Stack and Function calls The stack on x86 is always addressed by the Stack Segment ( SS ). On 80186 stack operations are word aligned (16-bit). So PUSH decrements SP by 2 and POP increments SP by 2 . On x86-64 stack operations are quad-wor (64-bit) aligned. So PUSH decrements RSP by 8 and POP increments RSP by 8 . It's possible to use pop and push instructions for operands smaller than 64-bits. But it's not recomended. The stack on x86 grow downwards (reverse in memory). RSP points to the last written item. It is the top of the stack (smallest address on stack). Push and Pop ( PUSH , PUSHF , PUSHA , POP , POPF , POPA ) The different push and pop commands save/restore 16-bit words to/from the stack. PUSH PUSH can be called with all registers as as operands or a memory operand. Immediate addressing is not possible with PUSH . The registers that can be pushed are: AX , BX , CX , DX , SP , BP , SI , DI , ES , SS , DS and CS . POP POP can use the same operands as PUSH with the exception of CS . Memory operands are also possible. The registers that can be poped are: AX , BX , CX , DX , SP , BP , SI , DI , ES , SS and DS . PUSHA and POPA With the commands PUSHA and POPA (introduced with 80186) the 8 working registers are pushed to the stack and poped in the reversed order: AX , CX , DX , BX , SP + , BP , SI , DI SP + is the SP before the first push to the stack. With POPA SP is not poped. Is is just decremented (by 2) at the end of all the pop operations. PUSHF and POPF PUSHF and POPF push and pop the flag register to/from the stack. Examples PUSH result ; variable result PUSH [ BX + 7 ] ; memory word at address [BX+7] PUSH tab [ SI ] ; memory word at address tab[SI] POP AX ; value from stack to AX CALL and RET CALL stores the return address (address of the instruction after CALL ) on the stack, increments SP (by 2) and calls the function. RET returns from the function by loading the stored address in to IP and decrements SP (by 2). Defining a Function Functions need to be placed in the code segment. In NASM a function is defined as follows: my_func: ; code of the function... ret For calling the function: call far my_func The pseudo commands PROC and ENDP ` (as in MASM / TASM ) are not supported by NASM . Function definitions can not be nested. Function Prologue At the beginning of each function some registers have to be saved. This code is called function prologue . The usual tasks in a function prologue are: Save old BP (push it on the stack). Assign SP to BP ( PB = SP ). So the BP points the the old BP . Save register contents to stack. So the registers can be used in the function. Allocate memory on stack for use in function. The the actual code of the function can run. Before the function ends it has to undo most of the things that were done in the prologue (i.e restoring registers, adjusting SP …). This code is called function epilogue. String Operations The x86 architecture contains some commands that can be executed on consecutive memory location (strings, arrays…). This commands are powerful but not so easy to understand. Here is a good explanation. Commands Mostly SI is used for addressing the source operator and DI is used for addressing the destination operator. Hence the names. For some commands the accumulator ( AL / AX ) is used as operator. SI is DS -relative by default. But the segment can be overridden. DI is ES -relative by default. The segment can not be overridden. Command Purpose MOVSx Move (copy) data addressed by SI to position addressed by DI . STOSx Load data from accumulator to position addressed by DI . LODSx Load data addressed by SI into accumulator. CMPSx Compare data addressed by SI with data addressed by DI . Flags are set according to result of [ SI ] - [ DI ]. SCASx Compare data from accumulator with data addressed by DI . Flags are set according to result of accumulator - [ DI ]. In the commands listed above x can be B for operation on bytes or W for operation on words (16-bit). Processors 80186 and newer have also the commands: INS and OUTS for string input and output from/to ports. Direction The direction of the string commands can be controlled by the direction flag: CLD : Clear direction flag. Index registers are incremented. STD : Set direction flag. Index registers are decremented. Repeat Prefix REP : Repeat the string command as long as CX \\(\\neq\\) 0. Decrement CX in each iteration. REPE : Repeat while operands are equal. REPZ : Repeat while zero ( ZF = 1). REPNE : Repeat while operands are not equal. REPNZ : Repeat while not zero ( ZF = 0). There is now practical use for LODSx with REP .","tags":"Programming","url":"intel_architecture.html","loc":"intel_architecture.html"},{"title":"Endianness","text":"The endianness means the representation of a variable (if it is bigger than one byte) in the memory. Big-Endian: Most Significant Byte ( MSB ) is first (bigger end is first) Little Endian: Least Significant Byte ( LSB ) is first (little end is first) CPU 's Big-Endian: Motorola-6800 Motorola-68k Coldfire System-z Sun- SPARC PowerPC (some models can be switched to little-endian) Atmel AVR32 TMS9900 Little-Endian: 6502 NEC -V800 PICmicro Intel-x86 Alpha Altera Nios Atmel AVR SH3 / SH4 (some) VAX Configurable: PowerPC (some models) IA -64 (Hewlett-Packard and Intel) ARM (and XScale, little-endian as default) Uses Technical IP and Ethernet Network Byte Order: Big-Endian Real-Life use Big-Endian: written arabic numbers: 1234 (thousend-two-hundert-thirty-four) Time: 18:34 Little-Endian: European Date (dd. MM YYYY ): 17. June 2015","tags":"Programming","url":"endianness.html","loc":"endianness.html"},{"title":"Memory Layout","text":"When a process is started the operating system allocates memory for this process. On modern systems this memory is usually mapped to an virtual address space (using the MMU ). But also on deeply embedded system the compiler and runtime environment need to define a memory layout. In this case there is only one process and there is no virtual memory. Memory Layout on different Operating Systems The use of the memory for a process is different in each operating system. This example overwiew is taken from Secure Coding in C and C++, by Robert Seacord : See also Anatomy of a Program in Memory Stack Growth Depending on the platform the stack grows in different directions: Downwards (descending): the stack grows from high adresses to low ones Upwards (ascending): it grows from high addresses to low ones There is also a distinction on where a stack pointer points to: Full: on the last written position. Empty: on the position where the next value is going to be written. So for example x86 and ARM (usually) have a full descending stack. Here is a short explanation. On most platforms the stack grows downwards. But there are some exeptions. This table is an inclomplete (but hopefully correct) overview. Most data in this table is taken from the FreeRTOS code. Architecture Compiler Stack growth Windows MSVC , MingW downwards POSIX GCC downwards DOS (16Bit) Open Watcom downwards x86 any downwards ARM any downwards Renesas 78K0R IAR downwards ARM7 (LPC2xxx, AT91SAM7S , AT91FR40008 ) GCC , RVDS , IAR downwards ARM Cortex (A5, A9, M0, M4, M3, M7, R4) GCC , IAR , RVDS , CCS , Tasking downwards ATMega323, AVR32 UC3 , ATSAM7S64 , SAM9XE … GCC , IAR downwards CORTUS APS3 GCC downwards Freescale ColdFire CodeWarrior, GCC downwards Cygnal SDCC upwards H8S2329 GCC downwards HCS12 CodeWarrior, GCC downwards MB91460 , MB96340 (Fujitsu) Softune downwards MCF5235 GCC downwards TI MSP430 GCC , IAR , Rowley, CCS downwards MicroBlaze ( IP core) GCC downwards NiosII ( IP core) GCC downwards Microchip PIC18 WizC downwards Microchip PIC18F MPLAB upwards Microchip PIC24 /dsPIC MPLAB upwards Microchip PIC32MX , PIC32MZ MPLAB downwards PowerPC GCC downwards Renesas X100, X200, X600, RL78 GCC , IAR , Renesas downwards SH2A_FPU Renesas downwards STMicroelectronics STR71x, STR75x, STR91x GCC , IAR downwards Tern EE Paradigm downwards TriCore 1782 GCC downwards V850ES IAR downwards","tags":"Programming","url":"memory_layout.html","loc":"memory_layout.html"},{"title":"Agile Software Development","text":"This article is an overview of Scrum and XP . Roles There are three main roles in Scrum. Each of it has some special responsibility. Product Owner ( PO ) The Product Owner is the connection between the Team and the stakeholders or clients. He's the only one who is allowed to give tasks to the Team. The Product Owner should not be part of the Team. The Product Owner maintains and manages the Product Backlog , release plane and the Return of Investment . The Product Owner is not part of the team and there should be only one Product Owner per product. Scrum Master ( SM ) The Scrum Master is responsible that the Scrum process is working properly and that everybody plays by the rules. He's also in charge if there are problems in the work progress of the Team and to remove any impediments. He has to solve social problems and organize anything that helps the Team to work productively. Ideally the Scrum Master isn't part of the Team. But often he is a member of the Developer-Team. Team The Team is the group of people (usually developers) that do the work. They turn the Backlog into an working increment of the product within a sprint. The team is self organizing. That means they decide how they do something. Ideally every teammember can do anything needed in the team. But sometimes there are special roles like teser, documentation writer or specialists of any kind. The main tasks of the Team are: Development (coding) Unit Tests Definition of Done Development Documentation Organize themselves Other There are no other people involved in the productive work cycle than the three mentioned above. If someone wants the Team do something he has to speak with the Product Owner . At some presentations and meetings there are guests allowed. But they can't decide anything or delegate any work to the team. There are some other people that are indirectly involved in the project. They are generally called Stakeholder . The most important ones are: Customer User Management There are two kind of people involved in a project. In Scrum jargon they are called Chickens and Pigs. This roles are based on a comic strip: Pig and Chicken Chickens There are always some people which have ideas, like to criticize, talk a lot about the project and try to get the reputation at the end. But they don't do any work or just a little. They are called the Chickens. Pigs On the other hand there are the kind of people that are highly involved in and motivated for a project. They do all the hard work and take some amount of risk. In Scrum they are called the Pigs. Timeboxes (Rituals) In Scrum all work is donne in time-boxes of defined length. The length of any time-box is defined by the involved people in advance! Adjust the times if needed (retrospective). Use a timer! The Sprint The most important timebox is the Sprint . It is a time span of two to four weeks (in most Teams it's 2 weeks). In this time the team works on the tasks they have themselves comitted to at the Scrum Planning Meeting . At the end of this time span the team should be able to present a product that can be potentially delivered to the customer. The product should be (before the sprint ends): thoroughly tested documented (developer and customer documentation) well developed (architecture, unit tests, design…) To achieve this goal there are several meetings that help to do the work organized and on the right time. A Sprint has this pattern: Sprint Planing Meeting Daily Scrum (every day for 15 minutes) Sprint Review Retrospective Release Planning Meeting In this meeting the team and the product owner discuss when the next release should be finished and what requirements shoud be included into this release. They discuss the time each requirement needs to be implemented. And they discuss the urgency of each requirement. Basically the Team decides the time needed for implementing a requirement and the Product Owner decides about the urgency. All the requirements are stored in a list called the Product Backlog . The Release Planning Meeting can be merged into the Sprint Planning Meeting if after each Sprint a real release is created. Sprint Planning Meeting The Product Backlog needs to be clean before the Sprint Planning Meeting! At the beginning of each Sprint the Team , the Scrum Master and the Product Owner decide which of the requirements of the Product Backlog are going to be implemented in the new Sprint . They are put into the Sprint Backlog . If needed the User Stories can be split into smaller Tasks. The Planning Meeting takes about one day (take the needed time!) The meeting has three phases: The objective (goal) of the sprint ( PO , 30 minutes) Pre-Selection of requirements ( PO , 3 hours) Analyze the requirements and split them to smaller tasks (activities) → put them in the Sprint Backlog (Team, 4 hours) The output of the meeting is: A sprint goal A list of team members (and their time commitment) A sprint backlog A demo date and place 5 - 15 Stories per Sprint is usually a good number. If the Sprint Planning Meeting takes to long the Scrum Master shall cut it short or reschedule it. Publish Sprint Goal Keep the whole company informed about what is going on. Publish short notes: Web/Intranet Wiki Printout (on door…) If people are not informed they will complain. Or they assume that the team is working on something but it isn't. Product Backlog Refinement It usually makes sense that backlog refinement (estimation, story splitting, etc.) is donne in a separate meeting (one hour per week) so the Sprint Planning can be more focused. So the needed time for the sprint planning meeting can be reduced. Rule: Number of weeks per sprint * 1 hour It's not accepted to reduce the estimate of the story in exchange of quality! Some teams estimate only the stories that go in the sprint backlog in the sprint planning meeting. Estimation The effort of each user story has to be estimated by the team (only by the team!). It is one of the most difficult parts of Scrum. But there are some techniques for doing the estimation. Storypoints The user stories are estimated in story points. Don't bother at first about how much work exactly a story point is. It depends on the team. And after a few sprints it becomes clear how many story points can be acomplished in one sprint. So how to define story points? Just chose a really small task (maybe just find the smallest user story in the backlog) and assign it the amount of one story point. Now it makes sense to define a range for the possible story points. The fibonacci numbers are a commonly used range: 1, 2, 3, 5, 8, 13, 21, 34, 55, 89 … For really small tasks the value of 1/2 can be added. If a user story is too big it should be split in smaller stories or tasks. When the range is defined the estimation can be done. There are quite a lot of possibilities to do the estimations. Planning poker The planning poker is a very good tool for the estimation of story points: Planning Poker Gut feeling It's a good practice to ask the team if they have a good feeling about the estimation. Ask it at least at the end of the estimation meeting (Product Backlog Refinement or Sprint Planning). Experience of previous Sprints After a few Sprints the team gets some experience in estimating User Stories. Stories and Tasks Each story can be divided into smaller tasks by the team. This makes estimation easier. And it helps the team when implementing the story. The tasks don't need to be represented in the backlog. It's too detailed and they can change often. Overview This is an overview of who attends which meetings and the aproximate time that the meeting should take (for a 2 week sprint). Meeting Sprint Planning Daily Scrum Sprint Review Retrospective PO Owner only if needed Paricipant Paricipant SM Paricipant Owner Paricipant Owner Team Paricipants Paricipants Owner Paricipants Others no may silently attend yes only if invited Timebox 4 hours 15 minutes 2 hours 2 hours Sprint Goal It's difficult to define a sprint goal. But it should be donne! Just ask the question: \"Why are we doing this sprint? We could do something else.\" The sprint goal should be in business terms, not technical terms Daily Scrum Every day the Team gathers for a fifteen minutes standup meeting and every member gives answers to the following questions: What have I donne since the last meeting. What am I going to do until the next meeting. What are my actual problems. There should be no discussions in this meeting It's just an information to the coworkers so people with similar interests or some good solutions can connect after the meeting. The sprint progress should be checked (by viewing the burndown chart) and adjustment to the sprint shall be made if necessary. Sprint Review (Presentation) When a Sprint is finished the Team presents the result. Anyone can join the presentation. Everybody is allowed to try out the resulting product. It should not be a pure presentation but more like a workshop. In this meeting the Product Owner (alone) decides if all the requirements are implemented completely. For each requirement this is a pure yes or no criteria. The product that is shown at the review is potentially shippable to the users (Product increment). The review meeting should take about 4 hours and the team should need at most 2 hours to prepare it. The Sprint Review is about feedback Insist on the Sprint Review The team gets credit Checklist: Present Sprint Goal ( PO ) What was good/bad (team) Don't waste time in preparing the Review Show product Workshop style: everybody is allowed to check out new features Focus on working code Fast and not beautiful Business oriented leave out technical detaion ‘ what did we do' (not ‘how did we do it') Let the audience try the product (if possible) No powerpoint… Retrospective The Retrospective is a very important meeting. All involved people have to attend this meeting ( PO , SM , Team). No other people are allowed in this meeting. It is moderated my the Scrum Master. The topic of the meeting is to discuss the Scrum process of the last Sprint. There should be discussed what was good and what was bad. There have to be decided what steps will be donne to improve the production cycle. It should take place after the Review. No PC 's are allowed. After the retrospective a new Sprint starts with the Sprint Planning Meeting. one hour for a two week sprint longer (half-, full-day) every few month It's the possiblity to improve! Without Retrospective mistakes are repeated over and over again Organisation: Members: OP , team, Scrum Master Closed room Designated secretary Summarize sprint Each person gets the chance to talk without getting interrupted respect each other be honest don't take things personally (separate person from things) talk abut good things and what could be improved how they are feeling what is expected It can help if everybody writes down its topics on a card then one after the other presents his/hers card write down what was good and what was bad feelings expectation Analyze processes and social topics Discuss if result (of sprint) is achieved and why (or why not) Formulate goals for improvement and make them public (i.e next to Scrum Board) If needed reformulate rules Coding standards Definition of Done Processes Documentation rules … Retrospective of Retrospective (5-10 min) Focus on few improvements for new sprint Artefacts Product Backlog The Product Backlog is a list with all Requirements that have to be donne to improve the product. The only person that can put new requirements into the Product Backlog is the Product Owner. Each requirement has to have several properties when put into the Backlog. ID (unambigous) Significance/Importance (unambigous, set only by PO ) Initial Estimate/Effort (set only by Team in the Refinement Meeting) Name Label/Topic Description (User story / use case, \"As X, I want Y, so that Z\") Criteria for aceptance Notes / References Source (who added this requirement) Risk How to Demo Bug tracking ID Components (i,e Database, UI , Business-Logic…) The Product Owner is responsible for the Product Backlog. He has to understand each item and he has to decide about the significance of the items. But he is not allowed to estimate the effort of an item. This can be only done by the team. It's a good practice to estimate the most significant backlog items that are not estimated yet once a week. There should be only one Product Backlog per product. Requirements S imple M easurable A chievable R ealisitc T raceable User Stories I ndependant N egotiable V aluable E stimatable S mall T raceable Tasks T ime boxed E verybody can do it C omplete H uman readable Sprint Backlog The whole team should be involved in maintaining the Sprint Backlog Sprint Burndown Chart There's a good article about the Burndown Chart. Release Burndown Tools Scrum Board The Scrum Board (Task Board) helps the Team (and the PO ) to have an overview about the actual ongoing work. It's inspired by Kanban. Each team has a slightly different Scrum board but they have similarities. There are three main sections on the Scrum Board: Backlog In Progress Donne A backlog item moves from Backlog to In Progress when someone has started to work on it. After it's finished it moves to the section Donne (see Definition of Donne ). The items should be sorted by importance from top to bottom. It's possible to add additional information: Sort by importance (top to down) Add subtasks Add colors (issue, bug, test, documentatin …) Add a mark on the cards for each day that was worked on the topic Index Cards All the Backlog items should be printed (or written) on index cards. So it's easier to handle them on the Scrum Board. Handling items on index cards can also simplify the Sprint Planning Meeting or the Backlog Refinement. It's a good idea to add a mark (sticker) on a index card for each day the issue is in the progress state. The marks can have different colors (i.e Coding, Testing, Documentation …) Definition of Donne The team with the PO need to decide what requirements need to be fulfiled for a story to be marked as done . Code complete is not feature complete! The best approach is to define a checklist for the stories. i.e: Feature/Bug is completely implemented Unit Tests are written Developer documentation is written Customer documentation is written … Maybe for different kind of stories (features, bugs, …) a different checklist is needed. Impediment Backlog What do I need to fulfill my task. i.e A server WiFi isn't working Order a book eXtreme Programming ( XP ) Extreme Programming has no roles and no process. There are just a few artefacts and about 25 rules: Pair Programing Developers should thing loud Switch roles every 20 minutes It's very tiring (exhausting) No overtime (because it's tiring) Refactoring Regularly After small development steps TDD /Testing Code coverage: 100% Continous integration General Notes Tools Day Plan a fixed timebox each Sprint (i.e each Friday Afternoon) for working on tools and internal tasks. This allows to improve and fix internal tools that otherwise would be postponed regurarly (even endlessly). This doesn't mean nobody is allowed to work on tools in the usual working time. But it's not allowed to work on stories on Tools Day. Knowledge Sync-Up Take regularly some time to synchronize internal knowledge between team members. Imagine for each team member what knowledge he/she has that wold be lost/missed if that memer leaves the company. Then try to share this knowledge with as many other team members as possible. Pair Programming Improve code quality Improve team focus Should not be done all day (exhausting) Changing pairs frequently Spread knowledge fast in team Both developer should have a computer at hand Don't force pair programming, encourage people Literature and Links The official Scrum Guide Scrum & XP from the Trenches Pigs and Chickens A good page with Podcasts (German) Cheat Sheet Scrum Checklist","tags":"Programming","url":"agile_software_development.html","loc":"agile_software_development.html"},{"title":"Move actual work to a new Branch","text":"This useful description is taken from: git: fetch and merge, don't pull Suppose you're working on the main branch of a project (called ‘master') and realise later that what you've been doing might have been a bad idea, and you would rather it were on a topic branch. If the commit graph looks like this: last version from another repository | v M---N-----O----P---Q (\"master\") Then you separate out your work with the following set of commands (where the diagrams show how the state has changed after them): $ git branch dubious-experiment M---N-----O----P---Q ( \"master\" and \"dubious-experiment\" ) $ git checkout master # Be careful with this next command: make sure \"git status\" is # clean, you're definitely on \"master\" and the # \"dubious-experiment\" branch has the commits you were working # on first... $ git reset --hard <SHA1sum of commit N> ( \"master\" ) M---N-------------O----P---Q ( \"dubious-experiment\" ) $ git pull # Or something that updates \"master\" from # somewhere else... M--N----R---S ( \"master\" ) \\ O---P---Q ( \"dubious-experiment\" )","tags":"Programming","url":"move_actual_work_to_a_new_branch.html","loc":"move_actual_work_to_a_new_branch.html"},{"title":"Introduction to Statistics","text":"This article is still work in progress! The notes on this page are from the Udacity course Intro to Statistics . Statistics and Probability Independent Events Probability of Event: \\(P\\) Probability of opposite Event: \\(1-P\\) Probability of composite Events: \\(P \\cdot P \\cdot ... \\cdot P\\) Dependent Events \\(P(A\\mid B)\\) : Probability of Event \\(A\\) when \\(B\\) already occured. Bayes' Rule See also Wikipedia $$P(A\\mid B) \\; = \\; \\frac {P(B\\mid A) \\cdot P(A)} {P(B)}$$ Prior Probability & Test Evidence -> Posterior Probability Prior: \\(P(A)\\) Posterior: $$P(A \\mid B) = P(A) \\cdot P(B \\mid A)\\\\ P(\\lnot A\\mid B) = P(\\lnot A) \\cdot P(B \\mid \\lnot A)$$ \\(P(A)\\) : the prior, is the initial degree of belief in \\(A\\) . \\(P(A \\mid B)\\) : the posterior, is the degree of belief having accounted for \\(B\\) . the quotient \\(\\frac{P(B \\mid A)}{P(B)}\\) represents the support \\(B\\) provides for \\(A\\) . Algorithm Imagine there is a disease. The possibility to get this particular disease is \\(P(D)\\) . There is a test to check if someone has that disease. But the test is not completely reliable. Prior: \\(P(D)\\) Sensitivity: \\(P(Pos \\mid D)\\) Specificity: \\(P(Neg \\mid \\lnot D)\\) The Prior says how many people have the disease. The Sensitivity says how many get a positive test if they have the disease . The Sensitivity says how many get a negative test if they don't have the disease . Imagine someone get's a positive test result. How can we calculate the probability that this person has the disease? Take the Prior and multiplicate it with \\(P(Pos \\mid D)\\) (Sensitivity) → \\(P(Pos, D)\\) . Take the Prior and multiplicate it with \\(P(Pos \\mid \\lnot D)\\) → \\(P(Pos, \\lnot D)\\) . Add the results of 1. and 2. up → \\(P(Pos)\\) . Divide the result from 1. \\(P(Pos, D)\\) by the result of 3. \\(P(Pos)\\) → \\(P(D \\mid Pos)\\) . Divide the result from 2. \\(P(Pos, \\lnot D)\\) by the result of 3. \\(P(Pos)\\) → \\(P(\\lnot D \\mid Pos)\\) . Check: add the results of 4. and 5. → \\(1\\) . Note: \\(P(A, B) = P(B, A)\\) The calculation for a negative test result are analogous. Just replace \\(Pos\\) with \\(Neg\\) . Probability Distributions In Continuous Distributions every outcome has the Probability \\(0\\) . Density PDF : Probability Density Function Probability for continuous spaces. Density can be bigger than \\(1\\) . Density: Always non-negative Doesn't need to be continuous Doesn't need to be smaller or equal than one Integrates to one Estimation Estimation problem is: Data → P P → P(Data) Maximum Likelihood Estimator ( MLE ): $$\\frac{1}{N} \\cdot \\sum_{i}&#94;{} X_i$$ The sum is always between \\(0\\) and \\(1\\) . Laplace Estimator: $$\\frac{1}{N + k} \\cdot \\left(1 + \\sum_{i}&#94;{} X_i \\right)$$ \\(k\\) : Number of Outcomes \\(N\\) : Number of Experiments Averages Mean $$\\mu = \\frac{1}{n} \\sum_{i=1}&#94;n{x_i}$$ Sum up all elements and divide by the number of elements. Median Sort all elements and take the one in the middle. Mode The value of the elements that occur most in the data set. Variance The variance is a measure how disperse a data set is. Subtract the mean from every item. Then sum up the squares of the subtractions and devide the result by the number of data items. $$Var(X) = \\sigma&#94;2_X = \\frac{1}{n} \\sum_{i}(x_i - \\mu)&#94;2$$ Alternative Formula: $$\\sigma&#94;2_X = \\frac{\\sum X_i&#94;2}{N} - \\frac{\\left( \\sum X_i \\right)&#94;2}{N&#94;2}$$ Standard Deviation The standard deviation is giving a sense how far away the items in a data set are from the mean. $$\\sigma_X = \\sqrt{Var(X)}$$ Overwiev of Mean, Variance and Standard Deviation This notes are taken from Khan Academy Concept Population Samples Mean \\(\\mu = \\frac{1}{N}\\sum_{i=1}&#94;{N} x_i\\) \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}&#94;{n} x_i\\) Variance \\(\\sigma&#94;2 = \\frac{1}{N} \\sum_{i=1}&#94;{N} (x_i - \\mu)&#94;2\\) \\(s&#94;2 = \\frac{1}{n-1} \\sum_{i=1}&#94;{N} (x_i - \\bar{x})&#94;2\\) Standard Deviation \\(\\sigma = \\sqrt{\\sigma&#94;2}\\) \\(s = \\sqrt{s&#94;2}\\) \\(N\\) : Number of items in Population \\(n\\) : Number of items in sample set taken from the population For an unbiased estimator (Variance of samples) the sum is divided by \\(n-1\\) ! \\(s = \\sqrt{s&#94;2}\\) : is not an unbiased estimator due to the non-linear nature of the square root. Binomial Coefficients Choose \\(k\\) elements from \\(n\\) possible elements (without putting back elements and without caring about the order of the chosen elements): $$\\binom{n}{k} = \\frac{n!}{k! \\cdot (n - k)!}$$ Binomial Distribution i.e. Flip a loaded coin: \\(p = P(heads)\\) : Probability to get a head from one coin flip. Flip coin \\(n\\) times. \\(P(\\#heads = k)\\) : Expectation to get \\(k\\) heads from all the flips. $$P(\\#heads = k) = \\frac{n!}{k! \\cdot (n - k)!} \\cdot p&#94;k \\cdot (1-p)&#94;{n-k} = \\binom{n}{k} \\cdot p&#94;k \\cdot (1-p)&#94;{n-k}$$ The Normal Distribution \\(\\mu\\) : Mean \\(\\sigma&#94;2\\) : Variance \\(\\sigma\\) : Standard Deviation $$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma&#94;2}}\\cdot e&#94;{\\left(-\\frac{1}{2}\\cdot\\frac{\\left(x-\\mu\\right)&#94;2}{\\sigma&#94;2}\\right)}$$ Normaliser The expression \\(\\frac{1}{\\sqrt{2\\pi\\sigma&#94;2}}\\) is needed to normalise the area underneath curve given by the rest of the formula ( \\(e&#94;{\\left(-\\frac{1}{2}\\cdot\\frac{\\left(x-\\mu\\right)&#94;2}{\\sigma&#94;2}\\right)}\\) ). Otherwise it would not add up to \\(1\\) .","tags":"Mathematics","url":"introduction_to_statistics.html","loc":"introduction_to_statistics.html"},{"title":"Macro Magic in C and C++","text":"Macro Tricks Stringification The # operator allows to create a string out of a macro parameter. With the stringize trick any defined constant can be converted into a string literal. #define stringize(s) _stringize(s) #define _stringize(s) #s #define IMPORTANT_CONST 23 If you'd use the stingize operator # directly in a macro you won't get the intended string: _stringize ( IMPORTANT_CONST ) \" IMPORTANT_CONST\" Thats why there is an other macro calling the first one: stringize (IMPORTANT_CONST) stringize(4) _stringize(4) \" 4\" Concatenation With the ## operator in a preprocessor macro it's possible to combine two tokens. #define CREATE_ID(name) ID_##name CREATE_ID ( IMPORTANT_THING ) ID_IMPORTANT_THING Further Reading There is a good explanation in the GCC online docs: Macros","tags":"Programming","url":"macro_magic_in_c_and_cpp.html","loc":"macro_magic_in_c_and_cpp.html"},{"title":"Emacs Artist Mode","text":"Keys Command Description - artist-mode Start artist mode C-c C-c artist-mode-off End artist mode C-c &#94; Move (and draw) up (north) C-c ‘ Move (and draw) up-right (north-east) C-c > Move (and draw) right (east) C-c \\ Move (and draw) right-down (south-east) C-c . Move (and draw) down (south) C-c / Move (and draw) left-down (south-west) C-c < Move (and draw) left (west) C-c ` Move (and draw) left-up (north-west)","tags":"Programming","url":"emacs_artist_mode.html","loc":"emacs_artist_mode.html"},{"title":"Harmful C Functions and their replacements","text":"Pre- and postfixes for printf and scanf There are quite a lot of functions in the printf and scanff amily. It might not be very clear when to use which of them. Here is an explanation of the functions in the Standard C Library . Prefixes Most prefixed define where the function reads ( scanf ) or wirtes ( printf ) data from or to: no prefix : use STDIN or STDOUT f : use a FILE stream s : use a char buffer (string) sn : same as s but checks for buffer size (only printf ) v : takes a va_list instead ... (ellipsis), can be combined with the other prefixes Postfix The C11 standard introduced addtional functions with the _s postfix which do some checks on the data. Overview Output So we get following printf -like functions: Output ellipsis va_list STDOUT printf ( _s ) vprintf ( _s ) file fprintf( _s ) vfprintf ( _s ) char buffer sprintf ( _s ) vsprintf ( _s ) char buffer with size snprintf ( _s ) vsnprintf ( _s ) The functions with _s postfix should be preferred if available (C11). The sprintf and vsprintf functions should not be used since the can result in stack overflow. They are also suffer from string vulnerability. Use snpritf or vsnprintf instead. If possible with _s postfix. And force null-termination manually 1 . Input And we get following scanf -like functions: Input ellipsis va_list STDIN scanf ( _s ) vscanf ( _s ) file fscanf ( _s ) vfscanf ( _s ) char buffer sscanf ( _s ) vsscanf ( _s ) The functions with _s postfix should be preferred if available (C11). And force null-termination manually 1 . OpenBSD The OpenBSD kernel library defines some additional functions that are safer than their counterparts in the standard library. C standard library OpenBSD kernellibrary Copying string strcpy strlcpy Applying (concatenating) string strcat strlcat Replacements These C functions suffer buffer overflow problems: Original Replacement gets() fgets() cuserid() getlogin() or getpwuid() scanf() family See 2 and 3 , use functions with _s postfix (C11) sprintf() snprintf() , use functions with _s postfix (C11) vsprintf() vsnprintf() , use functions with _s postfix (C11) strcat() strncat() strcpy() strncpy() streadd() , strtrns() , strecpy() … Check lengths of buffers or use standard library functions getwd() getcwd() See also 4 References randomascii.wordpress.com ↩ ↩ stackoverflow.com ↩ stackoverflow.com ↩ stackoverflow.com ↩","tags":"Programming","url":"harmful_c_functions_and_their_replacements.html","loc":"harmful_c_functions_and_their_replacements.html"},{"title":"Enums in C and C++","text":"Enums in C and C++ are a very simple construct. It's just a collection of identifiers that have (usually) distinctive values. By default the values increase by one in the stated order. But the values can defined by the programmer. Even if enums are a simple language construct there are special precautions that need to be taken when using them: Is a enum continuous (i.e. can I iterate over it?) Have the enum values meaning outside of my software or module what is the size (in Bytes) of an enum variable Iterating over enums Be careful if you iterate over an enum. If the enum is not continuously defined your code is probably doing some things you don't want it to do. For exampe: typedef enum { VAL_1 = 10 , VAL_2 = 15 , VAL_3 = 20 , VAL_NB } my_enum_t ; int main () { int enum_var ; for ( enum_var = VAL_1 ; enum_var < VAL_NB ; ++ enum_var ) { printf ( \"%i \\n \" , enum_var ); } } printf is called 11 times with the values 10 to 20. That's usually not what we want. So what's the solution? Don't iterate over enums that have fixed values assigned. If you have to do it there is a trick called X Macros that I'll cover in another post. There is one exception. You can assign a value to the first enum type: typedef enum { VAL_1 = 10 , VAL_2 , VAL_3 , VAL_NB } my_enum_t ; If you call the same loop as above you'll get calls to printf with the values: 10 (VAL_1), 11 (VAL_2) and 12 (VAL_3). Enums with fixed values If you don't need to iterate over your enum it's usually a good practice to assign a defined value to your enum fields. This is even mandatory if your enum has a meaning across your software (or module) or even different versions of the same software. Add defined values to your enums if: the value can be saved to a file the value is sent (or received) over a physical interface the value is needed in an other software (or library…) or a different version of the same software (i. e. feature release) But if you do so, don't iterate over your enum! If you serialize/deserialize your values (i.e. save into a file, send over an interface) be careful about the enum size and the endianness of the used platform. Size and endianness The size (and endianness) of a enum variable depends on the compiler and on the target platform. So if you write platform independent code (and you should always do that) take care of this. There are some language extension in different compilers to address this topic. For example in Visual Studio: typedef enum : uint8_t { VAL_1 , VAL2 , /* … */ , VAL_NB } my_enum_t ; For platform independent code you can do: #define ENUM_AS_UINT8 : uint8_t Romable values Another use of enums is mainly used in embedded systems development. In embedded systems you are often concerned that a constant is stored in the program flash. Otherwise it has to be loaded from RAM at runtime (and even stored there at start time). The usual way is to use defines. But they are not really type safe. So for int-types it's possible to use enums. If your compiler(s) support it you can even use anonymous enums. Advanced enum constructs (C++11, Qt) C++11 introduces the enum class. And Qt some extensions for enums (Q_ENUMS) that I'll not cover here. If you can use one of this advanced enum constructs then do it! Conclusion If you have to iterate over your enum: don't add explicit values to your enum, but add and {identifier}_NB value to the end (or something similar) to check when the iteration is finished If your enum value is meaningful (serialization…): assign a defined value and take care about endianness and size of the enum variables Use (anonymous) enums to make values Romable Use an advanced enum type (C++11, Qt) if you can If you need fixed enum values and you want to iterate over the values you need two enums. Use X Macros for it (there is going to be a feature post on that) References Endianness (Wikipedia) The usage of anonymous enums (Stack Overflow)","tags":"Programming","url":"enums_in_c_and_cpp.html","loc":"enums_in_c_and_cpp.html"},{"title":"Intro to SQL","text":"Most notes taken here are from the Udacity SQL course Aggregations Aggregation functions work on the values of a column. They return a single value. Function Description FIRST () First value LAST () Last value COUNT () Number of rows MAX () Largest value MIN () Smallest value SUM () Sum of column AVG () Average value Not all functions work on all kinds of columns. Joins JOIN clause is used to combine rows from several tables. The combination is based on a common field between them. There are different JOIN operaions in SQL : INNER JOIN : Returns rows when at least one match in both tables LEFT JOIN : Return all rows from left table and matched rows from right table RIGHT JOIN : the other way round FULL JOIN : Return all rows when a match in one table Example: 2 Tables: Animals in Zoo ( animals ) name species birthdate Diet of Animals ( diet ) species food Example: Which animals eat fisch? SELECT animals . name , animals . species , diet . food FROM animals JOIN diet ON animal . species = diet . species WHERE food = 'fish' ; Unique Keys Primary keys used to identify an entry (row) in a table Most RDBMS can generate unique queues Sometimes a table already has a field that can be used as unique primary key Select Clauses LIMIT count [ OFFSET skip] count: how many rows to return skip: how far into the result to start ORDER BY columns [ DESC ] columns: which columns to sort by, seperated by comas DESC : sort in reverse order (descending) GROUP BY columns used only with aggregations (e.g. count, sum) which columns to use for aggregation Select example: :::sql SELECT name, birthday FROM animals WHERE species = ‘gorilla' AND name = ‘Max'; SELECT : selects colums FROM : table to select from WHERE : row restriction","tags":"Programming","url":"intro_to_sql.html","loc":"intro_to_sql.html"},{"title":"Modern OS 's","text":"This page collects some notes about different Operating System approaches. Most of the information gathered here is from the course Advanced Operating Systems . Some information is from Operating System . And other Wikipedia pages. The book Modern Operating Systems by Andrew S. Tanenbaum is also a very good resource I'm using for learning about Operating Systems. Overview of Kernels Kernel Type Programming Language Notes SPIN Microkernel (Mach-like) Modula-3 Special approach. Linux Monolithic (modular) C, Assembly Loadable kernel modules allow loading extensions (drivers) at runtime. XNU Hybrid C, C++ Kernel of OS X. Mach-3.0 and FreeBSD combined. BSD Monolithic C FreeBSD, OpenBSD, NetBSD… Mach Microkernel C? One of the earlyest mikrokernel. Not all mach versions are mikrokernels. Windows Hybrid C, C++, Assembly Win NT : Hybrid, Win 9x and earlyer: Mololithic. FreeRTOS Microkernel ( RTOS ) C, Assembly Real Time OS . Mainly for embedded systems. UNIX Monolithic C, Assembly Original: AT &T Unix. L3 Microkernel ELAN Predecessor of L4. Barrelfish \"Multikernel\" C Special aproach. Mac OS 9 Microkernel (Nanokernel) ? Legacy QNX Microkernel ( RTOS ) ? Unix-Like ( POSIX ), Qt supported. VxWorks Monolithic ( RTOS ) ? Spring Microkernel independent ( CORBA IDL ) Solaris emulation, OOP design, doors … Other OS 's: Nucleus RTOS ChorusOS Sharing Resources One of the most important task of an OS is to share (hardware) resources. There are two posibilities how resources can be shared. time sharing: i.e CPU , Printer space sharing: i.e RAM , Hard Disc Exokernel Allocating resources to library OS 's: space (memory) time ( CPU ) Library OS 's can ‘download' code into the kernel. A security management mechanism checks if it is allowed by th library OS . Memory ( TLB ) TLB : Translation Lookaside Buffer. Software- TLB is snapshot for the TLB for switching the library OS 's. CPU -Scheduling Linear vector of \"time slots\" Each library- OS marks the time slots for own use If OS takes more time than allowed in a time slot it gets less time in the next time slot (penalty) Strand as abstraction of threads Revoction of Resources Exokernel tells the library OS which resources are revoked (repossesion vector) L3 L3 strikes against microkernel Kernel-User switches (boarder crossing cost) Address space switches (Protected Procedures Calls for Cross Protection domain calls) Thread switches + IPC (Kernel mediation for PPC ) Memory effects (locality loss) Thesis of L3 for OS structuring Minimal abstractions in microkernel Microkernels are processor specific in implementation => non-portable Right set of microkernel abstractions and processor-specific implementation => efficient processor independent abstractions at higher levels L3 is faster than Mach (Microkernel). Virtualization Hypervisor: Operation system of operation systems ( VMM : Virtal Machine Monitor). Native Hypervisor (bare metal) Hypervisor runs directly on hardware. Guest OS 's running inside the hypervisor. Hosted Hypervisors Run on top of a Host OS (as an application process). Guest OS 's running inside the hypervisor. VMWare Workstation VirtualBox Full virtualization Guest OS 's are not touched (unchanged binaries) They are running as user process in host OS Privileged instructios in guest OS 's trigger trap in hypervisor (trap and emulate) i.e. VMWare Para Virtualization Guest OS is modified to run on an an hypervisor A very small percentage of the guest OS code needs to be changed i.e. Xen Overview Virtualize hardware: memory hierarchy CPU Devices Memory Virtualization Not virtualized: Page Table ( PT ) maps Virtual Page Numbers ( VPN ) of processes to Physical Page Number ( PPN ). Virtualized: Guest OS translates Virtual Page Number ( VPN ) to Physical Page Number ( PPN ) with Page Table ( PT ). Hypervisor then translates Physical Page Number ( PPN ) to Machine Page Number ( MPN ) with Shadow Page Table (S- PT ). CPU Virtualization Events that happen in a task of the Guest OS need to be delivered to the Guest OS by the Hypervisor. An event can be: External Interrupt ( HW Interrupt) Exception ( HW - and SW -Exception) Page Fault Syscall The occured events are delivered to the Guest OS wrapped in a SW -Interrupt by the Hypervisor. Communication from the Guest OS to the CPU hapens generally through traps. So the hypervisor can handle it. In para-virtualized environment the Hypervisor can provide an API for the Guest OS instead of using traps. Full virtualization: \"trap and emulate\" Para virtualization: more opportunity for innovation Control Transfer Full virtualization implicit (traps) guest → hypervisor software interrupts (events) hypervisor → guest Para virtualization explicit (hypercalls) guest → hypervisor software interrupts (events) hypervisor → guest Guest has control via hypercalls on when event notifications need to be delivered. Data Transfer Full virtualization implicit Para virtualization (e.g. Xen) explicit ⇒ opportunity to innovate Xen I/O-Rings Xen uses a ring buffer for data transfer with producer-consumer pattern. There are 4 pointer to the buffer. Request producer (shared, updated by guest) Request consumer (private to Xen) Response producer (shared, updated by guest) Response consumer (private to guest) Memory Models Memory Consistency: What is the Model presented to the Programmer? Cache Coherence: How is the System implementing the Model in presence of private caches? Sequential Consistency (Memory Model) Access to a shared memory location is performed in sequence by the processors. The accesses can be interwoven. Cache Coherence Non cache coherent shared address space multi processor ( NCC shared memory multi processor) System only gives access to shared address space. System Software maintains chaching. Shared address space available for all processors Private caches If you modify data it's a problem of the system software to make sure the caches are coherent! CC shared memory multi processor Hardware provides shared address space Maintains cache coherence in hardware Write-Invalidate Hardware ensures that written memory location is invalidated in all caches. Write-Update Hardware ensure that modified memory location is updated in all caches. \" Shared memory machines scale well when you don't share memory.\", Chuck Thacker Synchronization Synchronization Primitives Mutex Locks (single exclusive access to resource) Shared Lock (Multiple reader to one resource) Barriers (Synchronize threads, wait for other threads till all completed their work) Atomic Instruction During the execution of an instruction the processor can not be interrupted. Aquiring a lock needs to be atomic. Read-Modify-Write ( RMW ) Different aproaches: Test-and-Set (T+S): Reads a memory location. Then returns the actual value and sets it to 1 atomically. Fetch-and-Inc: Reads a memory location. Then returns the actual value and increments it atomically. Fetch-and- \\(\\Phi\\) : Generally with any given function ( \\(\\Phi\\) ) after fetching and returning the actual value. Scalability issues with Synchronitation Latency: Latency is the time that a thread needs to acquire a lock. Waiting time: The time that a thread needs to wait to get the lock. This time is in the hands of the application developers and not of the OS developers. Contention: If a lock is released and several threads are waiting for it. How long does it take until a thread is chosen from the waiting threads. Spinlock Naive Spinlock (Spin on T+S) A thread or processor waiting for a lock loops (spins) without doing any useful work. LOCK(L): WHILE(T+S(L) == locked); Problems with this naive spinlock implementation: Too much contention: Every processor tries to access lock. Does not exploit caches: Private caches in processor can't contain lock variable (T+S needs to be atomic. That's not possible with cached variables). Disrupts useful work: After releasing a lock a processor usually wants to do some work. But other processors need resources for trying to acquire the lock. Caching Spinlock (Spin on read) Waiting processors spin on cached copy of L . So there is no communication to memory. The cached copy of L is updated by the cache coherence mechanism of the system. LOCK(L): WHILE(L == locked); // Spinning on cached var. Reading L is atomic. IF(T+S(L) == locked) // Read L from memory (not cache). go back; // If it fails start spinning on cached L again. Less traffic on bus. Disruptive. Spinlock with Delay If a lock is released every process waits for a given time before trying to aquire the lock. Delay after lock release WHILE (( L == locked ) or ( T + S ( L ) == locked )) { WHILE ( L == locked ); delay ( d [ Pi ] ); // when I get the lock I wait a while } The delay time is dependent on the processor. Delay with exponential backoff WHILE(T+S(L) == locked) // not using chaching at all { delay(d); // d is small at first d = d * 2; } Every time when the lock is checked and not free the processor waits a longer time before trying again. T+S can be used because we wait before trying to quire the lock again. This reduces the contention. This algorithm works also on architecture without chaches (or cache coherent system in HW ). Ticket Lock The process that tries to acquire a lock gets a ticket. When the lock is released the process is notified. This adds fairness to the locking algorithm. But it causes contention. Summary Spin on T+S, Spin on read and Spinlock with Delay are not fair Ticket Lock is fair but noisy (contention) Queuing Lock Array-based queuing Lock (Anderson Lock) For each lock there is an array with flags. The size of the array is equal to the number of processors. Flags: has-lock ( hl ) must-wait ( mw ) Only one slot can be marked as hl . The slots are not statically associated with a particular processor. LOCK ( L ): myPlace = fetch_and_inc ( queuelast ); WHILE ( flags [ myPlace mod N ] == mw ); UNLOCK ( L ): flags [ current mod N ] = mw ; // release lock for feature use flags [( current + 1 ) mod N ] = hl ; // next processor in queue gets lock Only one atomic operation needed per critical section Fairness: first-come, first-served But needs a lot of space. For each lock there is one array with the size of number of processors Link Based queuing Lock The Locks are stored in a linked list. Synchronization for adding and removing clients to a lock. Published by John M. Mellor-Crummey and Michael L. Scott ( MCS ) Parallel Architectures Symmetric multiprocessing ( SMP ) Non-uniform memory access ( NUMA ) Cache coherence Computer cluster OS : Spring Wikipedia:Spring (operating system) An Overview of the Spring System Indepentant of programming language ( CORBA IDL ) OOP Microkernel Spring Nucleus: secure objects with high speed object invocation between address spaces and networked machines Object managers: e.g. file system: file objects running in non-kernel mode like server IDL ( CORBA ) Generate: Interface (e.g. header file) Client side stub code: get access to object implemented in other address space or machine Server side stub code: used by object manager to translate remote object invocations into the run-time environment of object implementation Serverless objects entire state of object always in clients address space passing to other address space: copy entire state Subcontracts plugging different kinds of object runtimes control how object invocation is implemented object references are transmitted between address spaces object references are released … Subcontracts Singleton Replication Cheap objects Caching Crash recovery … Overall System Structure Microkernel Processes, IPC and Memory management part of kernel (Liedtke) Kernel mode Nucleus: processes and IPC Entered by trap mechanism Virtual Memory Manager page faults external pagers (looks like any other object server) … All other system services implemented as user-level servers naming paging network IO file systems keyboard management … inherently distributed: all services and objects available on one node are also a available on other nodes in the distributed system Nucleus Three basic abstractions: domains, threads and doors No memory management Domains analogous to processes in Unix or tasks in Mach address space for applications resources: threads, doors, … Threads execute within domains Doors Cross domain calls OOP calls between domains entry points to domains represented by a PC and a unique value (nominated by domain) unique value used by object server to identify state of object unique value might be a C++ pointer Doors pieces of protected nucleus state each domain has a table with doors it has access to refers to doors using door identifiers same door may be referenced by several different door identifiers in different domains possesion of door: right to send invocation request to door valid door can be obtained with consent of target domain someone who already has door identifier to that door doors can be passed as arguments or be returned as values from functions Object invocations via Doors cross-address-space invocation Algorithm nucleus allocates server thread in target address space nucleus transfers control to that thread, passing information about door and the arguments of the invoked method when the called thread returns: deactivation of the thread activation of caller thread passing return data Network Proxies rovide object invocation across network - connect nuclei of different machines transparently - proxies are normal user-mode domains (no special support from nucleus) - client and server need not to be aware that proxies exist: normal door invocation Security model secure access to objects two basic security mechanisms: Access Control Lists software capabilities invocations are controlled by nucleus doors and front objects Virtual Memory demand-paged virtual memory system per machine virtual memory manager ( VMM ) handles local memory: mapping sharing protection transfer caching depends on external pagers for accessing store and maintaining inter-machine coherency Address Space and Memory objects used by clients Adress Space: virtual address space of domain implemented by VMM Memory abstraction of memory that can be mapped to address space can be a file operations to set and query length and bind no read/write or page-in/out operations Separating memory from interface for paging useful for implementing filesystem Memory Object server can be on different machine than the Pager Object server Cache and Pager Objects two-way connection between the VMMs and external pagers cache must be coherent between more than one VMM VMM obtains data by invoking pager object implemented by external pager external pager performs coherency actions by invoking cache object implemented by a VMM the coherency protocol is not specified by the architecture: external pagers can implement any coherency protocol File System file objects implemented by fileserver file objects may be memory mapped access to files on local disks or over the network Spring security and naming architectures to provide access control and directory services Spring file system typically consists of several layered file servers Spring Naming Naming for: users files printers machines services … uniform name service any object can be bound to any name Object can be: local to a process local to a machine resident on the network transient or persistent standard system object process environment object user specific object new name spaces can be compose of different name spaces name service allows objects to be associated with a name in a context objects need not be bound to a name naming graph (name space): a directed graph nodes with outgoing edges are contexts naming provide persistence access control and authentication UNIX Emulation Spring can run Solaris binaries implemented by user-level code contains no UNIX code two components: a shared library (libue.so), dynamically linked with each Solaris binary a set of UNIX -specific services Barrelfish Multikernel OS as distributed system of functional units (network of independent cores) communication via explicit messages (message passing) Design principles Make all inter-core communication explicit (message passing) Make OS structure HW -neutral View state as replicated instead of shared Single computer as networked system RPC latency is lower than shared memory access asynchronous or pipelined RPC : client processors can avoid stalling on cache misses Asynchronous messaging: event-driven programming pitfalls with shared data structures in scalable progams: correctness performance lock granularity field layout in structures (alignment …) applications can share memory across cores, but OS design does not reky on shared memory networking optimizations for message passing: pipelining: a number of requests in flight at once batching: sending or processing a number of messages together Multikernel separates OS structure from hardware Replication of state is a useful framework for: changes to the running cores hotplugging processors shutting down subsystems to save power Replication: Benefit of performance and availability at cost of ensuring replica consistency porting application to Barrelfish are straightforward standard C and mate library Virtual Memory Management subset of POSIX threads and file I/O APIs System Structure multiple independent OS instances communication via explicit messages each OS instance per core privileged mode CPU driver user mode monitor process CPU driver local to core monitor performes inter-core coordination coordinate system-wide state handling message-oriented inter-core communication mostly processor-agnostic monitor and CPU driver encapsulate: scheduling, communication and resource allocation device drivers and system services (network stack, memory allocators …) run in user mode device interrupts are routed in hardware to the appropriate core CPU driver enfonces protection performs authorization timeslices processes mediates access to core and associated hardware ( MMU , APIC …) completely event-driven and non-preemptable (single threaded) processes events: traps from user processes, interrupts from devices to other cores Process structure collection of dispatcher objects (cores) dispatchers scheduled by local CPU driver Inter-core communication user-level RPC ( URPC ): marschaling code generated with stub compiler name service to locate services channels established between services (setup performed by monitors) no shared memory other than URPC channel and packet payload cross-core thread management is performed in user-space thread schedulers exchange messages to create and unblock threads and migrate threads between cores POSIX like threads Memory Management global set of resources user-level app and services services use shared memory across multiple cores capability system ( seL4 ) virtual memory management (allocation/manipulation of page tables…) entirely in user-space capability code is complex Knowledge and Policy Engine Maintaining knowledge of hardware in ‘system knowledge base' subset of first-order logic: The ECLiPSe Constraint Programming System declarative approach hardware discovery","tags":"Programming","url":"modern_os's.html","loc":"modern_os's.html"}]};